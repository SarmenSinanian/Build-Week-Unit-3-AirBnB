{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XsUEp8oxvHby"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "OyQgWS0NzE_u",
    "outputId": "cfd5e369-c538-4614-9fff-43c3aa878526"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in c:\\programdata\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: pandas>=0.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from category_encoders) (0.24.2)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from category_encoders) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from category_encoders) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from category_encoders) (1.2.1)\n",
      "Requirement already satisfied: patsy>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from category_encoders) (0.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.21.1->category_encoders) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.21.1->category_encoders) (2019.1)\n",
      "Requirement already satisfied: six in c:\\users\\sarmen\\appdata\\roaming\\python\\python37\\site-packages (from patsy>=0.4.1->category_encoders) (1.12.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (0.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Sp5R6-bgxyju",
    "outputId": "81ad9236-68e6-4b7c-c9bb-02e9dcf31385"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22552, 96)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'E:\\Desktop\\Lambda_School\\Group Projects\\Unit 4 Sprint 4 Build Week AirBnB\\listings_summary.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0O1fzFEKoZDm"
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'neighbourhood_group_cleansed',\n",
    "    'room_type',\n",
    "    'accommodates',\n",
    "    'bathrooms',\n",
    "    'bedrooms',\n",
    "    'beds',\n",
    "    'bed_type',\n",
    "    'amenities',\n",
    "    'price',\n",
    "    'security_deposit',\n",
    "    'cleaning_fee',\n",
    "    'minimum_nights',\n",
    "    'instant_bookable'         \n",
    "]\n",
    "\n",
    "df = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "colab_type": "code",
    "id": "0qLvVzUdVQOv",
    "outputId": "6590b6fc-d06e-4391-e393-8301786fb26a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "room_type           0\n",
       "accommodates        0\n",
       "bathrooms           0\n",
       "bedrooms            0\n",
       "beds                0\n",
       "bed_type            0\n",
       "amenities           0\n",
       "price               0\n",
       "security_deposit    0\n",
       "cleaning_fee        0\n",
       "minimum_nights      0\n",
       "instant_bookable    0\n",
       "neighborhood        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bathrooms'] = df['bathrooms'].fillna(df['bathrooms'].mean()).astype(float)\n",
    "df['bedrooms'] = df['bedrooms'].fillna(df['bedrooms'].mean()).astype(float)\n",
    "df['beds'] = df['beds'].fillna(df['beds'].mean()).astype(float)\n",
    "df['security_deposit'] = df['security_deposit'].fillna('$0.00')\n",
    "df['cleaning_fee'] = df['cleaning_fee'].fillna('$0.00')\n",
    "\n",
    "df['price'] = df['price'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "df['security_deposit'] = df['security_deposit'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "df['cleaning_fee'] = df['cleaning_fee'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "\n",
    "df['neighborhood'] = df['neighbourhood_group_cleansed']\n",
    "df = df.drop(columns=['neighbourhood_group_cleansed'])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "colab_type": "code",
    "id": "9oipK0V2Wsu9",
    "outputId": "1801790f-62a6-4520-97b6-197fbba8f43d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>room_type</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>bed_type</th>\n",
       "      <th>amenities</th>\n",
       "      <th>price</th>\n",
       "      <th>security_deposit</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>neighborhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>{TV,\"Cable TV\",Wifi,Kitchen,Gym,Heating,\"Famil...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Mitte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Private room</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>{Wifi,Kitchen,Elevator,Heating,Washer,Essentia...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Pankow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>{Internet,Wifi,Kitchen,\"Buzzer/wireless interc...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>Pankow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Private room</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Pull-out Sofa</td>\n",
       "      <td>{Internet,Wifi,\"Pets allowed\",\"Pets live on th...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Tempelhof - Schoneberg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         room_type  accommodates  bathrooms  bedrooms  beds       bed_type  \\\n",
       "0  Entire home/apt             3        1.0       1.0   2.0       Real Bed   \n",
       "1     Private room             2        1.0       1.0   1.0       Real Bed   \n",
       "2  Entire home/apt             4        1.0       1.0   2.0       Real Bed   \n",
       "3     Private room             2        1.0       1.0   1.0  Pull-out Sofa   \n",
       "\n",
       "                                           amenities  price  security_deposit  \\\n",
       "0  {TV,\"Cable TV\",Wifi,Kitchen,Gym,Heating,\"Famil...   60.0             200.0   \n",
       "1  {Wifi,Kitchen,Elevator,Heating,Washer,Essentia...   17.0               0.0   \n",
       "2  {Internet,Wifi,Kitchen,\"Buzzer/wireless interc...   90.0             200.0   \n",
       "3  {Internet,Wifi,\"Pets allowed\",\"Pets live on th...   26.0             250.0   \n",
       "\n",
       "   cleaning_fee  minimum_nights  instant_bookable            neighborhood  \n",
       "0          30.0               4                 0                   Mitte  \n",
       "1           0.0               2                 0                  Pankow  \n",
       "2          50.0              62                 1                  Pankow  \n",
       "3          30.0               5                 0  Tempelhof - Schoneberg  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['instant_bookable'] = df['instant_bookable'].replace({'t': 1, 'f': 0})\n",
    "df['neighborhood'] = df['neighborhood'].str.replace('รถ', 'o')\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "id": "EvQg5oT3h3ih",
    "outputId": "df047e25-21a8-4eb2-80cc-04423d252c91"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>room_type</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>bed_type</th>\n",
       "      <th>price</th>\n",
       "      <th>security_deposit</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>wifi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>60.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Mitte</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Private room</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Pankow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>90.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>Pankow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Private room</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Pull-out Sofa</td>\n",
       "      <td>26.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Tempelhof - Schoneberg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Private room</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Pankow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         room_type  accommodates  bathrooms  bedrooms  beds       bed_type  \\\n",
       "0  Entire home/apt             3        1.0       1.0   2.0       Real Bed   \n",
       "1     Private room             2        1.0       1.0   1.0       Real Bed   \n",
       "2  Entire home/apt             4        1.0       1.0   2.0       Real Bed   \n",
       "3     Private room             2        1.0       1.0   1.0  Pull-out Sofa   \n",
       "4     Private room             2        1.0       1.0   2.0       Real Bed   \n",
       "\n",
       "   price  security_deposit  cleaning_fee  minimum_nights  instant_bookable  \\\n",
       "0   60.0             200.0          30.0               4                 0   \n",
       "1   17.0               0.0           0.0               2                 0   \n",
       "2   90.0             200.0          50.0              62                 1   \n",
       "3   26.0             250.0          30.0               5                 0   \n",
       "4   42.0               0.0           0.0               2                 0   \n",
       "\n",
       "             neighborhood  wifi  \n",
       "0                   Mitte     1  \n",
       "1                  Pankow     1  \n",
       "2                  Pankow     1  \n",
       "3  Tempelhof - Schoneberg     1  \n",
       "4                  Pankow     1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['amenities'] = df['amenities'].str.replace('{',\"[\")\n",
    "df['amenities'] = df['amenities'].str.replace('}',\"]\")\n",
    "df['amenities'] = df['amenities'].str.replace('\"',\"\")\n",
    "df['amenities'] = df['amenities'].apply(lambda x: x.lower())\n",
    "\n",
    "df['wifi'] = df['amenities'].str.contains('wifi').astype(str)\n",
    "df['wifi'] = df['wifi'].replace({'True': 1, 'False': 0})\n",
    "df = df.drop(columns='amenities')\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "id": "YSnm3pAoUxnk",
    "outputId": "fc26388c-5bf4-4493-d112-6e496d7e2cba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>price</th>\n",
       "      <th>security_deposit</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>wifi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22552.000000</td>\n",
       "      <td>22552.000000</td>\n",
       "      <td>22552.000000</td>\n",
       "      <td>22552.000000</td>\n",
       "      <td>22552.000000</td>\n",
       "      <td>22552.000000</td>\n",
       "      <td>22552.000000</td>\n",
       "      <td>22552.000000</td>\n",
       "      <td>22552.000000</td>\n",
       "      <td>22552.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.637460</td>\n",
       "      <td>1.092695</td>\n",
       "      <td>1.161134</td>\n",
       "      <td>1.620558</td>\n",
       "      <td>67.143668</td>\n",
       "      <td>118.284454</td>\n",
       "      <td>18.253991</td>\n",
       "      <td>7.157059</td>\n",
       "      <td>0.311901</td>\n",
       "      <td>0.954106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.510548</td>\n",
       "      <td>0.329660</td>\n",
       "      <td>0.645898</td>\n",
       "      <td>1.173798</td>\n",
       "      <td>220.266210</td>\n",
       "      <td>304.232661</td>\n",
       "      <td>28.172816</td>\n",
       "      <td>40.665073</td>\n",
       "      <td>0.463280</td>\n",
       "      <td>0.209260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>9000.000000</td>\n",
       "      <td>4280.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       accommodates     bathrooms      bedrooms          beds         price  \\\n",
       "count  22552.000000  22552.000000  22552.000000  22552.000000  22552.000000   \n",
       "mean       2.637460      1.092695      1.161134      1.620558     67.143668   \n",
       "std        1.510548      0.329660      0.645898      1.173798    220.266210   \n",
       "min        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        2.000000      1.000000      1.000000      1.000000     30.000000   \n",
       "50%        2.000000      1.000000      1.000000      1.000000     45.000000   \n",
       "75%        3.000000      1.000000      1.000000      2.000000     70.000000   \n",
       "max       16.000000      8.500000     12.000000     22.000000   9000.000000   \n",
       "\n",
       "       security_deposit  cleaning_fee  minimum_nights  instant_bookable  \\\n",
       "count      22552.000000  22552.000000    22552.000000      22552.000000   \n",
       "mean         118.284454     18.253991        7.157059          0.311901   \n",
       "std          304.232661     28.172816       40.665073          0.463280   \n",
       "min            0.000000      0.000000        1.000000          0.000000   \n",
       "25%            0.000000      0.000000        2.000000          0.000000   \n",
       "50%            0.000000     10.000000        2.000000          0.000000   \n",
       "75%          150.000000     30.000000        4.000000          1.000000   \n",
       "max         4280.000000   2000.000000     5000.000000          1.000000   \n",
       "\n",
       "               wifi  \n",
       "count  22552.000000  \n",
       "mean       0.954106  \n",
       "std        0.209260  \n",
       "min        0.000000  \n",
       "25%        1.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VV5GXjPqZixq"
   },
   "outputs": [],
   "source": [
    "# accomodates = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "# bathrooms = [1.0, 1.5, 2.0]\n",
    "# bedrooms = [0.0, 1.0, 2.0, 3.0]\n",
    "# beds = [1.0, 2.0, 3.0, 4.0]\n",
    "# bed_type = [Real Bed, Pull-out Sofa, Futon, Couch, Airbed]\n",
    "# instant_bookable = [0, 1] <---boolean\n",
    "# minimum_nights = [1, 2, 3, 4, 5, 6, 7]\n",
    "# neighborhood = [Friedrichshain-Kreuzberg, Mitte, Pankow, Neukolln]\n",
    "# room_type = [Private room, Entire home/apt, Shared room]\n",
    "# wifi = [0, 1]  <---boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['price'] <= 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['minimum_nights'] <= 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['accommodates'] <= 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['beds'] <= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['bedrooms'] <= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RL-Vo7UdwrTx"
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "encoder = ce.OrdinalEncoder(cols = [\n",
    "    'neighborhood',\n",
    "    'room_type',\n",
    "    'bed_type'\n",
    "    ])\n",
    "\n",
    "X = df.drop(['price'], axis=1)\n",
    "y = df['price']\n",
    "\n",
    "X_encoded = encoder.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6smS_XoNzKkO",
    "outputId": "d31a6ae5-58ba-4ae4-a758-bd3afded8b2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15667, 12), (5223, 12), (15667,), (5223,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.25)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vlEvZXj_z2yR"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'encoder.pkl'\n",
    "pickle.dump(encoder, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRCgVcrr0Tbv"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "inputs = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "KzyI38gd0dgX",
    "outputId": "3144d7c1-410c-4be6-e034-ac79a66b7280"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(inputs,)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 814
    },
    "colab_type": "code",
    "id": "jeSTACK20kma",
    "outputId": "6e23f53b-1a19-4234-eff2-102bec54fbf9"
   },
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "model.compile(optimizer='adagrad', loss='mae', metrics=['mae'])\n",
    "\n",
    "# Fit Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 10496 samples, validate on 5171 samples\n",
      "Epoch 1/3000\n",
      "10496/10496 [==============================] - 1s 62us/sample - loss: 25.8168 - mae: 25.8168 - val_loss: 21.2947 - val_mae: 21.2947\n",
      "Epoch 2/3000\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 21.4660 - mae: 21.4660 - val_loss: 20.6321 - val_mae: 20.6321\n",
      "Epoch 3/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 20.6735 - mae: 20.6735 - val_loss: 20.2816 - val_mae: 20.2816\n",
      "Epoch 4/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 20.3015 - mae: 20.3015 - val_loss: 20.0880 - val_mae: 20.0880\n",
      "Epoch 5/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 19.9656 - mae: 19.9656 - val_loss: 19.1813 - val_mae: 19.1813\n",
      "Epoch 6/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 19.6604 - mae: 19.6604 - val_loss: 19.0286 - val_mae: 19.0286\n",
      "Epoch 7/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 19.4852 - mae: 19.4852 - val_loss: 18.6952 - val_mae: 18.6952\n",
      "Epoch 8/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 19.2811 - mae: 19.2811 - val_loss: 18.5535 - val_mae: 18.5535\n",
      "Epoch 9/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 19.1590 - mae: 19.1590 - val_loss: 18.4400 - val_mae: 18.4400\n",
      "Epoch 10/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 19.0839 - mae: 19.0839 - val_loss: 18.3692 - val_mae: 18.3692\n",
      "Epoch 11/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 18.9780 - mae: 18.9780 - val_loss: 18.3272 - val_mae: 18.3272\n",
      "Epoch 12/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 18.9147 - mae: 18.9147 - val_loss: 18.3063 - val_mae: 18.3063\n",
      "Epoch 13/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 18.8226 - mae: 18.8226 - val_loss: 18.3081 - val_mae: 18.3081\n",
      "Epoch 14/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 18.8025 - mae: 18.8025 - val_loss: 18.0899 - val_mae: 18.0899\n",
      "Epoch 15/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 18.7485 - mae: 18.7485 - val_loss: 18.0872 - val_mae: 18.0872\n",
      "Epoch 16/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 18.7156 - mae: 18.7156 - val_loss: 18.1384 - val_mae: 18.1384\n",
      "Epoch 17/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 18.6459 - mae: 18.6459 - val_loss: 18.2175 - val_mae: 18.2175\n",
      "Epoch 18/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 18.6354 - mae: 18.6354 - val_loss: 18.2975 - val_mae: 18.2975\n",
      "Epoch 19/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 18.6020 - mae: 18.6020 - val_loss: 17.9481 - val_mae: 17.9481\n",
      "Epoch 20/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 18.5471 - mae: 18.5471 - val_loss: 17.8998 - val_mae: 17.8998\n",
      "Epoch 21/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 18.4982 - mae: 18.4982 - val_loss: 17.8345 - val_mae: 17.8345\n",
      "Epoch 22/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 18.4694 - mae: 18.4694 - val_loss: 17.8932 - val_mae: 17.8932\n",
      "Epoch 23/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 18.4393 - mae: 18.4393 - val_loss: 17.8348 - val_mae: 17.8348\n",
      "Epoch 24/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 18.4151 - mae: 18.4151 - val_loss: 17.7988 - val_mae: 17.7988\n",
      "Epoch 25/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 18.4203 - mae: 18.4203 - val_loss: 17.7334 - val_mae: 17.7335\n",
      "Epoch 26/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 18.3604 - mae: 18.3604 - val_loss: 17.8060 - val_mae: 17.8060\n",
      "Epoch 27/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 18.3619 - mae: 18.3619 - val_loss: 17.7192 - val_mae: 17.7192\n",
      "Epoch 28/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 18.3537 - mae: 18.3536 - val_loss: 17.6720 - val_mae: 17.6720\n",
      "Epoch 29/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 18.3074 - mae: 18.3074 - val_loss: 17.6482 - val_mae: 17.6482\n",
      "Epoch 30/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 18.2968 - mae: 18.2968 - val_loss: 17.6567 - val_mae: 17.6567\n",
      "Epoch 31/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 18.2715 - mae: 18.2715 - val_loss: 17.6427 - val_mae: 17.6427\n",
      "Epoch 32/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 18.2563 - mae: 18.2563 - val_loss: 17.6250 - val_mae: 17.6250\n",
      "Epoch 33/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 18.2445 - mae: 18.2445 - val_loss: 17.5944 - val_mae: 17.5944\n",
      "Epoch 34/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 18.2288 - mae: 18.2288 - val_loss: 17.5841 - val_mae: 17.5841\n",
      "Epoch 35/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 18.2216 - mae: 18.2216 - val_loss: 17.5951 - val_mae: 17.5951\n",
      "Epoch 36/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 18.1987 - mae: 18.1987 - val_loss: 17.5982 - val_mae: 17.5982\n",
      "Epoch 37/3000\n",
      "10496/10496 [==============================] - ETA: 0s - loss: 18.2344 - mae: 18.234 - 0s 46us/sample - loss: 18.1805 - mae: 18.1805 - val_loss: 17.6537 - val_mae: 17.6537\n",
      "Epoch 38/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 18.1625 - mae: 18.1625 - val_loss: 17.5994 - val_mae: 17.5994\n",
      "Epoch 39/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 18.1646 - mae: 18.1646 - val_loss: 17.5335 - val_mae: 17.5335\n",
      "Epoch 40/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 18.1215 - mae: 18.1215 - val_loss: 17.4911 - val_mae: 17.4911\n",
      "Epoch 41/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 18.1163 - mae: 18.1163 - val_loss: 17.4988 - val_mae: 17.4988\n",
      "Epoch 42/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 18.1291 - mae: 18.1291 - val_loss: 17.4996 - val_mae: 17.4996\n",
      "Epoch 43/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 18.1114 - mae: 18.1114 - val_loss: 17.4562 - val_mae: 17.4562\n",
      "Epoch 44/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 18.0999 - mae: 18.0999 - val_loss: 17.4662 - val_mae: 17.4662\n",
      "Epoch 45/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 18.0918 - mae: 18.0918 - val_loss: 17.5520 - val_mae: 17.5520\n",
      "Epoch 46/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 18.0748 - mae: 18.0748 - val_loss: 17.4640 - val_mae: 17.4640\n",
      "Epoch 47/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 18.0610 - mae: 18.0610 - val_loss: 17.4802 - val_mae: 17.4802\n",
      "Epoch 48/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 18.0436 - mae: 18.0436 - val_loss: 17.4704 - val_mae: 17.4704\n",
      "Epoch 49/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 18.0314 - mae: 18.0314 - val_loss: 17.3987 - val_mae: 17.3987\n",
      "Epoch 50/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 18.0204 - mae: 18.0204 - val_loss: 17.3792 - val_mae: 17.3793\n",
      "Epoch 51/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.9903 - mae: 17.9903 - val_loss: 17.4763 - val_mae: 17.4763\n",
      "Epoch 52/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 18.0056 - mae: 18.0056 - val_loss: 17.3848 - val_mae: 17.3848\n",
      "Epoch 53/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.9818 - mae: 17.9818 - val_loss: 17.3917 - val_mae: 17.3917\n",
      "Epoch 54/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.9942 - mae: 17.9942 - val_loss: 17.3455 - val_mae: 17.3455\n",
      "Epoch 55/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.9692 - mae: 17.9692 - val_loss: 17.3734 - val_mae: 17.3734\n",
      "Epoch 56/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.9594 - mae: 17.9594 - val_loss: 17.3289 - val_mae: 17.3289\n",
      "Epoch 57/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.9576 - mae: 17.9576 - val_loss: 17.3310 - val_mae: 17.3310\n",
      "Epoch 58/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.9552 - mae: 17.9552 - val_loss: 17.3456 - val_mae: 17.3456\n",
      "Epoch 59/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.9589 - mae: 17.9589 - val_loss: 17.3025 - val_mae: 17.3025\n",
      "Epoch 60/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.9331 - mae: 17.9331 - val_loss: 17.3974 - val_mae: 17.3974\n",
      "Epoch 61/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.9123 - mae: 17.9123 - val_loss: 17.2892 - val_mae: 17.2892\n",
      "Epoch 62/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.9294 - mae: 17.9294 - val_loss: 17.3067 - val_mae: 17.3067\n",
      "Epoch 63/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.9176 - mae: 17.9176 - val_loss: 17.2975 - val_mae: 17.2975\n",
      "Epoch 64/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.9087 - mae: 17.9087 - val_loss: 17.2903 - val_mae: 17.2903\n",
      "Epoch 65/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.9173 - mae: 17.9174 - val_loss: 17.2686 - val_mae: 17.2686\n",
      "Epoch 66/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.8899 - mae: 17.8899 - val_loss: 17.2966 - val_mae: 17.2966\n",
      "Epoch 67/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.8807 - mae: 17.8807 - val_loss: 17.3105 - val_mae: 17.3105\n",
      "Epoch 68/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.8686 - mae: 17.8686 - val_loss: 17.2663 - val_mae: 17.2663\n",
      "Epoch 69/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.8694 - mae: 17.8694 - val_loss: 17.2613 - val_mae: 17.2613\n",
      "Epoch 70/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.8548 - mae: 17.8548 - val_loss: 17.2787 - val_mae: 17.2787\n",
      "Epoch 71/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.8564 - mae: 17.8564 - val_loss: 17.5386 - val_mae: 17.5386\n",
      "Epoch 72/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.8618 - mae: 17.8618 - val_loss: 17.2992 - val_mae: 17.2992\n",
      "Epoch 73/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.8430 - mae: 17.8430 - val_loss: 17.3174 - val_mae: 17.3174\n",
      "Epoch 74/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.8341 - mae: 17.8341 - val_loss: 17.2199 - val_mae: 17.2199\n",
      "Epoch 75/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.8289 - mae: 17.8289 - val_loss: 17.3250 - val_mae: 17.3250\n",
      "Epoch 76/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.8048 - mae: 17.8048 - val_loss: 17.2022 - val_mae: 17.2022\n",
      "Epoch 77/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.8128 - mae: 17.8128 - val_loss: 17.1960 - val_mae: 17.1960\n",
      "Epoch 78/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.8193 - mae: 17.8193 - val_loss: 17.2163 - val_mae: 17.2163\n",
      "Epoch 79/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.8270 - mae: 17.8270 - val_loss: 17.1990 - val_mae: 17.1990\n",
      "Epoch 80/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.7954 - mae: 17.7954 - val_loss: 17.2203 - val_mae: 17.2203\n",
      "Epoch 81/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.8002 - mae: 17.8002 - val_loss: 17.3413 - val_mae: 17.3413\n",
      "Epoch 82/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.8013 - mae: 17.8013 - val_loss: 17.2226 - val_mae: 17.2226\n",
      "Epoch 83/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.7825 - mae: 17.7825 - val_loss: 17.1695 - val_mae: 17.1695\n",
      "Epoch 84/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.7834 - mae: 17.7834 - val_loss: 17.1986 - val_mae: 17.1986\n",
      "Epoch 85/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.7819 - mae: 17.7819 - val_loss: 17.1526 - val_mae: 17.1526\n",
      "Epoch 86/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.7618 - mae: 17.7618 - val_loss: 17.1529 - val_mae: 17.1529\n",
      "Epoch 87/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.7590 - mae: 17.7590 - val_loss: 17.1457 - val_mae: 17.1457\n",
      "Epoch 88/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.7634 - mae: 17.7634 - val_loss: 17.2003 - val_mae: 17.2003\n",
      "Epoch 89/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.7724 - mae: 17.7724 - val_loss: 17.1627 - val_mae: 17.1627\n",
      "Epoch 90/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.7510 - mae: 17.7510 - val_loss: 17.2124 - val_mae: 17.2124\n",
      "Epoch 91/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.7630 - mae: 17.7630 - val_loss: 17.1446 - val_mae: 17.1446\n",
      "Epoch 92/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.7451 - mae: 17.7451 - val_loss: 17.1339 - val_mae: 17.1339\n",
      "Epoch 93/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.7495 - mae: 17.7495 - val_loss: 17.2402 - val_mae: 17.2402\n",
      "Epoch 94/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.7381 - mae: 17.7381 - val_loss: 17.1738 - val_mae: 17.1738\n",
      "Epoch 95/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.7193 - mae: 17.7193 - val_loss: 17.1309 - val_mae: 17.1309\n",
      "Epoch 96/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.7265 - mae: 17.7265 - val_loss: 17.1255 - val_mae: 17.1255\n",
      "Epoch 97/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.7201 - mae: 17.7201 - val_loss: 17.1567 - val_mae: 17.1567\n",
      "Epoch 98/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6980 - mae: 17.6980 - val_loss: 17.1482 - val_mae: 17.1482\n",
      "Epoch 99/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.7127 - mae: 17.7127 - val_loss: 17.0896 - val_mae: 17.0896\n",
      "Epoch 100/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.7156 - mae: 17.7156 - val_loss: 17.0869 - val_mae: 17.0869\n",
      "Epoch 101/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6986 - mae: 17.6986 - val_loss: 17.1267 - val_mae: 17.1267\n",
      "Epoch 102/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6981 - mae: 17.6981 - val_loss: 17.1424 - val_mae: 17.1424\n",
      "Epoch 103/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.7045 - mae: 17.7045 - val_loss: 17.0863 - val_mae: 17.0863\n",
      "Epoch 104/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.6772 - mae: 17.6772 - val_loss: 17.0730 - val_mae: 17.0730\n",
      "Epoch 105/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6662 - mae: 17.6662 - val_loss: 17.0974 - val_mae: 17.0974\n",
      "Epoch 106/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6834 - mae: 17.6834 - val_loss: 17.0795 - val_mae: 17.0795\n",
      "Epoch 107/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6889 - mae: 17.6889 - val_loss: 17.0688 - val_mae: 17.0688\n",
      "Epoch 108/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6670 - mae: 17.6670 - val_loss: 17.1317 - val_mae: 17.1317\n",
      "Epoch 109/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6798 - mae: 17.6798 - val_loss: 17.0735 - val_mae: 17.0735\n",
      "Epoch 110/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.6611 - mae: 17.6611 - val_loss: 17.1045 - val_mae: 17.1045\n",
      "Epoch 111/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6589 - mae: 17.6589 - val_loss: 17.0625 - val_mae: 17.0625\n",
      "Epoch 112/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.6526 - mae: 17.6526 - val_loss: 17.0661 - val_mae: 17.0661\n",
      "Epoch 113/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.6450 - mae: 17.6451 - val_loss: 17.2682 - val_mae: 17.2682\n",
      "Epoch 114/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6448 - mae: 17.6448 - val_loss: 17.0673 - val_mae: 17.0673\n",
      "Epoch 115/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6428 - mae: 17.6428 - val_loss: 17.0487 - val_mae: 17.0487\n",
      "Epoch 116/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6402 - mae: 17.6402 - val_loss: 17.1171 - val_mae: 17.1171\n",
      "Epoch 117/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6409 - mae: 17.6409 - val_loss: 17.1569 - val_mae: 17.1569\n",
      "Epoch 118/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6384 - mae: 17.6384 - val_loss: 17.0317 - val_mae: 17.0317\n",
      "Epoch 119/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6348 - mae: 17.6348 - val_loss: 17.0923 - val_mae: 17.0923\n",
      "Epoch 120/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6430 - mae: 17.6430 - val_loss: 17.0337 - val_mae: 17.0337\n",
      "Epoch 121/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6205 - mae: 17.6205 - val_loss: 17.0170 - val_mae: 17.0170\n",
      "Epoch 122/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6307 - mae: 17.6307 - val_loss: 17.0130 - val_mae: 17.0130\n",
      "Epoch 123/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6233 - mae: 17.6233 - val_loss: 17.0463 - val_mae: 17.0463\n",
      "Epoch 124/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6182 - mae: 17.6182 - val_loss: 17.0082 - val_mae: 17.0082\n",
      "Epoch 125/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6035 - mae: 17.6035 - val_loss: 17.1392 - val_mae: 17.1392\n",
      "Epoch 126/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.6045 - mae: 17.6045 - val_loss: 17.0336 - val_mae: 17.0336\n",
      "Epoch 127/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.6029 - mae: 17.6029 - val_loss: 17.0048 - val_mae: 17.0048\n",
      "Epoch 128/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5852 - mae: 17.5852 - val_loss: 17.0134 - val_mae: 17.0134\n",
      "Epoch 129/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.6156 - mae: 17.6156 - val_loss: 16.9927 - val_mae: 16.9927\n",
      "Epoch 130/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.5929 - mae: 17.5930 - val_loss: 17.0073 - val_mae: 17.0073\n",
      "Epoch 131/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.5902 - mae: 17.5902 - val_loss: 17.0501 - val_mae: 17.0501\n",
      "Epoch 132/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.5747 - mae: 17.5747 - val_loss: 17.0350 - val_mae: 17.0350\n",
      "Epoch 133/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5801 - mae: 17.5801 - val_loss: 17.0443 - val_mae: 17.0444\n",
      "Epoch 134/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.5941 - mae: 17.5942 - val_loss: 17.0825 - val_mae: 17.0825\n",
      "Epoch 135/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5777 - mae: 17.5777 - val_loss: 17.0514 - val_mae: 17.0514\n",
      "Epoch 136/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.5724 - mae: 17.5723 - val_loss: 16.9742 - val_mae: 16.9742\n",
      "Epoch 137/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.5609 - mae: 17.5609 - val_loss: 16.9825 - val_mae: 16.9825\n",
      "Epoch 138/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5637 - mae: 17.5637 - val_loss: 16.9805 - val_mae: 16.9805\n",
      "Epoch 139/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.5680 - mae: 17.5680 - val_loss: 17.0217 - val_mae: 17.0217\n",
      "Epoch 140/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.5724 - mae: 17.5724 - val_loss: 17.0192 - val_mae: 17.0192\n",
      "Epoch 141/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.5605 - mae: 17.5605 - val_loss: 16.9587 - val_mae: 16.9587\n",
      "Epoch 142/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5611 - mae: 17.5611 - val_loss: 16.9711 - val_mae: 16.9711\n",
      "Epoch 143/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5584 - mae: 17.5584 - val_loss: 16.9771 - val_mae: 16.9771\n",
      "Epoch 144/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5381 - mae: 17.5381 - val_loss: 17.0850 - val_mae: 17.0850\n",
      "Epoch 145/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.5537 - mae: 17.5537 - val_loss: 17.0046 - val_mae: 17.0046\n",
      "Epoch 146/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5551 - mae: 17.5551 - val_loss: 16.9531 - val_mae: 16.9531\n",
      "Epoch 147/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5509 - mae: 17.5509 - val_loss: 16.9796 - val_mae: 16.9796\n",
      "Epoch 148/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.5424 - mae: 17.5424 - val_loss: 16.9493 - val_mae: 16.9493\n",
      "Epoch 149/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.5455 - mae: 17.5455 - val_loss: 16.9379 - val_mae: 16.9379\n",
      "Epoch 150/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5378 - mae: 17.5378 - val_loss: 16.9470 - val_mae: 16.9470\n",
      "Epoch 151/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5264 - mae: 17.5264 - val_loss: 17.1363 - val_mae: 17.1363\n",
      "Epoch 152/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5363 - mae: 17.5363 - val_loss: 17.1850 - val_mae: 17.1850\n",
      "Epoch 153/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5277 - mae: 17.5277 - val_loss: 17.1059 - val_mae: 17.1059\n",
      "Epoch 154/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5239 - mae: 17.5239 - val_loss: 16.9591 - val_mae: 16.9591\n",
      "Epoch 155/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.5149 - mae: 17.5149 - val_loss: 16.9310 - val_mae: 16.9310\n",
      "Epoch 156/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5268 - mae: 17.5268 - val_loss: 16.9598 - val_mae: 16.9598\n",
      "Epoch 157/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5168 - mae: 17.5168 - val_loss: 16.9404 - val_mae: 16.9404\n",
      "Epoch 158/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.5217 - mae: 17.5217 - val_loss: 16.9664 - val_mae: 16.9664\n",
      "Epoch 159/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.5035 - mae: 17.5035 - val_loss: 17.0518 - val_mae: 17.0518\n",
      "Epoch 160/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.5087 - mae: 17.5087 - val_loss: 16.9148 - val_mae: 16.9148\n",
      "Epoch 161/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4990 - mae: 17.4990 - val_loss: 16.9554 - val_mae: 16.9554\n",
      "Epoch 162/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.5024 - mae: 17.5024 - val_loss: 16.9065 - val_mae: 16.9065\n",
      "Epoch 163/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.4890 - mae: 17.4890 - val_loss: 16.9462 - val_mae: 16.9462\n",
      "Epoch 164/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4937 - mae: 17.4937 - val_loss: 16.9174 - val_mae: 16.9174\n",
      "Epoch 165/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4809 - mae: 17.4809 - val_loss: 16.9175 - val_mae: 16.9175\n",
      "Epoch 166/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.4786 - mae: 17.4786 - val_loss: 16.9594 - val_mae: 16.9594\n",
      "Epoch 167/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4898 - mae: 17.4898 - val_loss: 16.9130 - val_mae: 16.9130\n",
      "Epoch 168/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.4891 - mae: 17.4891 - val_loss: 16.8988 - val_mae: 16.8988\n",
      "Epoch 169/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.4912 - mae: 17.4912 - val_loss: 16.8926 - val_mae: 16.8926\n",
      "Epoch 170/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.4744 - mae: 17.4744 - val_loss: 16.8951 - val_mae: 16.8951\n",
      "Epoch 171/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4785 - mae: 17.4785 - val_loss: 16.9605 - val_mae: 16.9605\n",
      "Epoch 172/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.4842 - mae: 17.4842 - val_loss: 16.9405 - val_mae: 16.9405\n",
      "Epoch 173/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4788 - mae: 17.4788 - val_loss: 16.8987 - val_mae: 16.8987\n",
      "Epoch 174/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4784 - mae: 17.4784 - val_loss: 16.9769 - val_mae: 16.9769\n",
      "Epoch 175/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4720 - mae: 17.4720 - val_loss: 16.9820 - val_mae: 16.9820\n",
      "Epoch 176/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4672 - mae: 17.4672 - val_loss: 16.8852 - val_mae: 16.8852\n",
      "Epoch 177/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.4648 - mae: 17.4648 - val_loss: 16.8790 - val_mae: 16.8790\n",
      "Epoch 178/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4544 - mae: 17.4544 - val_loss: 16.9023 - val_mae: 16.9023\n",
      "Epoch 179/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.4620 - mae: 17.4620 - val_loss: 16.8745 - val_mae: 16.8745\n",
      "Epoch 180/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.4633 - mae: 17.4632 - val_loss: 16.8768 - val_mae: 16.8768\n",
      "Epoch 181/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.4587 - mae: 17.4587 - val_loss: 16.9378 - val_mae: 16.9378\n",
      "Epoch 182/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.4480 - mae: 17.4479 - val_loss: 16.8711 - val_mae: 16.8711\n",
      "Epoch 183/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.4585 - mae: 17.4585 - val_loss: 16.8635 - val_mae: 16.8635\n",
      "Epoch 184/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.4437 - mae: 17.4437 - val_loss: 16.8688 - val_mae: 16.8688\n",
      "Epoch 185/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4432 - mae: 17.4431 - val_loss: 16.8749 - val_mae: 16.8749\n",
      "Epoch 186/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4502 - mae: 17.4502 - val_loss: 16.8636 - val_mae: 16.8636\n",
      "Epoch 187/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.4416 - mae: 17.4416 - val_loss: 16.8677 - val_mae: 16.8677\n",
      "Epoch 188/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4333 - mae: 17.4333 - val_loss: 16.8650 - val_mae: 16.8650\n",
      "Epoch 189/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4413 - mae: 17.4413 - val_loss: 16.8871 - val_mae: 16.8871\n",
      "Epoch 190/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.4251 - mae: 17.4252 - val_loss: 16.8804 - val_mae: 16.8804\n",
      "Epoch 191/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.4193 - mae: 17.4193 - val_loss: 16.9231 - val_mae: 16.9231\n",
      "Epoch 192/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.4143 - mae: 17.4143 - val_loss: 17.2334 - val_mae: 17.2335\n",
      "Epoch 193/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.4310 - mae: 17.4310 - val_loss: 16.8985 - val_mae: 16.8985\n",
      "Epoch 194/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4263 - mae: 17.4263 - val_loss: 16.8575 - val_mae: 16.8575\n",
      "Epoch 195/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4149 - mae: 17.4149 - val_loss: 16.8442 - val_mae: 16.8442\n",
      "Epoch 196/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4244 - mae: 17.4244 - val_loss: 16.8677 - val_mae: 16.8677\n",
      "Epoch 197/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4200 - mae: 17.4200 - val_loss: 16.8413 - val_mae: 16.8413\n",
      "Epoch 198/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4116 - mae: 17.4116 - val_loss: 16.8771 - val_mae: 16.8771\n",
      "Epoch 199/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4090 - mae: 17.4089 - val_loss: 16.8377 - val_mae: 16.8377\n",
      "Epoch 200/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.4174 - mae: 17.4174 - val_loss: 16.8440 - val_mae: 16.8440\n",
      "Epoch 201/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3953 - mae: 17.3953 - val_loss: 16.8369 - val_mae: 16.8369\n",
      "Epoch 202/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3918 - mae: 17.3918 - val_loss: 17.0085 - val_mae: 17.0085\n",
      "Epoch 203/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.4016 - mae: 17.4016 - val_loss: 16.8517 - val_mae: 16.8517\n",
      "Epoch 204/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3955 - mae: 17.3955 - val_loss: 16.8894 - val_mae: 16.8894\n",
      "Epoch 205/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.4016 - mae: 17.4016 - val_loss: 16.8275 - val_mae: 16.8275\n",
      "Epoch 206/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.4049 - mae: 17.4049 - val_loss: 16.8207 - val_mae: 16.8207\n",
      "Epoch 207/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.3847 - mae: 17.3847 - val_loss: 16.8421 - val_mae: 16.8421\n",
      "Epoch 208/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.3974 - mae: 17.3975 - val_loss: 16.8497 - val_mae: 16.8497\n",
      "Epoch 209/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3910 - mae: 17.3910 - val_loss: 16.8172 - val_mae: 16.8172\n",
      "Epoch 210/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.3948 - mae: 17.3948 - val_loss: 16.8431 - val_mae: 16.8431\n",
      "Epoch 211/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.3793 - mae: 17.3793 - val_loss: 16.8408 - val_mae: 16.8408\n",
      "Epoch 212/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.3796 - mae: 17.3796 - val_loss: 16.8111 - val_mae: 16.8111\n",
      "Epoch 213/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.3847 - mae: 17.3848 - val_loss: 16.9520 - val_mae: 16.9520\n",
      "Epoch 214/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.3717 - mae: 17.3717 - val_loss: 16.8953 - val_mae: 16.8953\n",
      "Epoch 215/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3806 - mae: 17.3806 - val_loss: 17.0153 - val_mae: 17.0153\n",
      "Epoch 216/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.3808 - mae: 17.3808 - val_loss: 16.9749 - val_mae: 16.9749\n",
      "Epoch 217/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3747 - mae: 17.3747 - val_loss: 16.8126 - val_mae: 16.8126\n",
      "Epoch 218/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3693 - mae: 17.3693 - val_loss: 16.8112 - val_mae: 16.8112\n",
      "Epoch 219/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3870 - mae: 17.3870 - val_loss: 16.8003 - val_mae: 16.8003\n",
      "Epoch 220/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3616 - mae: 17.3615 - val_loss: 16.8316 - val_mae: 16.8315\n",
      "Epoch 221/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3553 - mae: 17.3554 - val_loss: 16.9990 - val_mae: 16.9990\n",
      "Epoch 222/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3725 - mae: 17.3725 - val_loss: 16.7950 - val_mae: 16.7950\n",
      "Epoch 223/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3645 - mae: 17.3645 - val_loss: 16.8363 - val_mae: 16.8362\n",
      "Epoch 224/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3467 - mae: 17.3467 - val_loss: 16.7986 - val_mae: 16.7986\n",
      "Epoch 225/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3582 - mae: 17.3582 - val_loss: 16.8123 - val_mae: 16.8123\n",
      "Epoch 226/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3469 - mae: 17.3469 - val_loss: 16.7936 - val_mae: 16.7936\n",
      "Epoch 227/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.3508 - mae: 17.3508 - val_loss: 16.7931 - val_mae: 16.7931\n",
      "Epoch 228/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 17.3534 - mae: 17.3534 - val_loss: 16.8104 - val_mae: 16.8104\n",
      "Epoch 229/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.3392 - mae: 17.3392 - val_loss: 16.8382 - val_mae: 16.8382\n",
      "Epoch 230/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.3584 - mae: 17.3584 - val_loss: 16.7868 - val_mae: 16.7868\n",
      "Epoch 231/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.3514 - mae: 17.3514 - val_loss: 16.8064 - val_mae: 16.8064\n",
      "Epoch 232/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3487 - mae: 17.3487 - val_loss: 16.7803 - val_mae: 16.7803\n",
      "Epoch 233/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.3384 - mae: 17.3384 - val_loss: 16.7796 - val_mae: 16.7796\n",
      "Epoch 234/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.3404 - mae: 17.3404 - val_loss: 16.8777 - val_mae: 16.8777\n",
      "Epoch 235/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.3449 - mae: 17.3449 - val_loss: 16.8952 - val_mae: 16.8952\n",
      "Epoch 236/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.3447 - mae: 17.3447 - val_loss: 16.7767 - val_mae: 16.7767\n",
      "Epoch 237/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.3373 - mae: 17.3373 - val_loss: 16.7804 - val_mae: 16.7804\n",
      "Epoch 238/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.3376 - mae: 17.3376 - val_loss: 16.7702 - val_mae: 16.7702\n",
      "Epoch 239/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.3455 - mae: 17.3455 - val_loss: 16.8014 - val_mae: 16.8014\n",
      "Epoch 240/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.3367 - mae: 17.3367 - val_loss: 16.7924 - val_mae: 16.7924\n",
      "Epoch 241/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3268 - mae: 17.3268 - val_loss: 16.8405 - val_mae: 16.8405\n",
      "Epoch 242/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3364 - mae: 17.3364 - val_loss: 16.8012 - val_mae: 16.8012\n",
      "Epoch 243/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3248 - mae: 17.3248 - val_loss: 16.8916 - val_mae: 16.8916\n",
      "Epoch 244/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3241 - mae: 17.3242 - val_loss: 16.7626 - val_mae: 16.7626\n",
      "Epoch 245/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3293 - mae: 17.3293 - val_loss: 16.7762 - val_mae: 16.7762\n",
      "Epoch 246/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.3177 - mae: 17.3176 - val_loss: 16.7575 - val_mae: 16.7575\n",
      "Epoch 247/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3290 - mae: 17.3290 - val_loss: 16.8208 - val_mae: 16.8208\n",
      "Epoch 248/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3132 - mae: 17.3132 - val_loss: 16.8311 - val_mae: 16.8311\n",
      "Epoch 249/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.3048 - mae: 17.3048 - val_loss: 16.7558 - val_mae: 16.7558\n",
      "Epoch 250/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3110 - mae: 17.3110 - val_loss: 16.7590 - val_mae: 16.7590\n",
      "Epoch 251/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.3189 - mae: 17.3189 - val_loss: 16.7537 - val_mae: 16.7537\n",
      "Epoch 252/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3185 - mae: 17.3185 - val_loss: 16.7579 - val_mae: 16.7579\n",
      "Epoch 253/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.3091 - mae: 17.3091 - val_loss: 16.8257 - val_mae: 16.8257\n",
      "Epoch 254/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3069 - mae: 17.3069 - val_loss: 16.7719 - val_mae: 16.7719\n",
      "Epoch 255/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2898 - mae: 17.2898 - val_loss: 16.7651 - val_mae: 16.7651\n",
      "Epoch 256/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.3127 - mae: 17.3126 - val_loss: 16.7437 - val_mae: 16.7437\n",
      "Epoch 257/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.3079 - mae: 17.3079 - val_loss: 16.7478 - val_mae: 16.7478\n",
      "Epoch 258/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.3092 - mae: 17.3092 - val_loss: 16.7527 - val_mae: 16.7527\n",
      "Epoch 259/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.3006 - mae: 17.3006 - val_loss: 16.7784 - val_mae: 16.7784\n",
      "Epoch 260/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2987 - mae: 17.2987 - val_loss: 16.7976 - val_mae: 16.7976\n",
      "Epoch 261/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2974 - mae: 17.2974 - val_loss: 16.7657 - val_mae: 16.7657\n",
      "Epoch 262/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2942 - mae: 17.2942 - val_loss: 16.8610 - val_mae: 16.8610\n",
      "Epoch 263/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2808 - mae: 17.2808 - val_loss: 16.7599 - val_mae: 16.7599\n",
      "Epoch 264/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2934 - mae: 17.2934 - val_loss: 16.7706 - val_mae: 16.7706\n",
      "Epoch 265/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2901 - mae: 17.2901 - val_loss: 16.7328 - val_mae: 16.7328\n",
      "Epoch 266/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2889 - mae: 17.2889 - val_loss: 16.8976 - val_mae: 16.8976\n",
      "Epoch 267/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2907 - mae: 17.2907 - val_loss: 16.7375 - val_mae: 16.7375\n",
      "Epoch 268/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2783 - mae: 17.2783 - val_loss: 16.7836 - val_mae: 16.7836\n",
      "Epoch 269/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2790 - mae: 17.2790 - val_loss: 16.7245 - val_mae: 16.7245\n",
      "Epoch 270/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2784 - mae: 17.2784 - val_loss: 16.8149 - val_mae: 16.8149\n",
      "Epoch 271/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2851 - mae: 17.2851 - val_loss: 16.7233 - val_mae: 16.7233\n",
      "Epoch 272/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2863 - mae: 17.2863 - val_loss: 16.8220 - val_mae: 16.8220\n",
      "Epoch 273/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2830 - mae: 17.2830 - val_loss: 16.7231 - val_mae: 16.7231\n",
      "Epoch 274/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2801 - mae: 17.2801 - val_loss: 16.7163 - val_mae: 16.7163\n",
      "Epoch 275/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2675 - mae: 17.2675 - val_loss: 16.7267 - val_mae: 16.7267\n",
      "Epoch 276/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2687 - mae: 17.2687 - val_loss: 16.8017 - val_mae: 16.8017\n",
      "Epoch 277/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2719 - mae: 17.2719 - val_loss: 16.7147 - val_mae: 16.7147\n",
      "Epoch 278/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2728 - mae: 17.2728 - val_loss: 16.7131 - val_mae: 16.7131\n",
      "Epoch 279/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2726 - mae: 17.2726 - val_loss: 16.7117 - val_mae: 16.7117\n",
      "Epoch 280/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2704 - mae: 17.2704 - val_loss: 16.7166 - val_mae: 16.7166\n",
      "Epoch 281/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2639 - mae: 17.2639 - val_loss: 16.7324 - val_mae: 16.7324\n",
      "Epoch 282/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2444 - mae: 17.2444 - val_loss: 16.7434 - val_mae: 16.7434\n",
      "Epoch 283/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2571 - mae: 17.2571 - val_loss: 16.7387 - val_mae: 16.7387\n",
      "Epoch 284/3000\n",
      "10496/10496 [==============================] - ETA: 0s - loss: 17.4330 - mae: 17.433 - 0s 45us/sample - loss: 17.2614 - mae: 17.2614 - val_loss: 16.7155 - val_mae: 16.7155\n",
      "Epoch 285/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2639 - mae: 17.2639 - val_loss: 16.7138 - val_mae: 16.7139\n",
      "Epoch 286/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2519 - mae: 17.2519 - val_loss: 16.7904 - val_mae: 16.7904\n",
      "Epoch 287/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2599 - mae: 17.2599 - val_loss: 16.7031 - val_mae: 16.7031\n",
      "Epoch 288/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2556 - mae: 17.2555 - val_loss: 16.7176 - val_mae: 16.7176\n",
      "Epoch 289/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2493 - mae: 17.2493 - val_loss: 16.7000 - val_mae: 16.7000\n",
      "Epoch 290/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2570 - mae: 17.2571 - val_loss: 16.7108 - val_mae: 16.7108\n",
      "Epoch 291/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2441 - mae: 17.2441 - val_loss: 16.6994 - val_mae: 16.6994\n",
      "Epoch 292/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2489 - mae: 17.2489 - val_loss: 16.7333 - val_mae: 16.7333\n",
      "Epoch 293/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2333 - mae: 17.2333 - val_loss: 16.7274 - val_mae: 16.7274\n",
      "Epoch 294/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2490 - mae: 17.2490 - val_loss: 16.6952 - val_mae: 16.6952\n",
      "Epoch 295/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2363 - mae: 17.2364 - val_loss: 16.7007 - val_mae: 16.7007\n",
      "Epoch 296/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2431 - mae: 17.2431 - val_loss: 16.6919 - val_mae: 16.6919\n",
      "Epoch 297/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2431 - mae: 17.2431 - val_loss: 16.6916 - val_mae: 16.6916\n",
      "Epoch 298/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2354 - mae: 17.2354 - val_loss: 16.7255 - val_mae: 16.7255\n",
      "Epoch 299/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2458 - mae: 17.2458 - val_loss: 16.8215 - val_mae: 16.8215\n",
      "Epoch 300/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2420 - mae: 17.2420 - val_loss: 16.6872 - val_mae: 16.6872\n",
      "Epoch 301/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2430 - mae: 17.2430 - val_loss: 16.6921 - val_mae: 16.6921\n",
      "Epoch 302/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2374 - mae: 17.2374 - val_loss: 16.6896 - val_mae: 16.6896\n",
      "Epoch 303/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2363 - mae: 17.2363 - val_loss: 16.7454 - val_mae: 16.7454\n",
      "Epoch 304/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2328 - mae: 17.2328 - val_loss: 16.7058 - val_mae: 16.7058\n",
      "Epoch 305/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2286 - mae: 17.2286 - val_loss: 16.7111 - val_mae: 16.7111\n",
      "Epoch 306/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2165 - mae: 17.2165 - val_loss: 16.8367 - val_mae: 16.8367\n",
      "Epoch 307/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2274 - mae: 17.2274 - val_loss: 16.6785 - val_mae: 16.6785\n",
      "Epoch 308/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2171 - mae: 17.2171 - val_loss: 16.7342 - val_mae: 16.7342\n",
      "Epoch 309/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2277 - mae: 17.2277 - val_loss: 16.6841 - val_mae: 16.6841\n",
      "Epoch 310/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2206 - mae: 17.2206 - val_loss: 16.6982 - val_mae: 16.6982\n",
      "Epoch 311/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2151 - mae: 17.2151 - val_loss: 16.7876 - val_mae: 16.7876\n",
      "Epoch 312/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2257 - mae: 17.2257 - val_loss: 16.7139 - val_mae: 16.7139\n",
      "Epoch 313/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2178 - mae: 17.2178 - val_loss: 16.7406 - val_mae: 16.7406\n",
      "Epoch 314/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2199 - mae: 17.2199 - val_loss: 16.7946 - val_mae: 16.7946\n",
      "Epoch 315/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2272 - mae: 17.2272 - val_loss: 16.7048 - val_mae: 16.7048\n",
      "Epoch 316/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2117 - mae: 17.2117 - val_loss: 16.6797 - val_mae: 16.6797\n",
      "Epoch 317/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2228 - mae: 17.2228 - val_loss: 16.6740 - val_mae: 16.6740\n",
      "Epoch 318/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2132 - mae: 17.2132 - val_loss: 16.6767 - val_mae: 16.6767\n",
      "Epoch 319/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.2118 - mae: 17.2118 - val_loss: 16.6745 - val_mae: 16.6745\n",
      "Epoch 320/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2168 - mae: 17.2168 - val_loss: 16.6743 - val_mae: 16.6743\n",
      "Epoch 321/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2149 - mae: 17.2149 - val_loss: 16.6898 - val_mae: 16.6898\n",
      "Epoch 322/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2112 - mae: 17.2112 - val_loss: 16.6907 - val_mae: 16.6907\n",
      "Epoch 323/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2059 - mae: 17.2059 - val_loss: 16.6607 - val_mae: 16.6607\n",
      "Epoch 324/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2087 - mae: 17.2087 - val_loss: 16.7555 - val_mae: 16.7556\n",
      "Epoch 325/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2095 - mae: 17.2095 - val_loss: 16.6744 - val_mae: 16.6744\n",
      "Epoch 326/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2066 - mae: 17.2066 - val_loss: 16.6595 - val_mae: 16.6595\n",
      "Epoch 327/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1932 - mae: 17.1932 - val_loss: 16.6663 - val_mae: 16.6663\n",
      "Epoch 328/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2022 - mae: 17.2022 - val_loss: 16.6801 - val_mae: 16.6801\n",
      "Epoch 329/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1979 - mae: 17.1979 - val_loss: 16.6856 - val_mae: 16.6856\n",
      "Epoch 330/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1955 - mae: 17.1955 - val_loss: 16.6540 - val_mae: 16.6540\n",
      "Epoch 331/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2064 - mae: 17.2064 - val_loss: 16.6532 - val_mae: 16.6532\n",
      "Epoch 332/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1980 - mae: 17.1980 - val_loss: 16.7068 - val_mae: 16.7068\n",
      "Epoch 333/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.2001 - mae: 17.2001 - val_loss: 16.6777 - val_mae: 16.6777\n",
      "Epoch 334/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.2006 - mae: 17.2006 - val_loss: 16.6617 - val_mae: 16.6617\n",
      "Epoch 335/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1936 - mae: 17.1936 - val_loss: 16.6678 - val_mae: 16.6678\n",
      "Epoch 336/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1934 - mae: 17.1934 - val_loss: 16.6681 - val_mae: 16.6681\n",
      "Epoch 337/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1985 - mae: 17.1985 - val_loss: 16.7076 - val_mae: 16.7076\n",
      "Epoch 338/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1869 - mae: 17.1869 - val_loss: 16.6566 - val_mae: 16.6566\n",
      "Epoch 339/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1836 - mae: 17.1836 - val_loss: 16.6883 - val_mae: 16.6884\n",
      "Epoch 340/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1918 - mae: 17.1918 - val_loss: 16.6485 - val_mae: 16.6485\n",
      "Epoch 341/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1849 - mae: 17.1849 - val_loss: 16.6726 - val_mae: 16.6726\n",
      "Epoch 342/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1884 - mae: 17.1884 - val_loss: 16.6481 - val_mae: 16.6481\n",
      "Epoch 343/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1875 - mae: 17.1875 - val_loss: 16.6487 - val_mae: 16.6487\n",
      "Epoch 344/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1811 - mae: 17.1811 - val_loss: 16.6838 - val_mae: 16.6838\n",
      "Epoch 345/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1805 - mae: 17.1805 - val_loss: 16.6835 - val_mae: 16.6835\n",
      "Epoch 346/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.1903 - mae: 17.1903 - val_loss: 16.6831 - val_mae: 16.6831\n",
      "Epoch 347/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1822 - mae: 17.1822 - val_loss: 16.7043 - val_mae: 16.7043\n",
      "Epoch 348/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1833 - mae: 17.1833 - val_loss: 16.6516 - val_mae: 16.6516\n",
      "Epoch 349/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1754 - mae: 17.1754 - val_loss: 16.7120 - val_mae: 16.7120\n",
      "Epoch 350/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1688 - mae: 17.1688 - val_loss: 16.7654 - val_mae: 16.7654\n",
      "Epoch 351/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1733 - mae: 17.1733 - val_loss: 16.6451 - val_mae: 16.6451\n",
      "Epoch 352/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.1733 - mae: 17.1733 - val_loss: 16.6841 - val_mae: 16.6841\n",
      "Epoch 353/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1647 - mae: 17.1647 - val_loss: 16.6468 - val_mae: 16.6468\n",
      "Epoch 354/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1724 - mae: 17.1724 - val_loss: 16.6469 - val_mae: 16.6469\n",
      "Epoch 355/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1638 - mae: 17.1638 - val_loss: 16.6328 - val_mae: 16.6328\n",
      "Epoch 356/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1760 - mae: 17.1760 - val_loss: 16.6417 - val_mae: 16.6417\n",
      "Epoch 357/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.1698 - mae: 17.1698 - val_loss: 16.7628 - val_mae: 16.7628\n",
      "Epoch 358/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 17.1555 - mae: 17.1555 - val_loss: 16.6335 - val_mae: 16.6335\n",
      "Epoch 359/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1601 - mae: 17.1601 - val_loss: 16.6533 - val_mae: 16.6533\n",
      "Epoch 360/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1616 - mae: 17.1616 - val_loss: 16.6400 - val_mae: 16.6400\n",
      "Epoch 361/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.1641 - mae: 17.1641 - val_loss: 16.6430 - val_mae: 16.6430\n",
      "Epoch 362/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1592 - mae: 17.1592 - val_loss: 16.6580 - val_mae: 16.6580\n",
      "Epoch 363/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1489 - mae: 17.1489 - val_loss: 16.6827 - val_mae: 16.6827\n",
      "Epoch 364/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1522 - mae: 17.1523 - val_loss: 16.7252 - val_mae: 16.7252\n",
      "Epoch 365/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1655 - mae: 17.1655 - val_loss: 16.6565 - val_mae: 16.6565\n",
      "Epoch 366/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1598 - mae: 17.1598 - val_loss: 16.6681 - val_mae: 16.6681\n",
      "Epoch 367/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1592 - mae: 17.1592 - val_loss: 16.6403 - val_mae: 16.6403\n",
      "Epoch 368/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1528 - mae: 17.1528 - val_loss: 16.6478 - val_mae: 16.6478\n",
      "Epoch 369/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1501 - mae: 17.1501 - val_loss: 16.6973 - val_mae: 16.6973\n",
      "Epoch 370/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1598 - mae: 17.1598 - val_loss: 16.6349 - val_mae: 16.6349\n",
      "Epoch 371/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1530 - mae: 17.1530 - val_loss: 16.7353 - val_mae: 16.7353\n",
      "Epoch 372/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1449 - mae: 17.1449 - val_loss: 16.6161 - val_mae: 16.6161\n",
      "Epoch 373/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1541 - mae: 17.1541 - val_loss: 16.6265 - val_mae: 16.6265\n",
      "Epoch 374/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1473 - mae: 17.1473 - val_loss: 16.6306 - val_mae: 16.6306\n",
      "Epoch 375/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1458 - mae: 17.1458 - val_loss: 16.7103 - val_mae: 16.7103\n",
      "Epoch 376/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1385 - mae: 17.1385 - val_loss: 16.6164 - val_mae: 16.6164\n",
      "Epoch 377/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1434 - mae: 17.1434 - val_loss: 16.6349 - val_mae: 16.6349\n",
      "Epoch 378/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1424 - mae: 17.1424 - val_loss: 16.6296 - val_mae: 16.6296\n",
      "Epoch 379/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1368 - mae: 17.1368 - val_loss: 16.6206 - val_mae: 16.6206\n",
      "Epoch 380/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.1460 - mae: 17.1460 - val_loss: 16.6261 - val_mae: 16.6261\n",
      "Epoch 381/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1482 - mae: 17.1482 - val_loss: 16.6393 - val_mae: 16.6393\n",
      "Epoch 382/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1468 - mae: 17.1468 - val_loss: 16.6146 - val_mae: 16.6146\n",
      "Epoch 383/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1279 - mae: 17.1279 - val_loss: 16.6160 - val_mae: 16.6160\n",
      "Epoch 384/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1339 - mae: 17.1339 - val_loss: 16.6525 - val_mae: 16.6525\n",
      "Epoch 385/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1398 - mae: 17.1398 - val_loss: 16.6140 - val_mae: 16.6140\n",
      "Epoch 386/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1422 - mae: 17.1422 - val_loss: 16.6087 - val_mae: 16.6087\n",
      "Epoch 387/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1284 - mae: 17.1284 - val_loss: 16.6069 - val_mae: 16.6069\n",
      "Epoch 388/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.1414 - mae: 17.1414 - val_loss: 16.6110 - val_mae: 16.6110\n",
      "Epoch 389/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1343 - mae: 17.1343 - val_loss: 16.6457 - val_mae: 16.6457\n",
      "Epoch 390/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.1332 - mae: 17.1332 - val_loss: 16.6374 - val_mae: 16.6374\n",
      "Epoch 391/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.1298 - mae: 17.1298 - val_loss: 16.6296 - val_mae: 16.6296\n",
      "Epoch 392/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1231 - mae: 17.1231 - val_loss: 16.6089 - val_mae: 16.6089\n",
      "Epoch 393/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1252 - mae: 17.1252 - val_loss: 16.6041 - val_mae: 16.6041\n",
      "Epoch 394/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1267 - mae: 17.1267 - val_loss: 16.6225 - val_mae: 16.6225\n",
      "Epoch 395/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1292 - mae: 17.1292 - val_loss: 16.6358 - val_mae: 16.6358\n",
      "Epoch 396/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1245 - mae: 17.1245 - val_loss: 16.6252 - val_mae: 16.6252\n",
      "Epoch 397/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1221 - mae: 17.1221 - val_loss: 16.7292 - val_mae: 16.7292\n",
      "Epoch 398/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1274 - mae: 17.1274 - val_loss: 16.6499 - val_mae: 16.6499\n",
      "Epoch 399/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1255 - mae: 17.1255 - val_loss: 16.6049 - val_mae: 16.6049\n",
      "Epoch 400/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.1252 - mae: 17.1252 - val_loss: 16.5978 - val_mae: 16.5978\n",
      "Epoch 401/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 17.1184 - mae: 17.1184 - val_loss: 16.6190 - val_mae: 16.6190\n",
      "Epoch 402/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1175 - mae: 17.1175 - val_loss: 16.6769 - val_mae: 16.6769\n",
      "Epoch 403/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1239 - mae: 17.1239 - val_loss: 16.6272 - val_mae: 16.6272\n",
      "Epoch 404/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1092 - mae: 17.1092 - val_loss: 16.6329 - val_mae: 16.6329\n",
      "Epoch 405/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1053 - mae: 17.1053 - val_loss: 16.7060 - val_mae: 16.7060\n",
      "Epoch 406/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1254 - mae: 17.1254 - val_loss: 16.5978 - val_mae: 16.5978\n",
      "Epoch 407/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1193 - mae: 17.1193 - val_loss: 16.6258 - val_mae: 16.6258\n",
      "Epoch 408/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1179 - mae: 17.1178 - val_loss: 16.6207 - val_mae: 16.6207\n",
      "Epoch 409/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1094 - mae: 17.1094 - val_loss: 16.5894 - val_mae: 16.5894\n",
      "Epoch 410/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1161 - mae: 17.1161 - val_loss: 16.6085 - val_mae: 16.6085\n",
      "Epoch 411/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1046 - mae: 17.1047 - val_loss: 16.5878 - val_mae: 16.5878\n",
      "Epoch 412/3000\n",
      "10496/10496 [==============================] - 0s 44us/sample - loss: 17.1099 - mae: 17.1099 - val_loss: 16.5880 - val_mae: 16.5880\n",
      "Epoch 413/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1203 - mae: 17.1203 - val_loss: 16.5867 - val_mae: 16.5867\n",
      "Epoch 414/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1132 - mae: 17.1132 - val_loss: 16.5897 - val_mae: 16.5897\n",
      "Epoch 415/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1055 - mae: 17.1055 - val_loss: 16.5860 - val_mae: 16.5860\n",
      "Epoch 416/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0955 - mae: 17.0955 - val_loss: 16.5911 - val_mae: 16.5911\n",
      "Epoch 417/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1021 - mae: 17.1021 - val_loss: 16.5941 - val_mae: 16.5941\n",
      "Epoch 418/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1022 - mae: 17.1022 - val_loss: 16.5864 - val_mae: 16.5864\n",
      "Epoch 419/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1116 - mae: 17.1116 - val_loss: 16.6480 - val_mae: 16.6480\n",
      "Epoch 420/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1108 - mae: 17.1108 - val_loss: 16.5986 - val_mae: 16.5986\n",
      "Epoch 421/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0968 - mae: 17.0968 - val_loss: 16.5870 - val_mae: 16.5870\n",
      "Epoch 422/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.1037 - mae: 17.1037 - val_loss: 16.5828 - val_mae: 16.5828\n",
      "Epoch 423/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.1023 - mae: 17.1023 - val_loss: 16.5858 - val_mae: 16.5857\n",
      "Epoch 424/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0982 - mae: 17.0982 - val_loss: 16.5882 - val_mae: 16.5882\n",
      "Epoch 425/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0912 - mae: 17.0912 - val_loss: 16.5802 - val_mae: 16.5802\n",
      "Epoch 426/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0907 - mae: 17.0907 - val_loss: 16.7610 - val_mae: 16.7610\n",
      "Epoch 427/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0962 - mae: 17.0962 - val_loss: 16.5819 - val_mae: 16.5819\n",
      "Epoch 428/3000\n",
      "10496/10496 [==============================] - 0s 44us/sample - loss: 17.0966 - mae: 17.0966 - val_loss: 16.6626 - val_mae: 16.6626\n",
      "Epoch 429/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0921 - mae: 17.0921 - val_loss: 16.6032 - val_mae: 16.6032\n",
      "Epoch 430/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0968 - mae: 17.0968 - val_loss: 16.5761 - val_mae: 16.5761\n",
      "Epoch 431/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0925 - mae: 17.0925 - val_loss: 16.6140 - val_mae: 16.6140\n",
      "Epoch 432/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0969 - mae: 17.0969 - val_loss: 16.6571 - val_mae: 16.6571\n",
      "Epoch 433/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0934 - mae: 17.0934 - val_loss: 16.6065 - val_mae: 16.6065\n",
      "Epoch 434/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0902 - mae: 17.0902 - val_loss: 16.5765 - val_mae: 16.5765\n",
      "Epoch 435/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0846 - mae: 17.0846 - val_loss: 16.5703 - val_mae: 16.5703\n",
      "Epoch 436/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0827 - mae: 17.0827 - val_loss: 16.5831 - val_mae: 16.5831\n",
      "Epoch 437/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0900 - mae: 17.0900 - val_loss: 16.5973 - val_mae: 16.5973\n",
      "Epoch 438/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0905 - mae: 17.0905 - val_loss: 16.6059 - val_mae: 16.6059\n",
      "Epoch 439/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0810 - mae: 17.0810 - val_loss: 16.5700 - val_mae: 16.5700\n",
      "Epoch 440/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0808 - mae: 17.0808 - val_loss: 16.5893 - val_mae: 16.5893\n",
      "Epoch 441/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0816 - mae: 17.0816 - val_loss: 16.6477 - val_mae: 16.6477\n",
      "Epoch 442/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0840 - mae: 17.0840 - val_loss: 16.5688 - val_mae: 16.5688\n",
      "Epoch 443/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0829 - mae: 17.0829 - val_loss: 16.6245 - val_mae: 16.6245\n",
      "Epoch 444/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.0829 - mae: 17.0829 - val_loss: 16.5686 - val_mae: 16.5686\n",
      "Epoch 445/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0772 - mae: 17.0772 - val_loss: 16.5710 - val_mae: 16.5710\n",
      "Epoch 446/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0771 - mae: 17.0771 - val_loss: 16.5664 - val_mae: 16.5664\n",
      "Epoch 447/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0836 - mae: 17.0836 - val_loss: 16.5750 - val_mae: 16.5750\n",
      "Epoch 448/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0802 - mae: 17.0802 - val_loss: 16.5637 - val_mae: 16.5637\n",
      "Epoch 449/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0768 - mae: 17.0768 - val_loss: 16.6377 - val_mae: 16.6377\n",
      "Epoch 450/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0721 - mae: 17.0721 - val_loss: 16.5893 - val_mae: 16.5893\n",
      "Epoch 451/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0811 - mae: 17.0811 - val_loss: 16.6292 - val_mae: 16.6292\n",
      "Epoch 452/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0708 - mae: 17.0708 - val_loss: 16.6510 - val_mae: 16.6510\n",
      "Epoch 453/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0774 - mae: 17.0774 - val_loss: 16.5590 - val_mae: 16.5590\n",
      "Epoch 454/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0667 - mae: 17.0667 - val_loss: 16.6453 - val_mae: 16.6453\n",
      "Epoch 455/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0649 - mae: 17.0649 - val_loss: 16.5906 - val_mae: 16.5906\n",
      "Epoch 456/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0758 - mae: 17.0758 - val_loss: 16.5587 - val_mae: 16.5587\n",
      "Epoch 457/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0700 - mae: 17.0700 - val_loss: 16.5675 - val_mae: 16.5675\n",
      "Epoch 458/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0730 - mae: 17.0730 - val_loss: 16.5547 - val_mae: 16.5547\n",
      "Epoch 459/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0738 - mae: 17.0738 - val_loss: 16.5581 - val_mae: 16.5581\n",
      "Epoch 460/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0661 - mae: 17.0661 - val_loss: 16.6070 - val_mae: 16.6070\n",
      "Epoch 461/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0659 - mae: 17.0659 - val_loss: 16.5566 - val_mae: 16.5566\n",
      "Epoch 462/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0644 - mae: 17.0644 - val_loss: 16.5515 - val_mae: 16.5515\n",
      "Epoch 463/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0627 - mae: 17.0627 - val_loss: 16.5529 - val_mae: 16.5529\n",
      "Epoch 464/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0644 - mae: 17.0644 - val_loss: 16.5731 - val_mae: 16.5731\n",
      "Epoch 465/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0692 - mae: 17.0692 - val_loss: 16.6033 - val_mae: 16.6033\n",
      "Epoch 466/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0612 - mae: 17.0612 - val_loss: 16.5514 - val_mae: 16.5514\n",
      "Epoch 467/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0615 - mae: 17.0615 - val_loss: 16.5867 - val_mae: 16.5867\n",
      "Epoch 468/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0623 - mae: 17.0623 - val_loss: 16.5532 - val_mae: 16.5532\n",
      "Epoch 469/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0634 - mae: 17.0634 - val_loss: 16.5871 - val_mae: 16.5871\n",
      "Epoch 470/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0627 - mae: 17.0627 - val_loss: 16.6218 - val_mae: 16.6218\n",
      "Epoch 471/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0492 - mae: 17.0492 - val_loss: 16.5601 - val_mae: 16.5601\n",
      "Epoch 472/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0546 - mae: 17.0546 - val_loss: 16.5722 - val_mae: 16.5722\n",
      "Epoch 473/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0614 - mae: 17.0614 - val_loss: 16.5490 - val_mae: 16.5490\n",
      "Epoch 474/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0537 - mae: 17.0537 - val_loss: 16.5513 - val_mae: 16.5513\n",
      "Epoch 475/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0594 - mae: 17.0594 - val_loss: 16.5442 - val_mae: 16.5442\n",
      "Epoch 476/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0591 - mae: 17.0591 - val_loss: 16.5459 - val_mae: 16.5459\n",
      "Epoch 477/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0593 - mae: 17.0593 - val_loss: 16.5876 - val_mae: 16.5877\n",
      "Epoch 478/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0575 - mae: 17.0575 - val_loss: 16.5783 - val_mae: 16.5783\n",
      "Epoch 479/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0471 - mae: 17.0471 - val_loss: 16.5502 - val_mae: 16.5502\n",
      "Epoch 480/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0521 - mae: 17.0521 - val_loss: 16.5484 - val_mae: 16.5483\n",
      "Epoch 481/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.0495 - mae: 17.0495 - val_loss: 16.5657 - val_mae: 16.5657\n",
      "Epoch 482/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0509 - mae: 17.0509 - val_loss: 16.5835 - val_mae: 16.5835\n",
      "Epoch 483/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0565 - mae: 17.0565 - val_loss: 16.5385 - val_mae: 16.5385\n",
      "Epoch 484/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0418 - mae: 17.0418 - val_loss: 16.5493 - val_mae: 16.5493\n",
      "Epoch 485/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.0520 - mae: 17.0520 - val_loss: 16.5367 - val_mae: 16.5367\n",
      "Epoch 486/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.0407 - mae: 17.0407 - val_loss: 16.6173 - val_mae: 16.6173\n",
      "Epoch 487/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 17.0497 - mae: 17.0497 - val_loss: 16.5396 - val_mae: 16.5396\n",
      "Epoch 488/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 17.0470 - mae: 17.0471 - val_loss: 16.5330 - val_mae: 16.5330\n",
      "Epoch 489/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.0435 - mae: 17.0435 - val_loss: 16.5373 - val_mae: 16.5373\n",
      "Epoch 490/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0405 - mae: 17.0405 - val_loss: 16.5778 - val_mae: 16.5778\n",
      "Epoch 491/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0461 - mae: 17.0461 - val_loss: 16.5343 - val_mae: 16.5343\n",
      "Epoch 492/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0459 - mae: 17.0459 - val_loss: 16.5924 - val_mae: 16.5924\n",
      "Epoch 493/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.0420 - mae: 17.0420 - val_loss: 16.5318 - val_mae: 16.5318\n",
      "Epoch 494/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 17.0421 - mae: 17.0421 - val_loss: 16.5319 - val_mae: 16.5319\n",
      "Epoch 495/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0444 - mae: 17.0444 - val_loss: 16.5311 - val_mae: 16.5311\n",
      "Epoch 496/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.0445 - mae: 17.0445 - val_loss: 16.5393 - val_mae: 16.5393\n",
      "Epoch 497/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.0371 - mae: 17.0371 - val_loss: 16.5971 - val_mae: 16.5971\n",
      "Epoch 498/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0410 - mae: 17.0410 - val_loss: 16.6197 - val_mae: 16.6197\n",
      "Epoch 499/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0383 - mae: 17.0383 - val_loss: 16.5726 - val_mae: 16.5726\n",
      "Epoch 500/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0363 - mae: 17.0363 - val_loss: 16.6553 - val_mae: 16.6553\n",
      "Epoch 501/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0289 - mae: 17.0289 - val_loss: 16.5623 - val_mae: 16.5623\n",
      "Epoch 502/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0339 - mae: 17.0339 - val_loss: 16.5657 - val_mae: 16.5657\n",
      "Epoch 503/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0308 - mae: 17.0308 - val_loss: 16.5347 - val_mae: 16.5347\n",
      "Epoch 504/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0234 - mae: 17.0234 - val_loss: 16.5324 - val_mae: 16.5324\n",
      "Epoch 505/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0380 - mae: 17.0380 - val_loss: 16.5354 - val_mae: 16.5354\n",
      "Epoch 506/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0309 - mae: 17.0309 - val_loss: 16.5792 - val_mae: 16.5792\n",
      "Epoch 507/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.0297 - mae: 17.0297 - val_loss: 16.5320 - val_mae: 16.5320\n",
      "Epoch 508/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0355 - mae: 17.0355 - val_loss: 16.5225 - val_mae: 16.5225\n",
      "Epoch 509/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0258 - mae: 17.0258 - val_loss: 16.5827 - val_mae: 16.5827\n",
      "Epoch 510/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0413 - mae: 17.0413 - val_loss: 16.5219 - val_mae: 16.5219\n",
      "Epoch 511/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0263 - mae: 17.0263 - val_loss: 16.5539 - val_mae: 16.5539\n",
      "Epoch 512/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0201 - mae: 17.0201 - val_loss: 16.5716 - val_mae: 16.5716\n",
      "Epoch 513/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0224 - mae: 17.0224 - val_loss: 16.6420 - val_mae: 16.6420\n",
      "Epoch 514/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0231 - mae: 17.0231 - val_loss: 16.5320 - val_mae: 16.5320\n",
      "Epoch 515/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0246 - mae: 17.0246 - val_loss: 16.5236 - val_mae: 16.5236\n",
      "Epoch 516/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.0205 - mae: 17.0205 - val_loss: 16.5412 - val_mae: 16.5412\n",
      "Epoch 517/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.0243 - mae: 17.0243 - val_loss: 16.5297 - val_mae: 16.5297\n",
      "Epoch 518/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 17.0345 - mae: 17.0345 - val_loss: 16.5282 - val_mae: 16.5282\n",
      "Epoch 519/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.0272 - mae: 17.0272 - val_loss: 16.5250 - val_mae: 16.5250\n",
      "Epoch 520/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0149 - mae: 17.0149 - val_loss: 16.5986 - val_mae: 16.5986\n",
      "Epoch 521/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0240 - mae: 17.0240 - val_loss: 16.5203 - val_mae: 16.5203\n",
      "Epoch 522/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 17.0182 - mae: 17.0182 - val_loss: 16.5615 - val_mae: 16.5615\n",
      "Epoch 523/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0222 - mae: 17.0222 - val_loss: 16.5238 - val_mae: 16.5238\n",
      "Epoch 524/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.0122 - mae: 17.0122 - val_loss: 16.5473 - val_mae: 16.5473\n",
      "Epoch 525/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.0199 - mae: 17.0199 - val_loss: 16.5178 - val_mae: 16.5178\n",
      "Epoch 526/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 17.0088 - mae: 17.0088 - val_loss: 16.5192 - val_mae: 16.5192\n",
      "Epoch 527/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.0141 - mae: 17.0141 - val_loss: 16.5611 - val_mae: 16.5611\n",
      "Epoch 528/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.0187 - mae: 17.0187 - val_loss: 16.5392 - val_mae: 16.5392\n",
      "Epoch 529/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.0223 - mae: 17.0223 - val_loss: 16.5157 - val_mae: 16.5157\n",
      "Epoch 530/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.0175 - mae: 17.0175 - val_loss: 16.5310 - val_mae: 16.5309\n",
      "Epoch 531/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.0011 - mae: 17.0011 - val_loss: 16.6807 - val_mae: 16.6807\n",
      "Epoch 532/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.0101 - mae: 17.0101 - val_loss: 16.5177 - val_mae: 16.5177\n",
      "Epoch 533/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.0180 - mae: 17.0180 - val_loss: 16.5472 - val_mae: 16.5472\n",
      "Epoch 534/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.0027 - mae: 17.0027 - val_loss: 16.5216 - val_mae: 16.5216\n",
      "Epoch 535/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0057 - mae: 17.0057 - val_loss: 16.5076 - val_mae: 16.5076\n",
      "Epoch 536/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0200 - mae: 17.0200 - val_loss: 16.5072 - val_mae: 16.5072\n",
      "Epoch 537/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.0132 - mae: 17.0132 - val_loss: 16.5133 - val_mae: 16.5133\n",
      "Epoch 538/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.0036 - mae: 17.0036 - val_loss: 16.5169 - val_mae: 16.5169\n",
      "Epoch 539/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 17.0030 - mae: 17.0030 - val_loss: 16.5115 - val_mae: 16.5115\n",
      "Epoch 540/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.0105 - mae: 17.0105 - val_loss: 16.5038 - val_mae: 16.5038\n",
      "Epoch 541/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.0084 - mae: 17.0084 - val_loss: 16.5470 - val_mae: 16.5470\n",
      "Epoch 542/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 17.0046 - mae: 17.0046 - val_loss: 16.5324 - val_mae: 16.5324\n",
      "Epoch 543/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 17.0013 - mae: 17.0013 - val_loss: 16.5063 - val_mae: 16.5063\n",
      "Epoch 544/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9977 - mae: 16.9977 - val_loss: 16.5544 - val_mae: 16.5544\n",
      "Epoch 545/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 17.0106 - mae: 17.0106 - val_loss: 16.5593 - val_mae: 16.5593\n",
      "Epoch 546/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9823 - mae: 16.9823 - val_loss: 16.5938 - val_mae: 16.5937\n",
      "Epoch 547/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.0042 - mae: 17.0042 - val_loss: 16.5049 - val_mae: 16.5049\n",
      "Epoch 548/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.0063 - mae: 17.0063 - val_loss: 16.5092 - val_mae: 16.5092\n",
      "Epoch 549/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9988 - mae: 16.9988 - val_loss: 16.4991 - val_mae: 16.4990\n",
      "Epoch 550/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9970 - mae: 16.9970 - val_loss: 16.4960 - val_mae: 16.4960\n",
      "Epoch 551/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9969 - mae: 16.9969 - val_loss: 16.5450 - val_mae: 16.5450\n",
      "Epoch 552/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9997 - mae: 16.9997 - val_loss: 16.5024 - val_mae: 16.5024\n",
      "Epoch 553/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.0006 - mae: 17.0006 - val_loss: 16.4987 - val_mae: 16.4987\n",
      "Epoch 554/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.9972 - mae: 16.9972 - val_loss: 16.4987 - val_mae: 16.4987\n",
      "Epoch 555/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9920 - mae: 16.9920 - val_loss: 16.6388 - val_mae: 16.6388\n",
      "Epoch 556/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9838 - mae: 16.9838 - val_loss: 16.5570 - val_mae: 16.5570\n",
      "Epoch 557/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9972 - mae: 16.9972 - val_loss: 16.5600 - val_mae: 16.5600\n",
      "Epoch 558/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 17.0031 - mae: 17.0031 - val_loss: 16.5384 - val_mae: 16.5384\n",
      "Epoch 559/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9970 - mae: 16.9970 - val_loss: 16.4996 - val_mae: 16.4996\n",
      "Epoch 560/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 17.0018 - mae: 17.0018 - val_loss: 16.4916 - val_mae: 16.4916\n",
      "Epoch 561/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9899 - mae: 16.9899 - val_loss: 16.6936 - val_mae: 16.6936\n",
      "Epoch 562/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9954 - mae: 16.9954 - val_loss: 16.5069 - val_mae: 16.5069\n",
      "Epoch 563/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9832 - mae: 16.9832 - val_loss: 16.4922 - val_mae: 16.4922\n",
      "Epoch 564/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9818 - mae: 16.9818 - val_loss: 16.6031 - val_mae: 16.6031\n",
      "Epoch 565/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9876 - mae: 16.9876 - val_loss: 16.4974 - val_mae: 16.4974\n",
      "Epoch 566/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9830 - mae: 16.9830 - val_loss: 16.5097 - val_mae: 16.5097\n",
      "Epoch 567/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9937 - mae: 16.9937 - val_loss: 16.5133 - val_mae: 16.5133\n",
      "Epoch 568/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9896 - mae: 16.9896 - val_loss: 16.5113 - val_mae: 16.5113\n",
      "Epoch 569/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9937 - mae: 16.9937 - val_loss: 16.4878 - val_mae: 16.4878\n",
      "Epoch 570/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9877 - mae: 16.9877 - val_loss: 16.5084 - val_mae: 16.5084\n",
      "Epoch 571/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9859 - mae: 16.9859 - val_loss: 16.4977 - val_mae: 16.4977\n",
      "Epoch 572/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.9916 - mae: 16.9916 - val_loss: 16.5153 - val_mae: 16.5153\n",
      "Epoch 573/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9840 - mae: 16.9840 - val_loss: 16.4895 - val_mae: 16.4895\n",
      "Epoch 574/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9870 - mae: 16.9870 - val_loss: 16.4876 - val_mae: 16.4876\n",
      "Epoch 575/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9873 - mae: 16.9873 - val_loss: 16.4942 - val_mae: 16.4942\n",
      "Epoch 576/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9927 - mae: 16.9927 - val_loss: 16.4904 - val_mae: 16.4904\n",
      "Epoch 577/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9820 - mae: 16.9820 - val_loss: 16.4860 - val_mae: 16.4860\n",
      "Epoch 578/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9785 - mae: 16.9785 - val_loss: 16.5426 - val_mae: 16.5426\n",
      "Epoch 579/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9789 - mae: 16.9789 - val_loss: 16.4829 - val_mae: 16.4829\n",
      "Epoch 580/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9760 - mae: 16.9760 - val_loss: 16.4997 - val_mae: 16.4997\n",
      "Epoch 581/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.9736 - mae: 16.9736 - val_loss: 16.5119 - val_mae: 16.5119\n",
      "Epoch 582/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.9798 - mae: 16.9798 - val_loss: 16.4845 - val_mae: 16.4845\n",
      "Epoch 583/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.9798 - mae: 16.9798 - val_loss: 16.5099 - val_mae: 16.5099\n",
      "Epoch 584/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9741 - mae: 16.9741 - val_loss: 16.5646 - val_mae: 16.5646\n",
      "Epoch 585/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9794 - mae: 16.9794 - val_loss: 16.4804 - val_mae: 16.4804\n",
      "Epoch 586/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.9774 - mae: 16.9774 - val_loss: 16.5245 - val_mae: 16.5245\n",
      "Epoch 587/3000\n",
      "10496/10496 [==============================] - 1s 60us/sample - loss: 16.9770 - mae: 16.9770 - val_loss: 16.4833 - val_mae: 16.4833\n",
      "Epoch 588/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.9785 - mae: 16.9785 - val_loss: 16.4880 - val_mae: 16.4880\n",
      "Epoch 589/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9789 - mae: 16.9789 - val_loss: 16.4836 - val_mae: 16.4836\n",
      "Epoch 590/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9784 - mae: 16.9784 - val_loss: 16.4834 - val_mae: 16.4834\n",
      "Epoch 591/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9773 - mae: 16.9773 - val_loss: 16.4833 - val_mae: 16.4833\n",
      "Epoch 592/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.9731 - mae: 16.9731 - val_loss: 16.4943 - val_mae: 16.4943\n",
      "Epoch 593/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9663 - mae: 16.9663 - val_loss: 16.4909 - val_mae: 16.4909\n",
      "Epoch 594/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9730 - mae: 16.9730 - val_loss: 16.4980 - val_mae: 16.4980\n",
      "Epoch 595/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.9659 - mae: 16.9659 - val_loss: 16.5741 - val_mae: 16.5741\n",
      "Epoch 596/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.9639 - mae: 16.9639 - val_loss: 16.5655 - val_mae: 16.5655\n",
      "Epoch 597/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.9717 - mae: 16.9717 - val_loss: 16.4912 - val_mae: 16.4912\n",
      "Epoch 598/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9718 - mae: 16.9718 - val_loss: 16.4808 - val_mae: 16.4808\n",
      "Epoch 599/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.9686 - mae: 16.9686 - val_loss: 16.4828 - val_mae: 16.4828\n",
      "Epoch 600/3000\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 16.9702 - mae: 16.9702 - val_loss: 16.5062 - val_mae: 16.5062\n",
      "Epoch 601/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9645 - mae: 16.9645 - val_loss: 16.4706 - val_mae: 16.4706\n",
      "Epoch 602/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.9684 - mae: 16.9684 - val_loss: 16.4713 - val_mae: 16.4713\n",
      "Epoch 603/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9597 - mae: 16.9597 - val_loss: 16.5134 - val_mae: 16.5134\n",
      "Epoch 604/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9633 - mae: 16.9632 - val_loss: 16.4873 - val_mae: 16.4873\n",
      "Epoch 605/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9697 - mae: 16.9697 - val_loss: 16.4758 - val_mae: 16.4758\n",
      "Epoch 606/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9685 - mae: 16.9685 - val_loss: 16.4900 - val_mae: 16.4900\n",
      "Epoch 607/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9595 - mae: 16.9595 - val_loss: 16.4665 - val_mae: 16.4665\n",
      "Epoch 608/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9613 - mae: 16.9613 - val_loss: 16.5423 - val_mae: 16.5423\n",
      "Epoch 609/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9650 - mae: 16.9650 - val_loss: 16.4733 - val_mae: 16.4733\n",
      "Epoch 610/3000\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 16.9604 - mae: 16.9604 - val_loss: 16.5070 - val_mae: 16.5069\n",
      "Epoch 611/3000\n",
      "10496/10496 [==============================] - 1s 61us/sample - loss: 16.9683 - mae: 16.9683 - val_loss: 16.4684 - val_mae: 16.4684\n",
      "Epoch 612/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.9571 - mae: 16.9571 - val_loss: 16.4667 - val_mae: 16.4667\n",
      "Epoch 613/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9552 - mae: 16.9552 - val_loss: 16.4729 - val_mae: 16.4729\n",
      "Epoch 614/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.9598 - mae: 16.9598 - val_loss: 16.4649 - val_mae: 16.4649\n",
      "Epoch 615/3000\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 16.9630 - mae: 16.9630 - val_loss: 16.4626 - val_mae: 16.4626\n",
      "Epoch 616/3000\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 16.9592 - mae: 16.9592 - val_loss: 16.4941 - val_mae: 16.4941\n",
      "Epoch 617/3000\n",
      "10496/10496 [==============================] - 1s 60us/sample - loss: 16.9600 - mae: 16.9600 - val_loss: 16.4969 - val_mae: 16.4969\n",
      "Epoch 618/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9539 - mae: 16.9539 - val_loss: 16.4678 - val_mae: 16.4678\n",
      "Epoch 619/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9506 - mae: 16.9506 - val_loss: 16.4640 - val_mae: 16.4640\n",
      "Epoch 620/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9489 - mae: 16.9489 - val_loss: 16.4817 - val_mae: 16.4817\n",
      "Epoch 621/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9538 - mae: 16.9538 - val_loss: 16.5506 - val_mae: 16.5506\n",
      "Epoch 622/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9513 - mae: 16.9513 - val_loss: 16.4581 - val_mae: 16.4581\n",
      "Epoch 623/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.9490 - mae: 16.9490 - val_loss: 16.4880 - val_mae: 16.4880\n",
      "Epoch 624/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9463 - mae: 16.9463 - val_loss: 16.6594 - val_mae: 16.6594\n",
      "Epoch 625/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9425 - mae: 16.9425 - val_loss: 16.5444 - val_mae: 16.5444\n",
      "Epoch 626/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9581 - mae: 16.9581 - val_loss: 16.4841 - val_mae: 16.4841\n",
      "Epoch 627/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.9381 - mae: 16.9381 - val_loss: 16.5644 - val_mae: 16.5644\n",
      "Epoch 628/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.9567 - mae: 16.9567 - val_loss: 16.4645 - val_mae: 16.4645\n",
      "Epoch 629/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9503 - mae: 16.9503 - val_loss: 16.4944 - val_mae: 16.4944\n",
      "Epoch 630/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9499 - mae: 16.9499 - val_loss: 16.4779 - val_mae: 16.4779\n",
      "Epoch 631/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.9539 - mae: 16.9539 - val_loss: 16.4759 - val_mae: 16.4759\n",
      "Epoch 632/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.9389 - mae: 16.9389 - val_loss: 16.4583 - val_mae: 16.4583\n",
      "Epoch 633/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9455 - mae: 16.9455 - val_loss: 16.5195 - val_mae: 16.5195\n",
      "Epoch 634/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.9493 - mae: 16.9493 - val_loss: 16.4598 - val_mae: 16.4598\n",
      "Epoch 635/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.9489 - mae: 16.9489 - val_loss: 16.4871 - val_mae: 16.4871\n",
      "Epoch 636/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9482 - mae: 16.9482 - val_loss: 16.4581 - val_mae: 16.4580\n",
      "Epoch 637/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9450 - mae: 16.9450 - val_loss: 16.4587 - val_mae: 16.4587\n",
      "Epoch 638/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9494 - mae: 16.9494 - val_loss: 16.4616 - val_mae: 16.4616\n",
      "Epoch 639/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9471 - mae: 16.9471 - val_loss: 16.4496 - val_mae: 16.4496\n",
      "Epoch 640/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9311 - mae: 16.9311 - val_loss: 16.4608 - val_mae: 16.4608\n",
      "Epoch 641/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9387 - mae: 16.9387 - val_loss: 16.5090 - val_mae: 16.5090\n",
      "Epoch 642/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.9382 - mae: 16.9382 - val_loss: 16.5015 - val_mae: 16.5015\n",
      "Epoch 643/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9471 - mae: 16.9471 - val_loss: 16.4748 - val_mae: 16.4748\n",
      "Epoch 644/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.9424 - mae: 16.9424 - val_loss: 16.4555 - val_mae: 16.4555\n",
      "Epoch 645/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9323 - mae: 16.9323 - val_loss: 16.4682 - val_mae: 16.4682\n",
      "Epoch 646/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9414 - mae: 16.9414 - val_loss: 16.5185 - val_mae: 16.5185\n",
      "Epoch 647/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9291 - mae: 16.9291 - val_loss: 16.4577 - val_mae: 16.4577\n",
      "Epoch 648/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9316 - mae: 16.9316 - val_loss: 16.4546 - val_mae: 16.4546\n",
      "Epoch 649/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9369 - mae: 16.9369 - val_loss: 16.4729 - val_mae: 16.4729\n",
      "Epoch 650/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.9259 - mae: 16.9259 - val_loss: 16.5589 - val_mae: 16.5589\n",
      "Epoch 651/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9408 - mae: 16.9409 - val_loss: 16.4500 - val_mae: 16.4500\n",
      "Epoch 652/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9334 - mae: 16.9333 - val_loss: 16.5129 - val_mae: 16.5129\n",
      "Epoch 653/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.9342 - mae: 16.9342 - val_loss: 16.4696 - val_mae: 16.4696\n",
      "Epoch 654/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.9265 - mae: 16.9265 - val_loss: 16.4451 - val_mae: 16.4451\n",
      "Epoch 655/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.9382 - mae: 16.9382 - val_loss: 16.4660 - val_mae: 16.4660\n",
      "Epoch 656/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9234 - mae: 16.9234 - val_loss: 16.4450 - val_mae: 16.4450\n",
      "Epoch 657/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.9313 - mae: 16.9313 - val_loss: 16.4712 - val_mae: 16.4712\n",
      "Epoch 658/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.9312 - mae: 16.9312 - val_loss: 16.4456 - val_mae: 16.4456\n",
      "Epoch 659/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.9267 - mae: 16.9267 - val_loss: 16.4577 - val_mae: 16.4577\n",
      "Epoch 660/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.9381 - mae: 16.9381 - val_loss: 16.4755 - val_mae: 16.4755\n",
      "Epoch 661/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.9304 - mae: 16.9304 - val_loss: 16.4493 - val_mae: 16.4493\n",
      "Epoch 662/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.9196 - mae: 16.9196 - val_loss: 16.4459 - val_mae: 16.4459\n",
      "Epoch 663/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9238 - mae: 16.9238 - val_loss: 16.5415 - val_mae: 16.5415\n",
      "Epoch 664/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.9347 - mae: 16.9347 - val_loss: 16.4417 - val_mae: 16.4417\n",
      "Epoch 665/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9213 - mae: 16.9213 - val_loss: 16.4624 - val_mae: 16.4624\n",
      "Epoch 666/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9219 - mae: 16.9219 - val_loss: 16.5381 - val_mae: 16.5381\n",
      "Epoch 667/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9293 - mae: 16.9293 - val_loss: 16.5303 - val_mae: 16.5303\n",
      "Epoch 668/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9303 - mae: 16.9304 - val_loss: 16.5224 - val_mae: 16.5224\n",
      "Epoch 669/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9349 - mae: 16.9349 - val_loss: 16.4402 - val_mae: 16.4402\n",
      "Epoch 670/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9167 - mae: 16.9167 - val_loss: 16.6678 - val_mae: 16.6678\n",
      "Epoch 671/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9207 - mae: 16.9207 - val_loss: 16.4652 - val_mae: 16.4652\n",
      "Epoch 672/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9319 - mae: 16.9319 - val_loss: 16.4543 - val_mae: 16.4543\n",
      "Epoch 673/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9290 - mae: 16.9290 - val_loss: 16.4530 - val_mae: 16.4530\n",
      "Epoch 674/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9212 - mae: 16.9212 - val_loss: 16.4468 - val_mae: 16.4468\n",
      "Epoch 675/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9195 - mae: 16.9195 - val_loss: 16.4589 - val_mae: 16.4589\n",
      "Epoch 676/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9228 - mae: 16.9228 - val_loss: 16.5292 - val_mae: 16.5292\n",
      "Epoch 677/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9202 - mae: 16.9202 - val_loss: 16.4501 - val_mae: 16.4501\n",
      "Epoch 678/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9227 - mae: 16.9227 - val_loss: 16.4606 - val_mae: 16.4606\n",
      "Epoch 679/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9182 - mae: 16.9182 - val_loss: 16.4496 - val_mae: 16.4496\n",
      "Epoch 680/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9289 - mae: 16.9289 - val_loss: 16.4555 - val_mae: 16.4555\n",
      "Epoch 681/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9045 - mae: 16.9045 - val_loss: 16.5339 - val_mae: 16.5340\n",
      "Epoch 682/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9132 - mae: 16.9132 - val_loss: 16.4531 - val_mae: 16.4531\n",
      "Epoch 683/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9179 - mae: 16.9179 - val_loss: 16.4382 - val_mae: 16.4382\n",
      "Epoch 684/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9184 - mae: 16.9184 - val_loss: 16.4524 - val_mae: 16.4524\n",
      "Epoch 685/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9099 - mae: 16.9099 - val_loss: 16.4408 - val_mae: 16.4408\n",
      "Epoch 686/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9231 - mae: 16.9231 - val_loss: 16.4362 - val_mae: 16.4362\n",
      "Epoch 687/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9231 - mae: 16.9231 - val_loss: 16.4336 - val_mae: 16.4336\n",
      "Epoch 688/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9163 - mae: 16.9163 - val_loss: 16.4606 - val_mae: 16.4606\n",
      "Epoch 689/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9175 - mae: 16.9175 - val_loss: 16.4398 - val_mae: 16.4397\n",
      "Epoch 690/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9197 - mae: 16.9197 - val_loss: 16.4902 - val_mae: 16.4902\n",
      "Epoch 691/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9204 - mae: 16.9204 - val_loss: 16.5113 - val_mae: 16.5113\n",
      "Epoch 692/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9187 - mae: 16.9187 - val_loss: 16.5133 - val_mae: 16.5133\n",
      "Epoch 693/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.9113 - mae: 16.9113 - val_loss: 16.4686 - val_mae: 16.4686\n",
      "Epoch 694/3000\n",
      "10496/10496 [==============================] - 1s 59us/sample - loss: 16.9141 - mae: 16.9141 - val_loss: 16.4313 - val_mae: 16.4313\n",
      "Epoch 695/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9135 - mae: 16.9135 - val_loss: 16.4432 - val_mae: 16.4432\n",
      "Epoch 696/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8988 - mae: 16.8988 - val_loss: 16.4526 - val_mae: 16.4526\n",
      "Epoch 697/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9058 - mae: 16.9058 - val_loss: 16.4405 - val_mae: 16.4405\n",
      "Epoch 698/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9058 - mae: 16.9058 - val_loss: 16.4456 - val_mae: 16.4456\n",
      "Epoch 699/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.9133 - mae: 16.9133 - val_loss: 16.4399 - val_mae: 16.4399\n",
      "Epoch 700/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.9120 - mae: 16.9120 - val_loss: 16.4255 - val_mae: 16.4255\n",
      "Epoch 701/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9121 - mae: 16.9121 - val_loss: 16.5019 - val_mae: 16.5019\n",
      "Epoch 702/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.9070 - mae: 16.9070 - val_loss: 16.4345 - val_mae: 16.4345\n",
      "Epoch 703/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9074 - mae: 16.9074 - val_loss: 16.4605 - val_mae: 16.4605\n",
      "Epoch 704/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.9018 - mae: 16.9018 - val_loss: 16.5102 - val_mae: 16.5102\n",
      "Epoch 705/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9060 - mae: 16.9060 - val_loss: 16.5199 - val_mae: 16.5199\n",
      "Epoch 706/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9072 - mae: 16.9072 - val_loss: 16.4244 - val_mae: 16.4244\n",
      "Epoch 707/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.8970 - mae: 16.8970 - val_loss: 16.4603 - val_mae: 16.4603\n",
      "Epoch 708/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9103 - mae: 16.9103 - val_loss: 16.4462 - val_mae: 16.4462\n",
      "Epoch 709/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.9016 - mae: 16.9016 - val_loss: 16.4708 - val_mae: 16.4707\n",
      "Epoch 710/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8997 - mae: 16.8997 - val_loss: 16.4288 - val_mae: 16.4288\n",
      "Epoch 711/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8990 - mae: 16.8990 - val_loss: 16.4280 - val_mae: 16.4280\n",
      "Epoch 712/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8979 - mae: 16.8979 - val_loss: 16.4344 - val_mae: 16.4344\n",
      "Epoch 713/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.9043 - mae: 16.9043 - val_loss: 16.4229 - val_mae: 16.4229\n",
      "Epoch 714/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8948 - mae: 16.8948 - val_loss: 16.4341 - val_mae: 16.4341\n",
      "Epoch 715/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8985 - mae: 16.8985 - val_loss: 16.4235 - val_mae: 16.4235\n",
      "Epoch 716/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8925 - mae: 16.8925 - val_loss: 16.4467 - val_mae: 16.4467\n",
      "Epoch 717/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8986 - mae: 16.8986 - val_loss: 16.4223 - val_mae: 16.4223\n",
      "Epoch 718/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.9019 - mae: 16.9019 - val_loss: 16.4252 - val_mae: 16.4252\n",
      "Epoch 719/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.9044 - mae: 16.9044 - val_loss: 16.4241 - val_mae: 16.4241\n",
      "Epoch 720/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8993 - mae: 16.8993 - val_loss: 16.4260 - val_mae: 16.4260\n",
      "Epoch 721/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.8985 - mae: 16.8985 - val_loss: 16.4446 - val_mae: 16.4446\n",
      "Epoch 722/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8887 - mae: 16.8887 - val_loss: 16.4805 - val_mae: 16.4805\n",
      "Epoch 723/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8970 - mae: 16.8970 - val_loss: 16.4431 - val_mae: 16.4431\n",
      "Epoch 724/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.8966 - mae: 16.8966 - val_loss: 16.4252 - val_mae: 16.4252\n",
      "Epoch 725/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8936 - mae: 16.8936 - val_loss: 16.4572 - val_mae: 16.4572\n",
      "Epoch 726/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.8977 - mae: 16.8977 - val_loss: 16.4390 - val_mae: 16.4390\n",
      "Epoch 727/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8969 - mae: 16.8969 - val_loss: 16.4196 - val_mae: 16.4196\n",
      "Epoch 728/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8950 - mae: 16.8950 - val_loss: 16.4327 - val_mae: 16.4327\n",
      "Epoch 729/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8884 - mae: 16.8884 - val_loss: 16.4215 - val_mae: 16.4215\n",
      "Epoch 730/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.8935 - mae: 16.8935 - val_loss: 16.5534 - val_mae: 16.5534\n",
      "Epoch 731/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.8946 - mae: 16.8946 - val_loss: 16.4387 - val_mae: 16.4387\n",
      "Epoch 732/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.8932 - mae: 16.8932 - val_loss: 16.4238 - val_mae: 16.4238\n",
      "Epoch 733/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8930 - mae: 16.8930 - val_loss: 16.4989 - val_mae: 16.4989\n",
      "Epoch 734/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.9021 - mae: 16.9021 - val_loss: 16.4547 - val_mae: 16.4547\n",
      "Epoch 735/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8952 - mae: 16.8952 - val_loss: 16.4279 - val_mae: 16.4279\n",
      "Epoch 736/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.8927 - mae: 16.8927 - val_loss: 16.4173 - val_mae: 16.4173\n",
      "Epoch 737/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.8948 - mae: 16.8948 - val_loss: 16.4788 - val_mae: 16.4788\n",
      "Epoch 738/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.8886 - mae: 16.8886 - val_loss: 16.4167 - val_mae: 16.4167\n",
      "Epoch 739/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.8804 - mae: 16.8804 - val_loss: 16.5537 - val_mae: 16.5537\n",
      "Epoch 740/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8885 - mae: 16.8885 - val_loss: 16.4372 - val_mae: 16.4372\n",
      "Epoch 741/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.8889 - mae: 16.8889 - val_loss: 16.4431 - val_mae: 16.4431\n",
      "Epoch 742/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8889 - mae: 16.8889 - val_loss: 16.4871 - val_mae: 16.4871\n",
      "Epoch 743/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8849 - mae: 16.8849 - val_loss: 16.4157 - val_mae: 16.4157\n",
      "Epoch 744/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8861 - mae: 16.8861 - val_loss: 16.4771 - val_mae: 16.4771\n",
      "Epoch 745/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8859 - mae: 16.8859 - val_loss: 16.4287 - val_mae: 16.4287\n",
      "Epoch 746/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8842 - mae: 16.8842 - val_loss: 16.4073 - val_mae: 16.4073\n",
      "Epoch 747/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8763 - mae: 16.8763 - val_loss: 16.4444 - val_mae: 16.4444\n",
      "Epoch 748/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8861 - mae: 16.8861 - val_loss: 16.4167 - val_mae: 16.4167\n",
      "Epoch 749/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8772 - mae: 16.8772 - val_loss: 16.4973 - val_mae: 16.4973\n",
      "Epoch 750/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8836 - mae: 16.8836 - val_loss: 16.4654 - val_mae: 16.4654\n",
      "Epoch 751/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.8737 - mae: 16.8737 - val_loss: 16.4098 - val_mae: 16.4098\n",
      "Epoch 752/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8745 - mae: 16.8745 - val_loss: 16.4091 - val_mae: 16.4091\n",
      "Epoch 753/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8854 - mae: 16.8853 - val_loss: 16.4083 - val_mae: 16.4083\n",
      "Epoch 754/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8793 - mae: 16.8793 - val_loss: 16.4356 - val_mae: 16.4355\n",
      "Epoch 755/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.8712 - mae: 16.8712 - val_loss: 16.4519 - val_mae: 16.4519\n",
      "Epoch 756/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8830 - mae: 16.8831 - val_loss: 16.4222 - val_mae: 16.4222\n",
      "Epoch 757/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8800 - mae: 16.8801 - val_loss: 16.4103 - val_mae: 16.4103\n",
      "Epoch 758/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.8770 - mae: 16.8771 - val_loss: 16.4146 - val_mae: 16.4146\n",
      "Epoch 759/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8759 - mae: 16.8759 - val_loss: 16.4251 - val_mae: 16.4251\n",
      "Epoch 760/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.8802 - mae: 16.8802 - val_loss: 16.4302 - val_mae: 16.4302\n",
      "Epoch 761/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8776 - mae: 16.8776 - val_loss: 16.4119 - val_mae: 16.4119\n",
      "Epoch 762/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8751 - mae: 16.8751 - val_loss: 16.4172 - val_mae: 16.4172\n",
      "Epoch 763/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8830 - mae: 16.8830 - val_loss: 16.4208 - val_mae: 16.4208\n",
      "Epoch 764/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8743 - mae: 16.8743 - val_loss: 16.4039 - val_mae: 16.4039\n",
      "Epoch 765/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8747 - mae: 16.8747 - val_loss: 16.4033 - val_mae: 16.4033\n",
      "Epoch 766/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8794 - mae: 16.8794 - val_loss: 16.5485 - val_mae: 16.5485\n",
      "Epoch 767/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8820 - mae: 16.8820 - val_loss: 16.4342 - val_mae: 16.4342\n",
      "Epoch 768/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8648 - mae: 16.8648 - val_loss: 16.4097 - val_mae: 16.4097\n",
      "Epoch 769/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.8795 - mae: 16.8795 - val_loss: 16.4136 - val_mae: 16.4136\n",
      "Epoch 770/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8778 - mae: 16.8778 - val_loss: 16.4233 - val_mae: 16.4233\n",
      "Epoch 771/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8726 - mae: 16.8726 - val_loss: 16.4791 - val_mae: 16.4791\n",
      "Epoch 772/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8782 - mae: 16.8782 - val_loss: 16.4149 - val_mae: 16.4149\n",
      "Epoch 773/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8782 - mae: 16.8782 - val_loss: 16.4072 - val_mae: 16.4072\n",
      "Epoch 774/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8600 - mae: 16.8600 - val_loss: 16.4047 - val_mae: 16.4047\n",
      "Epoch 775/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8716 - mae: 16.8716 - val_loss: 16.4970 - val_mae: 16.4970\n",
      "Epoch 776/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8712 - mae: 16.8712 - val_loss: 16.4971 - val_mae: 16.4971\n",
      "Epoch 777/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8665 - mae: 16.8665 - val_loss: 16.4334 - val_mae: 16.4334\n",
      "Epoch 778/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8666 - mae: 16.8666 - val_loss: 16.4066 - val_mae: 16.4066\n",
      "Epoch 779/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8721 - mae: 16.8721 - val_loss: 16.3995 - val_mae: 16.3995\n",
      "Epoch 780/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.8731 - mae: 16.8731 - val_loss: 16.4138 - val_mae: 16.4138\n",
      "Epoch 781/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.8653 - mae: 16.8653 - val_loss: 16.4089 - val_mae: 16.4089\n",
      "Epoch 782/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8680 - mae: 16.8680 - val_loss: 16.4008 - val_mae: 16.4008\n",
      "Epoch 783/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8764 - mae: 16.8764 - val_loss: 16.4858 - val_mae: 16.4858\n",
      "Epoch 784/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8609 - mae: 16.8609 - val_loss: 16.4276 - val_mae: 16.4276\n",
      "Epoch 785/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8663 - mae: 16.8663 - val_loss: 16.3998 - val_mae: 16.3998\n",
      "Epoch 786/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8626 - mae: 16.8626 - val_loss: 16.3979 - val_mae: 16.3979\n",
      "Epoch 787/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.8685 - mae: 16.8685 - val_loss: 16.4323 - val_mae: 16.4323\n",
      "Epoch 788/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.8636 - mae: 16.8636 - val_loss: 16.3963 - val_mae: 16.3963\n",
      "Epoch 789/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8630 - mae: 16.8630 - val_loss: 16.4300 - val_mae: 16.4300\n",
      "Epoch 790/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.8619 - mae: 16.8619 - val_loss: 16.4287 - val_mae: 16.4287\n",
      "Epoch 791/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8670 - mae: 16.8670 - val_loss: 16.4052 - val_mae: 16.4052\n",
      "Epoch 792/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8661 - mae: 16.8661 - val_loss: 16.3960 - val_mae: 16.3960\n",
      "Epoch 793/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.8593 - mae: 16.8593 - val_loss: 16.4744 - val_mae: 16.4744\n",
      "Epoch 794/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.8642 - mae: 16.8642 - val_loss: 16.3955 - val_mae: 16.3955\n",
      "Epoch 795/3000\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 16.8592 - mae: 16.8591 - val_loss: 16.4819 - val_mae: 16.4819\n",
      "Epoch 796/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.8600 - mae: 16.8600 - val_loss: 16.3953 - val_mae: 16.3953\n",
      "Epoch 797/3000\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 16.8558 - mae: 16.8559 - val_loss: 16.4096 - val_mae: 16.4096\n",
      "Epoch 798/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8595 - mae: 16.8596 - val_loss: 16.4155 - val_mae: 16.4155\n",
      "Epoch 799/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8623 - mae: 16.8623 - val_loss: 16.3971 - val_mae: 16.3971\n",
      "Epoch 800/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8591 - mae: 16.8591 - val_loss: 16.4055 - val_mae: 16.4055\n",
      "Epoch 801/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8594 - mae: 16.8594 - val_loss: 16.4853 - val_mae: 16.4853\n",
      "Epoch 802/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8514 - mae: 16.8514 - val_loss: 16.4119 - val_mae: 16.4119\n",
      "Epoch 803/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8632 - mae: 16.8631 - val_loss: 16.4234 - val_mae: 16.4234\n",
      "Epoch 804/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.8526 - mae: 16.8526 - val_loss: 16.3891 - val_mae: 16.3891\n",
      "Epoch 805/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8626 - mae: 16.8626 - val_loss: 16.4130 - val_mae: 16.4130\n",
      "Epoch 806/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.8541 - mae: 16.8541 - val_loss: 16.3940 - val_mae: 16.3940\n",
      "Epoch 807/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.8609 - mae: 16.8609 - val_loss: 16.3934 - val_mae: 16.3934\n",
      "Epoch 808/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8566 - mae: 16.8566 - val_loss: 16.3922 - val_mae: 16.3922\n",
      "Epoch 809/3000\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 16.8394 - mae: 16.8394 - val_loss: 16.5103 - val_mae: 16.5103\n",
      "Epoch 810/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8554 - mae: 16.8554 - val_loss: 16.3942 - val_mae: 16.3942\n",
      "Epoch 811/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8557 - mae: 16.8557 - val_loss: 16.3954 - val_mae: 16.3954\n",
      "Epoch 812/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8494 - mae: 16.8494 - val_loss: 16.4418 - val_mae: 16.4418\n",
      "Epoch 813/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.8550 - mae: 16.8550 - val_loss: 16.4260 - val_mae: 16.4260\n",
      "Epoch 814/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8533 - mae: 16.8533 - val_loss: 16.3882 - val_mae: 16.3883\n",
      "Epoch 815/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8544 - mae: 16.8544 - val_loss: 16.4615 - val_mae: 16.4615\n",
      "Epoch 816/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8539 - mae: 16.8539 - val_loss: 16.3918 - val_mae: 16.3918\n",
      "Epoch 817/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8513 - mae: 16.8513 - val_loss: 16.4903 - val_mae: 16.4903\n",
      "Epoch 818/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8522 - mae: 16.8522 - val_loss: 16.3857 - val_mae: 16.3857\n",
      "Epoch 819/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8494 - mae: 16.8494 - val_loss: 16.3966 - val_mae: 16.3966\n",
      "Epoch 820/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8523 - mae: 16.8523 - val_loss: 16.3900 - val_mae: 16.3900\n",
      "Epoch 821/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8552 - mae: 16.8552 - val_loss: 16.4718 - val_mae: 16.4718\n",
      "Epoch 822/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8416 - mae: 16.8416 - val_loss: 16.4528 - val_mae: 16.4527\n",
      "Epoch 823/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8456 - mae: 16.8456 - val_loss: 16.3974 - val_mae: 16.3974\n",
      "Epoch 824/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8503 - mae: 16.8503 - val_loss: 16.3873 - val_mae: 16.3873\n",
      "Epoch 825/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8477 - mae: 16.8477 - val_loss: 16.4236 - val_mae: 16.4236\n",
      "Epoch 826/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8510 - mae: 16.8510 - val_loss: 16.3837 - val_mae: 16.3837\n",
      "Epoch 827/3000\n",
      "10496/10496 [==============================] - 1s 56us/sample - loss: 16.8460 - mae: 16.8460 - val_loss: 16.4016 - val_mae: 16.4016\n",
      "Epoch 828/3000\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 16.8415 - mae: 16.8415 - val_loss: 16.4664 - val_mae: 16.4664\n",
      "Epoch 829/3000\n",
      "10496/10496 [==============================] - 1s 63us/sample - loss: 16.8374 - mae: 16.8374 - val_loss: 16.3857 - val_mae: 16.3857\n",
      "Epoch 830/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.8413 - mae: 16.8413 - val_loss: 16.4879 - val_mae: 16.4879\n",
      "Epoch 831/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.8413 - mae: 16.8413 - val_loss: 16.4137 - val_mae: 16.4137\n",
      "Epoch 832/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8470 - mae: 16.8470 - val_loss: 16.3841 - val_mae: 16.3841\n",
      "Epoch 833/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8378 - mae: 16.8378 - val_loss: 16.4042 - val_mae: 16.4042\n",
      "Epoch 834/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8373 - mae: 16.8373 - val_loss: 16.3846 - val_mae: 16.3846\n",
      "Epoch 835/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8418 - mae: 16.8418 - val_loss: 16.3921 - val_mae: 16.3921\n",
      "Epoch 836/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8446 - mae: 16.8446 - val_loss: 16.3975 - val_mae: 16.3975\n",
      "Epoch 837/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8405 - mae: 16.8405 - val_loss: 16.3885 - val_mae: 16.3885\n",
      "Epoch 838/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8423 - mae: 16.8423 - val_loss: 16.3858 - val_mae: 16.3858\n",
      "Epoch 839/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8458 - mae: 16.8458 - val_loss: 16.3900 - val_mae: 16.3900\n",
      "Epoch 840/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8295 - mae: 16.8295 - val_loss: 16.4284 - val_mae: 16.4284\n",
      "Epoch 841/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8456 - mae: 16.8456 - val_loss: 16.3918 - val_mae: 16.3918\n",
      "Epoch 842/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8430 - mae: 16.8430 - val_loss: 16.3940 - val_mae: 16.3940\n",
      "Epoch 843/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8432 - mae: 16.8432 - val_loss: 16.4123 - val_mae: 16.4123\n",
      "Epoch 844/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8383 - mae: 16.8383 - val_loss: 16.4018 - val_mae: 16.4018\n",
      "Epoch 845/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8367 - mae: 16.8367 - val_loss: 16.4146 - val_mae: 16.4146\n",
      "Epoch 846/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8329 - mae: 16.8329 - val_loss: 16.4062 - val_mae: 16.4062\n",
      "Epoch 847/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8334 - mae: 16.8334 - val_loss: 16.3949 - val_mae: 16.3949\n",
      "Epoch 848/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8441 - mae: 16.8441 - val_loss: 16.4207 - val_mae: 16.4207\n",
      "Epoch 849/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.8412 - mae: 16.8412 - val_loss: 16.3789 - val_mae: 16.3789\n",
      "Epoch 850/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8368 - mae: 16.8368 - val_loss: 16.3794 - val_mae: 16.3794\n",
      "Epoch 851/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8356 - mae: 16.8356 - val_loss: 16.3912 - val_mae: 16.3912\n",
      "Epoch 852/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.8390 - mae: 16.8390 - val_loss: 16.3875 - val_mae: 16.3875\n",
      "Epoch 853/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8316 - mae: 16.8316 - val_loss: 16.3845 - val_mae: 16.3845\n",
      "Epoch 854/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8419 - mae: 16.8419 - val_loss: 16.3888 - val_mae: 16.3888\n",
      "Epoch 855/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8345 - mae: 16.8345 - val_loss: 16.3799 - val_mae: 16.3799\n",
      "Epoch 856/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8305 - mae: 16.8305 - val_loss: 16.4191 - val_mae: 16.4191\n",
      "Epoch 857/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8318 - mae: 16.8318 - val_loss: 16.4204 - val_mae: 16.4204\n",
      "Epoch 858/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8341 - mae: 16.8341 - val_loss: 16.3786 - val_mae: 16.3786\n",
      "Epoch 859/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8251 - mae: 16.8251 - val_loss: 16.3912 - val_mae: 16.3912\n",
      "Epoch 860/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.8258 - mae: 16.8258 - val_loss: 16.4394 - val_mae: 16.4394\n",
      "Epoch 861/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8254 - mae: 16.8254 - val_loss: 16.4560 - val_mae: 16.4560\n",
      "Epoch 862/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8307 - mae: 16.8308 - val_loss: 16.4295 - val_mae: 16.4295\n",
      "Epoch 863/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8330 - mae: 16.8330 - val_loss: 16.3897 - val_mae: 16.3897\n",
      "Epoch 864/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8340 - mae: 16.8340 - val_loss: 16.3802 - val_mae: 16.3802\n",
      "Epoch 865/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8354 - mae: 16.8354 - val_loss: 16.4011 - val_mae: 16.4011\n",
      "Epoch 866/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8239 - mae: 16.8239 - val_loss: 16.4546 - val_mae: 16.4546\n",
      "Epoch 867/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8223 - mae: 16.8223 - val_loss: 16.3907 - val_mae: 16.3907\n",
      "Epoch 868/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8265 - mae: 16.8265 - val_loss: 16.3861 - val_mae: 16.3861\n",
      "Epoch 869/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8335 - mae: 16.8335 - val_loss: 16.3763 - val_mae: 16.3763\n",
      "Epoch 870/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8292 - mae: 16.8292 - val_loss: 16.3780 - val_mae: 16.3780\n",
      "Epoch 871/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8284 - mae: 16.8284 - val_loss: 16.3776 - val_mae: 16.3776\n",
      "Epoch 872/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8272 - mae: 16.8272 - val_loss: 16.3805 - val_mae: 16.3805\n",
      "Epoch 873/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8225 - mae: 16.8225 - val_loss: 16.4505 - val_mae: 16.4505\n",
      "Epoch 874/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8295 - mae: 16.8295 - val_loss: 16.3890 - val_mae: 16.3890\n",
      "Epoch 875/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8190 - mae: 16.8190 - val_loss: 16.4005 - val_mae: 16.4005\n",
      "Epoch 876/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8342 - mae: 16.8342 - val_loss: 16.3785 - val_mae: 16.3785\n",
      "Epoch 877/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8256 - mae: 16.8256 - val_loss: 16.3862 - val_mae: 16.3862\n",
      "Epoch 878/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8214 - mae: 16.8214 - val_loss: 16.3767 - val_mae: 16.3767\n",
      "Epoch 879/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8258 - mae: 16.8258 - val_loss: 16.3680 - val_mae: 16.3680\n",
      "Epoch 880/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8276 - mae: 16.8276 - val_loss: 16.3672 - val_mae: 16.3672\n",
      "Epoch 881/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8212 - mae: 16.8212 - val_loss: 16.4086 - val_mae: 16.4086\n",
      "Epoch 882/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8135 - mae: 16.8135 - val_loss: 16.3697 - val_mae: 16.3697\n",
      "Epoch 883/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8179 - mae: 16.8179 - val_loss: 16.3842 - val_mae: 16.3842\n",
      "Epoch 884/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8260 - mae: 16.8260 - val_loss: 16.3922 - val_mae: 16.3922\n",
      "Epoch 885/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8173 - mae: 16.8173 - val_loss: 16.4010 - val_mae: 16.4010\n",
      "Epoch 886/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8178 - mae: 16.8178 - val_loss: 16.3913 - val_mae: 16.3913\n",
      "Epoch 887/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8153 - mae: 16.8153 - val_loss: 16.3808 - val_mae: 16.3808\n",
      "Epoch 888/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8166 - mae: 16.8166 - val_loss: 16.3794 - val_mae: 16.3794\n",
      "Epoch 889/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8141 - mae: 16.8141 - val_loss: 16.3838 - val_mae: 16.3838\n",
      "Epoch 890/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8075 - mae: 16.8075 - val_loss: 16.4153 - val_mae: 16.4153\n",
      "Epoch 891/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8266 - mae: 16.8266 - val_loss: 16.3918 - val_mae: 16.3918\n",
      "Epoch 892/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8139 - mae: 16.8139 - val_loss: 16.3696 - val_mae: 16.3696\n",
      "Epoch 893/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8121 - mae: 16.8121 - val_loss: 16.3693 - val_mae: 16.3693\n",
      "Epoch 894/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8120 - mae: 16.8120 - val_loss: 16.3814 - val_mae: 16.3814\n",
      "Epoch 895/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8069 - mae: 16.8069 - val_loss: 16.4303 - val_mae: 16.4303\n",
      "Epoch 896/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8181 - mae: 16.8181 - val_loss: 16.3722 - val_mae: 16.3722\n",
      "Epoch 897/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8053 - mae: 16.8053 - val_loss: 16.3686 - val_mae: 16.3686\n",
      "Epoch 898/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8150 - mae: 16.8150 - val_loss: 16.3730 - val_mae: 16.3730\n",
      "Epoch 899/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8122 - mae: 16.8122 - val_loss: 16.4318 - val_mae: 16.4318\n",
      "Epoch 900/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8062 - mae: 16.8062 - val_loss: 16.5099 - val_mae: 16.5099\n",
      "Epoch 901/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8099 - mae: 16.8099 - val_loss: 16.4107 - val_mae: 16.4107\n",
      "Epoch 902/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8116 - mae: 16.8116 - val_loss: 16.3634 - val_mae: 16.3634\n",
      "Epoch 903/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8075 - mae: 16.8075 - val_loss: 16.3651 - val_mae: 16.3651\n",
      "Epoch 904/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8191 - mae: 16.8191 - val_loss: 16.3767 - val_mae: 16.3767\n",
      "Epoch 905/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8131 - mae: 16.8131 - val_loss: 16.3745 - val_mae: 16.3745\n",
      "Epoch 906/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8143 - mae: 16.8142 - val_loss: 16.3670 - val_mae: 16.3670\n",
      "Epoch 907/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8128 - mae: 16.8128 - val_loss: 16.3640 - val_mae: 16.3640\n",
      "Epoch 908/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8041 - mae: 16.8042 - val_loss: 16.4126 - val_mae: 16.4126\n",
      "Epoch 909/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8045 - mae: 16.8045 - val_loss: 16.3618 - val_mae: 16.3618\n",
      "Epoch 910/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8113 - mae: 16.8113 - val_loss: 16.4091 - val_mae: 16.4091\n",
      "Epoch 911/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8154 - mae: 16.8154 - val_loss: 16.3994 - val_mae: 16.3994\n",
      "Epoch 912/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8127 - mae: 16.8127 - val_loss: 16.4197 - val_mae: 16.4197\n",
      "Epoch 913/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8091 - mae: 16.8091 - val_loss: 16.3604 - val_mae: 16.3604\n",
      "Epoch 914/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8109 - mae: 16.8109 - val_loss: 16.3881 - val_mae: 16.3881\n",
      "Epoch 915/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8123 - mae: 16.8123 - val_loss: 16.3841 - val_mae: 16.3841\n",
      "Epoch 916/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7999 - mae: 16.7999 - val_loss: 16.3641 - val_mae: 16.3641\n",
      "Epoch 917/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8082 - mae: 16.8082 - val_loss: 16.3988 - val_mae: 16.3988\n",
      "Epoch 918/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7945 - mae: 16.7945 - val_loss: 16.3642 - val_mae: 16.3642\n",
      "Epoch 919/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7979 - mae: 16.7979 - val_loss: 16.3708 - val_mae: 16.3708\n",
      "Epoch 920/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8014 - mae: 16.8014 - val_loss: 16.3727 - val_mae: 16.3727\n",
      "Epoch 921/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8001 - mae: 16.8001 - val_loss: 16.4175 - val_mae: 16.4175\n",
      "Epoch 922/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8017 - mae: 16.8017 - val_loss: 16.3599 - val_mae: 16.3599\n",
      "Epoch 923/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.8019 - mae: 16.8019 - val_loss: 16.3691 - val_mae: 16.3691\n",
      "Epoch 924/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.8083 - mae: 16.8083 - val_loss: 16.3894 - val_mae: 16.3894\n",
      "Epoch 925/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8048 - mae: 16.8049 - val_loss: 16.3662 - val_mae: 16.3662\n",
      "Epoch 926/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8038 - mae: 16.8038 - val_loss: 16.3756 - val_mae: 16.3756\n",
      "Epoch 927/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8025 - mae: 16.8025 - val_loss: 16.3558 - val_mae: 16.3558\n",
      "Epoch 928/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7997 - mae: 16.7997 - val_loss: 16.3690 - val_mae: 16.3690\n",
      "Epoch 929/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8046 - mae: 16.8046 - val_loss: 16.3880 - val_mae: 16.3880\n",
      "Epoch 930/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.8051 - mae: 16.8051 - val_loss: 16.3694 - val_mae: 16.3694\n",
      "Epoch 931/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7964 - mae: 16.7964 - val_loss: 16.3732 - val_mae: 16.3732\n",
      "Epoch 932/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7944 - mae: 16.7944 - val_loss: 16.3630 - val_mae: 16.3630\n",
      "Epoch 933/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7980 - mae: 16.7980 - val_loss: 16.3539 - val_mae: 16.3539\n",
      "Epoch 934/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.8034 - mae: 16.8034 - val_loss: 16.3786 - val_mae: 16.3786\n",
      "Epoch 935/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7951 - mae: 16.7951 - val_loss: 16.4175 - val_mae: 16.4175\n",
      "Epoch 936/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7929 - mae: 16.7929 - val_loss: 16.4086 - val_mae: 16.4086\n",
      "Epoch 937/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7978 - mae: 16.7978 - val_loss: 16.3633 - val_mae: 16.3633\n",
      "Epoch 938/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7908 - mae: 16.7907 - val_loss: 16.3519 - val_mae: 16.3519\n",
      "Epoch 939/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7905 - mae: 16.7905 - val_loss: 16.3557 - val_mae: 16.3557\n",
      "Epoch 940/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7967 - mae: 16.7967 - val_loss: 16.3652 - val_mae: 16.3652\n",
      "Epoch 941/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7903 - mae: 16.7903 - val_loss: 16.3544 - val_mae: 16.3544\n",
      "Epoch 942/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7927 - mae: 16.7927 - val_loss: 16.3515 - val_mae: 16.3515\n",
      "Epoch 943/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7905 - mae: 16.7905 - val_loss: 16.3597 - val_mae: 16.3597\n",
      "Epoch 944/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7939 - mae: 16.7939 - val_loss: 16.3933 - val_mae: 16.3933\n",
      "Epoch 945/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7885 - mae: 16.7885 - val_loss: 16.3680 - val_mae: 16.3680\n",
      "Epoch 946/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7954 - mae: 16.7954 - val_loss: 16.3875 - val_mae: 16.3875\n",
      "Epoch 947/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7901 - mae: 16.7901 - val_loss: 16.3613 - val_mae: 16.3613\n",
      "Epoch 948/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7948 - mae: 16.7948 - val_loss: 16.3599 - val_mae: 16.3599\n",
      "Epoch 949/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7827 - mae: 16.7827 - val_loss: 16.3534 - val_mae: 16.3534\n",
      "Epoch 950/3000\n",
      "10496/10496 [==============================] - ETA: 0s - loss: 16.7431 - mae: 16.743 - 1s 51us/sample - loss: 16.7894 - mae: 16.7894 - val_loss: 16.3544 - val_mae: 16.3544\n",
      "Epoch 951/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7853 - mae: 16.7853 - val_loss: 16.3851 - val_mae: 16.3851\n",
      "Epoch 952/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7968 - mae: 16.7968 - val_loss: 16.4489 - val_mae: 16.4489\n",
      "Epoch 953/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7926 - mae: 16.7926 - val_loss: 16.3985 - val_mae: 16.3985\n",
      "Epoch 954/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7840 - mae: 16.7840 - val_loss: 16.3574 - val_mae: 16.3574\n",
      "Epoch 955/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7865 - mae: 16.7865 - val_loss: 16.3593 - val_mae: 16.3593\n",
      "Epoch 956/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7857 - mae: 16.7857 - val_loss: 16.3523 - val_mae: 16.3523\n",
      "Epoch 957/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7871 - mae: 16.7871 - val_loss: 16.3500 - val_mae: 16.3500\n",
      "Epoch 958/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7839 - mae: 16.7839 - val_loss: 16.3511 - val_mae: 16.3511\n",
      "Epoch 959/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7829 - mae: 16.7829 - val_loss: 16.4687 - val_mae: 16.4687\n",
      "Epoch 960/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7845 - mae: 16.7845 - val_loss: 16.3512 - val_mae: 16.3512\n",
      "Epoch 961/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7960 - mae: 16.7960 - val_loss: 16.3478 - val_mae: 16.3478\n",
      "Epoch 962/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7844 - mae: 16.7844 - val_loss: 16.3567 - val_mae: 16.3567\n",
      "Epoch 963/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7813 - mae: 16.7812 - val_loss: 16.3492 - val_mae: 16.3492\n",
      "Epoch 964/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7781 - mae: 16.7781 - val_loss: 16.3551 - val_mae: 16.3551\n",
      "Epoch 965/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7864 - mae: 16.7864 - val_loss: 16.3515 - val_mae: 16.3515\n",
      "Epoch 966/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7843 - mae: 16.7843 - val_loss: 16.3574 - val_mae: 16.3574\n",
      "Epoch 967/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7830 - mae: 16.7830 - val_loss: 16.3970 - val_mae: 16.3970\n",
      "Epoch 968/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7724 - mae: 16.7724 - val_loss: 16.4044 - val_mae: 16.4044\n",
      "Epoch 969/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7838 - mae: 16.7838 - val_loss: 16.3483 - val_mae: 16.3483\n",
      "Epoch 970/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7853 - mae: 16.7853 - val_loss: 16.3454 - val_mae: 16.3454\n",
      "Epoch 971/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7812 - mae: 16.7812 - val_loss: 16.3593 - val_mae: 16.3593\n",
      "Epoch 972/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7824 - mae: 16.7824 - val_loss: 16.3511 - val_mae: 16.3511\n",
      "Epoch 973/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7919 - mae: 16.7919 - val_loss: 16.3457 - val_mae: 16.3457\n",
      "Epoch 974/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7849 - mae: 16.7849 - val_loss: 16.3461 - val_mae: 16.3461\n",
      "Epoch 975/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7791 - mae: 16.7791 - val_loss: 16.3558 - val_mae: 16.3558\n",
      "Epoch 976/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7827 - mae: 16.7827 - val_loss: 16.3442 - val_mae: 16.3442\n",
      "Epoch 977/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7811 - mae: 16.7811 - val_loss: 16.3838 - val_mae: 16.3838\n",
      "Epoch 978/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7840 - mae: 16.7840 - val_loss: 16.4349 - val_mae: 16.4349\n",
      "Epoch 979/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7834 - mae: 16.7834 - val_loss: 16.3728 - val_mae: 16.3728\n",
      "Epoch 980/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7775 - mae: 16.7775 - val_loss: 16.3536 - val_mae: 16.3536\n",
      "Epoch 981/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7757 - mae: 16.7757 - val_loss: 16.4319 - val_mae: 16.4319\n",
      "Epoch 982/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7849 - mae: 16.7849 - val_loss: 16.3981 - val_mae: 16.3981\n",
      "Epoch 983/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7808 - mae: 16.7808 - val_loss: 16.3489 - val_mae: 16.3489\n",
      "Epoch 984/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7751 - mae: 16.7751 - val_loss: 16.3764 - val_mae: 16.3764\n",
      "Epoch 985/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7756 - mae: 16.7756 - val_loss: 16.3470 - val_mae: 16.3470\n",
      "Epoch 986/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7840 - mae: 16.7840 - val_loss: 16.3478 - val_mae: 16.3478\n",
      "Epoch 987/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7766 - mae: 16.7766 - val_loss: 16.3749 - val_mae: 16.3749\n",
      "Epoch 988/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7719 - mae: 16.7719 - val_loss: 16.3788 - val_mae: 16.3788\n",
      "Epoch 989/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7743 - mae: 16.7743 - val_loss: 16.3513 - val_mae: 16.3513\n",
      "Epoch 990/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7736 - mae: 16.7736 - val_loss: 16.3419 - val_mae: 16.3419\n",
      "Epoch 991/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7784 - mae: 16.7784 - val_loss: 16.3423 - val_mae: 16.3423\n",
      "Epoch 992/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7728 - mae: 16.7728 - val_loss: 16.3776 - val_mae: 16.3776\n",
      "Epoch 993/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7695 - mae: 16.7695 - val_loss: 16.3622 - val_mae: 16.3622\n",
      "Epoch 994/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7752 - mae: 16.7752 - val_loss: 16.3629 - val_mae: 16.3629\n",
      "Epoch 995/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.7720 - mae: 16.7720 - val_loss: 16.3398 - val_mae: 16.3398\n",
      "Epoch 996/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7766 - mae: 16.7766 - val_loss: 16.3416 - val_mae: 16.3416\n",
      "Epoch 997/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7755 - mae: 16.7755 - val_loss: 16.3505 - val_mae: 16.3505\n",
      "Epoch 998/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7720 - mae: 16.7720 - val_loss: 16.4011 - val_mae: 16.4011\n",
      "Epoch 999/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7682 - mae: 16.7682 - val_loss: 16.3629 - val_mae: 16.3629\n",
      "Epoch 1000/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7701 - mae: 16.7701 - val_loss: 16.3451 - val_mae: 16.3451\n",
      "Epoch 1001/3000\n",
      "10496/10496 [==============================] - 1s 57us/sample - loss: 16.7701 - mae: 16.7701 - val_loss: 16.3453 - val_mae: 16.3453\n",
      "Epoch 1002/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7659 - mae: 16.7659 - val_loss: 16.3750 - val_mae: 16.3750\n",
      "Epoch 1003/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7615 - mae: 16.7615 - val_loss: 16.4643 - val_mae: 16.4643\n",
      "Epoch 1004/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7738 - mae: 16.7738 - val_loss: 16.3404 - val_mae: 16.3404\n",
      "Epoch 1005/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7705 - mae: 16.7705 - val_loss: 16.3417 - val_mae: 16.3417\n",
      "Epoch 1006/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.7649 - mae: 16.7649 - val_loss: 16.3420 - val_mae: 16.3420\n",
      "Epoch 1007/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.7691 - mae: 16.7691 - val_loss: 16.3378 - val_mae: 16.3378\n",
      "Epoch 1008/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7737 - mae: 16.7737 - val_loss: 16.3368 - val_mae: 16.3368\n",
      "Epoch 1009/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7733 - mae: 16.7733 - val_loss: 16.3437 - val_mae: 16.3437\n",
      "Epoch 1010/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7670 - mae: 16.7670 - val_loss: 16.3393 - val_mae: 16.3393\n",
      "Epoch 1011/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7655 - mae: 16.7655 - val_loss: 16.3384 - val_mae: 16.3384\n",
      "Epoch 1012/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7687 - mae: 16.7687 - val_loss: 16.3405 - val_mae: 16.3405\n",
      "Epoch 1013/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7689 - mae: 16.7689 - val_loss: 16.4286 - val_mae: 16.4286\n",
      "Epoch 1014/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.7635 - mae: 16.7635 - val_loss: 16.4054 - val_mae: 16.4054\n",
      "Epoch 1015/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.7690 - mae: 16.7690 - val_loss: 16.3606 - val_mae: 16.3606\n",
      "Epoch 1016/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7681 - mae: 16.7681 - val_loss: 16.3387 - val_mae: 16.3387\n",
      "Epoch 1017/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.7618 - mae: 16.7618 - val_loss: 16.3425 - val_mae: 16.3425\n",
      "Epoch 1018/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7615 - mae: 16.7615 - val_loss: 16.5059 - val_mae: 16.5059\n",
      "Epoch 1019/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7601 - mae: 16.7601 - val_loss: 16.3377 - val_mae: 16.3377\n",
      "Epoch 1020/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7673 - mae: 16.7673 - val_loss: 16.3696 - val_mae: 16.3696\n",
      "Epoch 1021/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7675 - mae: 16.7675 - val_loss: 16.3377 - val_mae: 16.3377\n",
      "Epoch 1022/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7629 - mae: 16.7629 - val_loss: 16.3860 - val_mae: 16.3860\n",
      "Epoch 1023/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7534 - mae: 16.7534 - val_loss: 16.4101 - val_mae: 16.4101\n",
      "Epoch 1024/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7618 - mae: 16.7618 - val_loss: 16.3655 - val_mae: 16.3655\n",
      "Epoch 1025/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7613 - mae: 16.7613 - val_loss: 16.3826 - val_mae: 16.3826\n",
      "Epoch 1026/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7499 - mae: 16.7499 - val_loss: 16.3563 - val_mae: 16.3563\n",
      "Epoch 1027/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.7569 - mae: 16.7569 - val_loss: 16.3468 - val_mae: 16.3468\n",
      "Epoch 1028/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7665 - mae: 16.7665 - val_loss: 16.3647 - val_mae: 16.3647\n",
      "Epoch 1029/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7673 - mae: 16.7673 - val_loss: 16.3361 - val_mae: 16.3361\n",
      "Epoch 1030/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7615 - mae: 16.7615 - val_loss: 16.4025 - val_mae: 16.4025\n",
      "Epoch 1031/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7601 - mae: 16.7601 - val_loss: 16.3327 - val_mae: 16.3327\n",
      "Epoch 1032/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7635 - mae: 16.7635 - val_loss: 16.3334 - val_mae: 16.3334\n",
      "Epoch 1033/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7554 - mae: 16.7554 - val_loss: 16.4213 - val_mae: 16.4213\n",
      "Epoch 1034/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7629 - mae: 16.7629 - val_loss: 16.3380 - val_mae: 16.3381\n",
      "Epoch 1035/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7605 - mae: 16.7605 - val_loss: 16.3346 - val_mae: 16.3346\n",
      "Epoch 1036/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7601 - mae: 16.7601 - val_loss: 16.3312 - val_mae: 16.3312\n",
      "Epoch 1037/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.7601 - mae: 16.7601 - val_loss: 16.3449 - val_mae: 16.3449\n",
      "Epoch 1038/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7558 - mae: 16.7558 - val_loss: 16.3465 - val_mae: 16.3465\n",
      "Epoch 1039/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7578 - mae: 16.7578 - val_loss: 16.3333 - val_mae: 16.3333\n",
      "Epoch 1040/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7532 - mae: 16.7532 - val_loss: 16.3369 - val_mae: 16.3369\n",
      "Epoch 1041/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.7530 - mae: 16.7530 - val_loss: 16.4226 - val_mae: 16.4226\n",
      "Epoch 1042/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.7572 - mae: 16.7572 - val_loss: 16.3344 - val_mae: 16.3344\n",
      "Epoch 1043/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7617 - mae: 16.7617 - val_loss: 16.3622 - val_mae: 16.3622\n",
      "Epoch 1044/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.7533 - mae: 16.7533 - val_loss: 16.3611 - val_mae: 16.3611\n",
      "Epoch 1045/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7501 - mae: 16.7501 - val_loss: 16.3447 - val_mae: 16.3447\n",
      "Epoch 1046/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7536 - mae: 16.7535 - val_loss: 16.3806 - val_mae: 16.3806\n",
      "Epoch 1047/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7453 - mae: 16.7453 - val_loss: 16.3328 - val_mae: 16.3328\n",
      "Epoch 1048/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7548 - mae: 16.7548 - val_loss: 16.3333 - val_mae: 16.3333\n",
      "Epoch 1049/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7570 - mae: 16.7570 - val_loss: 16.3327 - val_mae: 16.3327\n",
      "Epoch 1050/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7499 - mae: 16.7499 - val_loss: 16.3939 - val_mae: 16.3939\n",
      "Epoch 1051/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7557 - mae: 16.7557 - val_loss: 16.4069 - val_mae: 16.4069\n",
      "Epoch 1052/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7542 - mae: 16.7542 - val_loss: 16.3655 - val_mae: 16.3655\n",
      "Epoch 1053/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.7439 - mae: 16.7439 - val_loss: 16.3701 - val_mae: 16.3701\n",
      "Epoch 1054/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7557 - mae: 16.7557 - val_loss: 16.3292 - val_mae: 16.3292\n",
      "Epoch 1055/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7529 - mae: 16.7529 - val_loss: 16.3813 - val_mae: 16.3813\n",
      "Epoch 1056/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7534 - mae: 16.7534 - val_loss: 16.3584 - val_mae: 16.3584\n",
      "Epoch 1057/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7497 - mae: 16.7497 - val_loss: 16.3520 - val_mae: 16.3520\n",
      "Epoch 1058/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7579 - mae: 16.7579 - val_loss: 16.3371 - val_mae: 16.3371\n",
      "Epoch 1059/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7544 - mae: 16.7544 - val_loss: 16.3297 - val_mae: 16.3297\n",
      "Epoch 1060/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7487 - mae: 16.7487 - val_loss: 16.3634 - val_mae: 16.3634\n",
      "Epoch 1061/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7479 - mae: 16.7479 - val_loss: 16.3312 - val_mae: 16.3312\n",
      "Epoch 1062/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7496 - mae: 16.7496 - val_loss: 16.3294 - val_mae: 16.3294\n",
      "Epoch 1063/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.7506 - mae: 16.7506 - val_loss: 16.3508 - val_mae: 16.3508\n",
      "Epoch 1064/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7443 - mae: 16.7443 - val_loss: 16.3677 - val_mae: 16.3677\n",
      "Epoch 1065/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7490 - mae: 16.7490 - val_loss: 16.3979 - val_mae: 16.3979\n",
      "Epoch 1066/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7480 - mae: 16.7480 - val_loss: 16.3621 - val_mae: 16.3621\n",
      "Epoch 1067/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7534 - mae: 16.7534 - val_loss: 16.3278 - val_mae: 16.3278\n",
      "Epoch 1068/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7453 - mae: 16.7453 - val_loss: 16.3299 - val_mae: 16.3299\n",
      "Epoch 1069/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7420 - mae: 16.7420 - val_loss: 16.3497 - val_mae: 16.3497\n",
      "Epoch 1070/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7484 - mae: 16.7484 - val_loss: 16.3480 - val_mae: 16.3480\n",
      "Epoch 1071/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7473 - mae: 16.7473 - val_loss: 16.4094 - val_mae: 16.4094\n",
      "Epoch 1072/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7475 - mae: 16.7475 - val_loss: 16.3356 - val_mae: 16.3356\n",
      "Epoch 1073/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7435 - mae: 16.7435 - val_loss: 16.4615 - val_mae: 16.4615\n",
      "Epoch 1074/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7425 - mae: 16.7425 - val_loss: 16.4294 - val_mae: 16.4294\n",
      "Epoch 1075/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7428 - mae: 16.7428 - val_loss: 16.3363 - val_mae: 16.3363\n",
      "Epoch 1076/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7444 - mae: 16.7444 - val_loss: 16.3430 - val_mae: 16.3430\n",
      "Epoch 1077/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7450 - mae: 16.7450 - val_loss: 16.3688 - val_mae: 16.3688\n",
      "Epoch 1078/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7423 - mae: 16.7424 - val_loss: 16.3253 - val_mae: 16.3253\n",
      "Epoch 1079/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.7375 - mae: 16.7375 - val_loss: 16.3257 - val_mae: 16.3257\n",
      "Epoch 1080/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.7363 - mae: 16.7363 - val_loss: 16.3258 - val_mae: 16.3258\n",
      "Epoch 1081/3000\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 16.7478 - mae: 16.7478 - val_loss: 16.3594 - val_mae: 16.3594\n",
      "Epoch 1082/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7423 - mae: 16.7423 - val_loss: 16.3241 - val_mae: 16.3241\n",
      "Epoch 1083/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.7467 - mae: 16.7467 - val_loss: 16.3252 - val_mae: 16.3252\n",
      "Epoch 1084/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7363 - mae: 16.7363 - val_loss: 16.3318 - val_mae: 16.3318\n",
      "Epoch 1085/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7437 - mae: 16.7437 - val_loss: 16.3271 - val_mae: 16.3271\n",
      "Epoch 1086/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7407 - mae: 16.7407 - val_loss: 16.3267 - val_mae: 16.3267\n",
      "Epoch 1087/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.7344 - mae: 16.7344 - val_loss: 16.3234 - val_mae: 16.3234\n",
      "Epoch 1088/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7354 - mae: 16.7354 - val_loss: 16.3479 - val_mae: 16.3479\n",
      "Epoch 1089/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.7380 - mae: 16.7380 - val_loss: 16.3324 - val_mae: 16.3324\n",
      "Epoch 1090/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7407 - mae: 16.7407 - val_loss: 16.3253 - val_mae: 16.3253\n",
      "Epoch 1091/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7304 - mae: 16.7304 - val_loss: 16.3248 - val_mae: 16.3248\n",
      "Epoch 1092/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.7305 - mae: 16.7305 - val_loss: 16.3431 - val_mae: 16.3431\n",
      "Epoch 1093/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7328 - mae: 16.7328 - val_loss: 16.3469 - val_mae: 16.3469\n",
      "Epoch 1094/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.7346 - mae: 16.7346 - val_loss: 16.3307 - val_mae: 16.3307\n",
      "Epoch 1095/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7387 - mae: 16.7387 - val_loss: 16.3229 - val_mae: 16.3229\n",
      "Epoch 1096/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7412 - mae: 16.7412 - val_loss: 16.3218 - val_mae: 16.3218\n",
      "Epoch 1097/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7366 - mae: 16.7366 - val_loss: 16.3275 - val_mae: 16.3275\n",
      "Epoch 1098/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7369 - mae: 16.7369 - val_loss: 16.3242 - val_mae: 16.3242\n",
      "Epoch 1099/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.7312 - mae: 16.7312 - val_loss: 16.3638 - val_mae: 16.3638\n",
      "Epoch 1100/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7387 - mae: 16.7387 - val_loss: 16.3411 - val_mae: 16.3411\n",
      "Epoch 1101/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7274 - mae: 16.7274 - val_loss: 16.3258 - val_mae: 16.3258\n",
      "Epoch 1102/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.7337 - mae: 16.7337 - val_loss: 16.3494 - val_mae: 16.3494\n",
      "Epoch 1103/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7325 - mae: 16.7325 - val_loss: 16.3796 - val_mae: 16.3796\n",
      "Epoch 1104/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.7260 - mae: 16.7260 - val_loss: 16.3282 - val_mae: 16.3282\n",
      "Epoch 1105/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.7278 - mae: 16.7278 - val_loss: 16.3375 - val_mae: 16.3375\n",
      "Epoch 1106/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7268 - mae: 16.7268 - val_loss: 16.3220 - val_mae: 16.3220\n",
      "Epoch 1107/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.7332 - mae: 16.7332 - val_loss: 16.3288 - val_mae: 16.3288\n",
      "Epoch 1108/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.7281 - mae: 16.7281 - val_loss: 16.3640 - val_mae: 16.3640\n",
      "Epoch 1109/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.7406 - mae: 16.7406 - val_loss: 16.3499 - val_mae: 16.3499\n",
      "Epoch 1110/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.7343 - mae: 16.7343 - val_loss: 16.3249 - val_mae: 16.3249\n",
      "Epoch 1111/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.7291 - mae: 16.7291 - val_loss: 16.3211 - val_mae: 16.3211\n",
      "Epoch 1112/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.7297 - mae: 16.7297 - val_loss: 16.3911 - val_mae: 16.3911\n",
      "Epoch 1113/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.7263 - mae: 16.7263 - val_loss: 16.3325 - val_mae: 16.3325\n",
      "Epoch 1114/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7329 - mae: 16.7329 - val_loss: 16.3613 - val_mae: 16.3613\n",
      "Epoch 1115/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.7331 - mae: 16.7331 - val_loss: 16.3223 - val_mae: 16.3223\n",
      "Epoch 1116/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7344 - mae: 16.7344 - val_loss: 16.3196 - val_mae: 16.3196\n",
      "Epoch 1117/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7340 - mae: 16.7340 - val_loss: 16.3193 - val_mae: 16.3193\n",
      "Epoch 1118/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7344 - mae: 16.7344 - val_loss: 16.3196 - val_mae: 16.3196\n",
      "Epoch 1119/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7281 - mae: 16.7281 - val_loss: 16.4239 - val_mae: 16.4239\n",
      "Epoch 1120/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7204 - mae: 16.7204 - val_loss: 16.4520 - val_mae: 16.4520\n",
      "Epoch 1121/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7318 - mae: 16.7318 - val_loss: 16.3364 - val_mae: 16.3364\n",
      "Epoch 1122/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7267 - mae: 16.7267 - val_loss: 16.3145 - val_mae: 16.3145\n",
      "Epoch 1123/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.7219 - mae: 16.7219 - val_loss: 16.3176 - val_mae: 16.3176\n",
      "Epoch 1124/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7254 - mae: 16.7254 - val_loss: 16.3298 - val_mae: 16.3298\n",
      "Epoch 1125/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7283 - mae: 16.7283 - val_loss: 16.3889 - val_mae: 16.3889\n",
      "Epoch 1126/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7240 - mae: 16.7240 - val_loss: 16.3170 - val_mae: 16.3170\n",
      "Epoch 1127/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7205 - mae: 16.7205 - val_loss: 16.3789 - val_mae: 16.3789\n",
      "Epoch 1128/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7225 - mae: 16.7225 - val_loss: 16.3159 - val_mae: 16.3159\n",
      "Epoch 1129/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7291 - mae: 16.7291 - val_loss: 16.3165 - val_mae: 16.3166\n",
      "Epoch 1130/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7206 - mae: 16.7206 - val_loss: 16.3547 - val_mae: 16.3547\n",
      "Epoch 1131/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7249 - mae: 16.7249 - val_loss: 16.3257 - val_mae: 16.3257\n",
      "Epoch 1132/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.7272 - mae: 16.7272 - val_loss: 16.3178 - val_mae: 16.3178\n",
      "Epoch 1133/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7250 - mae: 16.7250 - val_loss: 16.3212 - val_mae: 16.3212\n",
      "Epoch 1134/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7216 - mae: 16.7216 - val_loss: 16.3327 - val_mae: 16.3327\n",
      "Epoch 1135/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7267 - mae: 16.7267 - val_loss: 16.3174 - val_mae: 16.3175\n",
      "Epoch 1136/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7235 - mae: 16.7235 - val_loss: 16.3167 - val_mae: 16.3167\n",
      "Epoch 1137/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7269 - mae: 16.7269 - val_loss: 16.3154 - val_mae: 16.3154\n",
      "Epoch 1138/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7261 - mae: 16.7261 - val_loss: 16.3237 - val_mae: 16.3237\n",
      "Epoch 1139/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7276 - mae: 16.7276 - val_loss: 16.3177 - val_mae: 16.3177\n",
      "Epoch 1140/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7244 - mae: 16.7244 - val_loss: 16.3285 - val_mae: 16.3285\n",
      "Epoch 1141/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7168 - mae: 16.7169 - val_loss: 16.3204 - val_mae: 16.3203\n",
      "Epoch 1142/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7128 - mae: 16.7127 - val_loss: 16.3177 - val_mae: 16.3177\n",
      "Epoch 1143/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7201 - mae: 16.7201 - val_loss: 16.3257 - val_mae: 16.3257\n",
      "Epoch 1144/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7167 - mae: 16.7167 - val_loss: 16.3193 - val_mae: 16.3193\n",
      "Epoch 1145/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7244 - mae: 16.7244 - val_loss: 16.3294 - val_mae: 16.3294\n",
      "Epoch 1146/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7205 - mae: 16.7205 - val_loss: 16.3165 - val_mae: 16.3165\n",
      "Epoch 1147/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7211 - mae: 16.7211 - val_loss: 16.3406 - val_mae: 16.3406\n",
      "Epoch 1148/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7267 - mae: 16.7267 - val_loss: 16.3154 - val_mae: 16.3154\n",
      "Epoch 1149/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7181 - mae: 16.7181 - val_loss: 16.3581 - val_mae: 16.3581\n",
      "Epoch 1150/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7200 - mae: 16.7200 - val_loss: 16.3132 - val_mae: 16.3132\n",
      "Epoch 1151/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7199 - mae: 16.7199 - val_loss: 16.3141 - val_mae: 16.3141\n",
      "Epoch 1152/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7189 - mae: 16.7189 - val_loss: 16.3427 - val_mae: 16.3427\n",
      "Epoch 1153/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7231 - mae: 16.7231 - val_loss: 16.3259 - val_mae: 16.3259\n",
      "Epoch 1154/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7177 - mae: 16.7177 - val_loss: 16.3258 - val_mae: 16.3258\n",
      "Epoch 1155/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7139 - mae: 16.7139 - val_loss: 16.3121 - val_mae: 16.3121\n",
      "Epoch 1156/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7230 - mae: 16.7230 - val_loss: 16.3185 - val_mae: 16.3185\n",
      "Epoch 1157/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7128 - mae: 16.7128 - val_loss: 16.4238 - val_mae: 16.4238\n",
      "Epoch 1158/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7139 - mae: 16.7139 - val_loss: 16.3279 - val_mae: 16.3279\n",
      "Epoch 1159/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7230 - mae: 16.7230 - val_loss: 16.3092 - val_mae: 16.3092\n",
      "Epoch 1160/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.7107 - mae: 16.7107 - val_loss: 16.3523 - val_mae: 16.3523\n",
      "Epoch 1161/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7244 - mae: 16.7244 - val_loss: 16.3560 - val_mae: 16.3560\n",
      "Epoch 1162/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7105 - mae: 16.7105 - val_loss: 16.3138 - val_mae: 16.3138\n",
      "Epoch 1163/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7177 - mae: 16.7177 - val_loss: 16.3182 - val_mae: 16.3182\n",
      "Epoch 1164/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7039 - mae: 16.7039 - val_loss: 16.3150 - val_mae: 16.3150\n",
      "Epoch 1165/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7161 - mae: 16.7161 - val_loss: 16.3121 - val_mae: 16.3121\n",
      "Epoch 1166/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7049 - mae: 16.7049 - val_loss: 16.3672 - val_mae: 16.3672\n",
      "Epoch 1167/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7131 - mae: 16.7131 - val_loss: 16.3076 - val_mae: 16.3076\n",
      "Epoch 1168/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7045 - mae: 16.7045 - val_loss: 16.3094 - val_mae: 16.3094\n",
      "Epoch 1169/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7083 - mae: 16.7083 - val_loss: 16.3179 - val_mae: 16.3179\n",
      "Epoch 1170/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7096 - mae: 16.7096 - val_loss: 16.3310 - val_mae: 16.3310\n",
      "Epoch 1171/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7116 - mae: 16.7116 - val_loss: 16.3268 - val_mae: 16.3268\n",
      "Epoch 1172/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7155 - mae: 16.7155 - val_loss: 16.3089 - val_mae: 16.3090\n",
      "Epoch 1173/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7114 - mae: 16.7114 - val_loss: 16.3225 - val_mae: 16.3225\n",
      "Epoch 1174/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7158 - mae: 16.7158 - val_loss: 16.3336 - val_mae: 16.3336\n",
      "Epoch 1175/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7100 - mae: 16.7100 - val_loss: 16.3399 - val_mae: 16.3399\n",
      "Epoch 1176/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7059 - mae: 16.7059 - val_loss: 16.3271 - val_mae: 16.3271\n",
      "Epoch 1177/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7063 - mae: 16.7063 - val_loss: 16.3180 - val_mae: 16.3180\n",
      "Epoch 1178/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7114 - mae: 16.7114 - val_loss: 16.3860 - val_mae: 16.3860\n",
      "Epoch 1179/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7138 - mae: 16.7138 - val_loss: 16.3203 - val_mae: 16.3203\n",
      "Epoch 1180/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7119 - mae: 16.7119 - val_loss: 16.3303 - val_mae: 16.3303\n",
      "Epoch 1181/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7122 - mae: 16.7122 - val_loss: 16.3215 - val_mae: 16.3215\n",
      "Epoch 1182/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7048 - mae: 16.7048 - val_loss: 16.3969 - val_mae: 16.3969\n",
      "Epoch 1183/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7146 - mae: 16.7146 - val_loss: 16.3164 - val_mae: 16.3164\n",
      "Epoch 1184/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7090 - mae: 16.7090 - val_loss: 16.3621 - val_mae: 16.3621\n",
      "Epoch 1185/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7018 - mae: 16.7018 - val_loss: 16.3113 - val_mae: 16.3113\n",
      "Epoch 1186/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7085 - mae: 16.7085 - val_loss: 16.3108 - val_mae: 16.3108\n",
      "Epoch 1187/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7093 - mae: 16.7093 - val_loss: 16.3375 - val_mae: 16.3375\n",
      "Epoch 1188/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7098 - mae: 16.7098 - val_loss: 16.3174 - val_mae: 16.3174\n",
      "Epoch 1189/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7033 - mae: 16.7033 - val_loss: 16.3283 - val_mae: 16.3283\n",
      "Epoch 1190/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7046 - mae: 16.7046 - val_loss: 16.3082 - val_mae: 16.3082\n",
      "Epoch 1191/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7015 - mae: 16.7015 - val_loss: 16.3427 - val_mae: 16.3427\n",
      "Epoch 1192/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7055 - mae: 16.7055 - val_loss: 16.3180 - val_mae: 16.3180\n",
      "Epoch 1193/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7054 - mae: 16.7054 - val_loss: 16.3245 - val_mae: 16.3245\n",
      "Epoch 1194/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6980 - mae: 16.6980 - val_loss: 16.3286 - val_mae: 16.3286\n",
      "Epoch 1195/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7037 - mae: 16.7037 - val_loss: 16.3172 - val_mae: 16.3172\n",
      "Epoch 1196/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7096 - mae: 16.7096 - val_loss: 16.3132 - val_mae: 16.3132\n",
      "Epoch 1197/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6950 - mae: 16.6950 - val_loss: 16.3640 - val_mae: 16.3640\n",
      "Epoch 1198/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.7097 - mae: 16.7097 - val_loss: 16.3102 - val_mae: 16.3102\n",
      "Epoch 1199/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.7054 - mae: 16.7054 - val_loss: 16.4096 - val_mae: 16.4096\n",
      "Epoch 1200/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.6974 - mae: 16.6974 - val_loss: 16.3359 - val_mae: 16.3359\n",
      "Epoch 1201/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7035 - mae: 16.7035 - val_loss: 16.3246 - val_mae: 16.3246\n",
      "Epoch 1202/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7039 - mae: 16.7039 - val_loss: 16.3340 - val_mae: 16.3340\n",
      "Epoch 1203/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7044 - mae: 16.7044 - val_loss: 16.3141 - val_mae: 16.3141\n",
      "Epoch 1204/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7011 - mae: 16.7011 - val_loss: 16.3056 - val_mae: 16.3056\n",
      "Epoch 1205/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7013 - mae: 16.7013 - val_loss: 16.3464 - val_mae: 16.3464\n",
      "Epoch 1206/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.7087 - mae: 16.7087 - val_loss: 16.3035 - val_mae: 16.3035\n",
      "Epoch 1207/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7025 - mae: 16.7025 - val_loss: 16.3023 - val_mae: 16.3023\n",
      "Epoch 1208/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6855 - mae: 16.6855 - val_loss: 16.4422 - val_mae: 16.4422\n",
      "Epoch 1209/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7025 - mae: 16.7025 - val_loss: 16.3234 - val_mae: 16.3234\n",
      "Epoch 1210/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7057 - mae: 16.7057 - val_loss: 16.3291 - val_mae: 16.3291\n",
      "Epoch 1211/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7026 - mae: 16.7026 - val_loss: 16.3111 - val_mae: 16.3111\n",
      "Epoch 1212/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6982 - mae: 16.6982 - val_loss: 16.3494 - val_mae: 16.3494\n",
      "Epoch 1213/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7000 - mae: 16.7000 - val_loss: 16.3226 - val_mae: 16.3226\n",
      "Epoch 1214/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7037 - mae: 16.7037 - val_loss: 16.3421 - val_mae: 16.3421\n",
      "Epoch 1215/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6994 - mae: 16.6994 - val_loss: 16.3072 - val_mae: 16.3072\n",
      "Epoch 1216/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6999 - mae: 16.6999 - val_loss: 16.3125 - val_mae: 16.3125\n",
      "Epoch 1217/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6944 - mae: 16.6944 - val_loss: 16.3026 - val_mae: 16.3026\n",
      "Epoch 1218/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6972 - mae: 16.6972 - val_loss: 16.3190 - val_mae: 16.3190\n",
      "Epoch 1219/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7021 - mae: 16.7021 - val_loss: 16.3049 - val_mae: 16.3049\n",
      "Epoch 1220/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6993 - mae: 16.6993 - val_loss: 16.3556 - val_mae: 16.3556\n",
      "Epoch 1221/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6851 - mae: 16.6851 - val_loss: 16.3130 - val_mae: 16.3130\n",
      "Epoch 1222/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6995 - mae: 16.6995 - val_loss: 16.3407 - val_mae: 16.3407\n",
      "Epoch 1223/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6943 - mae: 16.6943 - val_loss: 16.3423 - val_mae: 16.3423\n",
      "Epoch 1224/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.7040 - mae: 16.7040 - val_loss: 16.3045 - val_mae: 16.3045\n",
      "Epoch 1225/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6929 - mae: 16.6929 - val_loss: 16.3088 - val_mae: 16.3088\n",
      "Epoch 1226/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7060 - mae: 16.7060 - val_loss: 16.3004 - val_mae: 16.3004\n",
      "Epoch 1227/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.7057 - mae: 16.7057 - val_loss: 16.3228 - val_mae: 16.3228\n",
      "Epoch 1228/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6998 - mae: 16.6998 - val_loss: 16.3007 - val_mae: 16.3007\n",
      "Epoch 1229/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6917 - mae: 16.6917 - val_loss: 16.3998 - val_mae: 16.3997\n",
      "Epoch 1230/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6902 - mae: 16.6902 - val_loss: 16.3017 - val_mae: 16.3017\n",
      "Epoch 1231/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6952 - mae: 16.6952 - val_loss: 16.3047 - val_mae: 16.3047\n",
      "Epoch 1232/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6930 - mae: 16.6930 - val_loss: 16.3491 - val_mae: 16.3491\n",
      "Epoch 1233/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6983 - mae: 16.6983 - val_loss: 16.3006 - val_mae: 16.3006\n",
      "Epoch 1234/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6978 - mae: 16.6978 - val_loss: 16.3004 - val_mae: 16.3004\n",
      "Epoch 1235/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6958 - mae: 16.6958 - val_loss: 16.3103 - val_mae: 16.3103\n",
      "Epoch 1236/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6954 - mae: 16.6954 - val_loss: 16.3277 - val_mae: 16.3277\n",
      "Epoch 1237/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6964 - mae: 16.6964 - val_loss: 16.3166 - val_mae: 16.3166\n",
      "Epoch 1238/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6975 - mae: 16.6975 - val_loss: 16.2992 - val_mae: 16.2992\n",
      "Epoch 1239/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6961 - mae: 16.6961 - val_loss: 16.2996 - val_mae: 16.2996\n",
      "Epoch 1240/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6833 - mae: 16.6833 - val_loss: 16.2986 - val_mae: 16.2986\n",
      "Epoch 1241/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6952 - mae: 16.6952 - val_loss: 16.3051 - val_mae: 16.3051\n",
      "Epoch 1242/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6973 - mae: 16.6973 - val_loss: 16.3113 - val_mae: 16.3113\n",
      "Epoch 1243/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6878 - mae: 16.6878 - val_loss: 16.3725 - val_mae: 16.3725\n",
      "Epoch 1244/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6902 - mae: 16.6902 - val_loss: 16.3348 - val_mae: 16.3348\n",
      "Epoch 1245/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6939 - mae: 16.6939 - val_loss: 16.3697 - val_mae: 16.3697\n",
      "Epoch 1246/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6823 - mae: 16.6823 - val_loss: 16.3496 - val_mae: 16.3495\n",
      "Epoch 1247/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6903 - mae: 16.6903 - val_loss: 16.3343 - val_mae: 16.3343\n",
      "Epoch 1248/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6973 - mae: 16.6973 - val_loss: 16.3344 - val_mae: 16.3344\n",
      "Epoch 1249/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6953 - mae: 16.6953 - val_loss: 16.3352 - val_mae: 16.3352\n",
      "Epoch 1250/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6905 - mae: 16.6905 - val_loss: 16.2990 - val_mae: 16.2990\n",
      "Epoch 1251/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6904 - mae: 16.6904 - val_loss: 16.3411 - val_mae: 16.3411\n",
      "Epoch 1252/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6798 - mae: 16.6798 - val_loss: 16.4002 - val_mae: 16.4003\n",
      "Epoch 1253/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6875 - mae: 16.6875 - val_loss: 16.3671 - val_mae: 16.3671\n",
      "Epoch 1254/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6873 - mae: 16.6873 - val_loss: 16.3031 - val_mae: 16.3031\n",
      "Epoch 1255/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6856 - mae: 16.6856 - val_loss: 16.3159 - val_mae: 16.3159\n",
      "Epoch 1256/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6813 - mae: 16.6813 - val_loss: 16.2928 - val_mae: 16.2928\n",
      "Epoch 1257/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6919 - mae: 16.6919 - val_loss: 16.3268 - val_mae: 16.3268\n",
      "Epoch 1258/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6844 - mae: 16.6844 - val_loss: 16.3192 - val_mae: 16.3192\n",
      "Epoch 1259/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6901 - mae: 16.6901 - val_loss: 16.2979 - val_mae: 16.2979\n",
      "Epoch 1260/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6924 - mae: 16.6924 - val_loss: 16.3002 - val_mae: 16.3002\n",
      "Epoch 1261/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6874 - mae: 16.6874 - val_loss: 16.3055 - val_mae: 16.3055\n",
      "Epoch 1262/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6951 - mae: 16.6951 - val_loss: 16.2947 - val_mae: 16.2947\n",
      "Epoch 1263/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6862 - mae: 16.6862 - val_loss: 16.3105 - val_mae: 16.3105\n",
      "Epoch 1264/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6861 - mae: 16.6861 - val_loss: 16.3092 - val_mae: 16.3092\n",
      "Epoch 1265/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6875 - mae: 16.6875 - val_loss: 16.3862 - val_mae: 16.3862\n",
      "Epoch 1266/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6850 - mae: 16.6850 - val_loss: 16.3455 - val_mae: 16.3455\n",
      "Epoch 1267/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6793 - mae: 16.6793 - val_loss: 16.2972 - val_mae: 16.2972\n",
      "Epoch 1268/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6827 - mae: 16.6827 - val_loss: 16.3020 - val_mae: 16.3020\n",
      "Epoch 1269/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6867 - mae: 16.6867 - val_loss: 16.3029 - val_mae: 16.3029\n",
      "Epoch 1270/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6840 - mae: 16.6840 - val_loss: 16.2972 - val_mae: 16.2972\n",
      "Epoch 1271/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6937 - mae: 16.6937 - val_loss: 16.3023 - val_mae: 16.3023\n",
      "Epoch 1272/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6835 - mae: 16.6835 - val_loss: 16.2941 - val_mae: 16.2941\n",
      "Epoch 1273/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6841 - mae: 16.6841 - val_loss: 16.3100 - val_mae: 16.3100\n",
      "Epoch 1274/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6850 - mae: 16.6850 - val_loss: 16.3020 - val_mae: 16.3020\n",
      "Epoch 1275/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6843 - mae: 16.6843 - val_loss: 16.2932 - val_mae: 16.2932\n",
      "Epoch 1276/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6829 - mae: 16.6829 - val_loss: 16.2972 - val_mae: 16.2972\n",
      "Epoch 1277/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6828 - mae: 16.6828 - val_loss: 16.3537 - val_mae: 16.3537\n",
      "Epoch 1278/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6856 - mae: 16.6856 - val_loss: 16.3004 - val_mae: 16.3004\n",
      "Epoch 1279/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6884 - mae: 16.6884 - val_loss: 16.3069 - val_mae: 16.3070\n",
      "Epoch 1280/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6827 - mae: 16.6827 - val_loss: 16.2979 - val_mae: 16.2979\n",
      "Epoch 1281/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6882 - mae: 16.6882 - val_loss: 16.3524 - val_mae: 16.3524\n",
      "Epoch 1282/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.6771 - mae: 16.6771 - val_loss: 16.2939 - val_mae: 16.2939\n",
      "Epoch 1283/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6807 - mae: 16.6807 - val_loss: 16.2951 - val_mae: 16.2951\n",
      "Epoch 1284/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6644 - mae: 16.6644 - val_loss: 16.2945 - val_mae: 16.2945\n",
      "Epoch 1285/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6873 - mae: 16.6873 - val_loss: 16.3144 - val_mae: 16.3144\n",
      "Epoch 1286/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6849 - mae: 16.6849 - val_loss: 16.2981 - val_mae: 16.2981\n",
      "Epoch 1287/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6773 - mae: 16.6773 - val_loss: 16.3848 - val_mae: 16.3848\n",
      "Epoch 1288/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6900 - mae: 16.6900 - val_loss: 16.2912 - val_mae: 16.2912\n",
      "Epoch 1289/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6807 - mae: 16.6807 - val_loss: 16.2914 - val_mae: 16.2914\n",
      "Epoch 1290/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6780 - mae: 16.6780 - val_loss: 16.3397 - val_mae: 16.3397\n",
      "Epoch 1291/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6873 - mae: 16.6873 - val_loss: 16.2910 - val_mae: 16.2910\n",
      "Epoch 1292/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6813 - mae: 16.6813 - val_loss: 16.3057 - val_mae: 16.3057\n",
      "Epoch 1293/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6757 - mae: 16.6757 - val_loss: 16.3266 - val_mae: 16.3266\n",
      "Epoch 1294/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.6822 - mae: 16.6822 - val_loss: 16.2945 - val_mae: 16.2945\n",
      "Epoch 1295/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6759 - mae: 16.6759 - val_loss: 16.3216 - val_mae: 16.3216\n",
      "Epoch 1296/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6734 - mae: 16.6734 - val_loss: 16.4529 - val_mae: 16.4529\n",
      "Epoch 1297/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6818 - mae: 16.6818 - val_loss: 16.2936 - val_mae: 16.2936\n",
      "Epoch 1298/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6790 - mae: 16.6790 - val_loss: 16.3344 - val_mae: 16.3344\n",
      "Epoch 1299/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6832 - mae: 16.6832 - val_loss: 16.2901 - val_mae: 16.2901\n",
      "Epoch 1300/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.6778 - mae: 16.6778 - val_loss: 16.3299 - val_mae: 16.3299\n",
      "Epoch 1301/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6799 - mae: 16.6799 - val_loss: 16.3808 - val_mae: 16.3808\n",
      "Epoch 1302/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6821 - mae: 16.6821 - val_loss: 16.2911 - val_mae: 16.2911\n",
      "Epoch 1303/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6764 - mae: 16.6764 - val_loss: 16.2903 - val_mae: 16.2903\n",
      "Epoch 1304/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6756 - mae: 16.6756 - val_loss: 16.3065 - val_mae: 16.3065\n",
      "Epoch 1305/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6794 - mae: 16.6794 - val_loss: 16.2941 - val_mae: 16.2941\n",
      "Epoch 1306/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6803 - mae: 16.6803 - val_loss: 16.3112 - val_mae: 16.3112\n",
      "Epoch 1307/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6808 - mae: 16.6808 - val_loss: 16.2935 - val_mae: 16.2935\n",
      "Epoch 1308/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6733 - mae: 16.6733 - val_loss: 16.4164 - val_mae: 16.4164\n",
      "Epoch 1309/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6739 - mae: 16.6739 - val_loss: 16.3107 - val_mae: 16.3107\n",
      "Epoch 1310/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6808 - mae: 16.6808 - val_loss: 16.3289 - val_mae: 16.3289\n",
      "Epoch 1311/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6673 - mae: 16.6673 - val_loss: 16.3044 - val_mae: 16.3044\n",
      "Epoch 1312/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6704 - mae: 16.6703 - val_loss: 16.3801 - val_mae: 16.3801\n",
      "Epoch 1313/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6738 - mae: 16.6738 - val_loss: 16.3264 - val_mae: 16.3264\n",
      "Epoch 1314/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6741 - mae: 16.6741 - val_loss: 16.3083 - val_mae: 16.3083\n",
      "Epoch 1315/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6639 - mae: 16.6639 - val_loss: 16.2919 - val_mae: 16.2919\n",
      "Epoch 1316/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6719 - mae: 16.6719 - val_loss: 16.2983 - val_mae: 16.2983\n",
      "Epoch 1317/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6724 - mae: 16.6724 - val_loss: 16.3070 - val_mae: 16.3070\n",
      "Epoch 1318/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6736 - mae: 16.6735 - val_loss: 16.3027 - val_mae: 16.3027\n",
      "Epoch 1319/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6800 - mae: 16.6800 - val_loss: 16.3147 - val_mae: 16.3147\n",
      "Epoch 1320/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6758 - mae: 16.6758 - val_loss: 16.2894 - val_mae: 16.2894\n",
      "Epoch 1321/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6775 - mae: 16.6775 - val_loss: 16.2861 - val_mae: 16.2861\n",
      "Epoch 1322/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6706 - mae: 16.6706 - val_loss: 16.2901 - val_mae: 16.2901\n",
      "Epoch 1323/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6782 - mae: 16.6782 - val_loss: 16.2898 - val_mae: 16.2898\n",
      "Epoch 1324/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6697 - mae: 16.6697 - val_loss: 16.3161 - val_mae: 16.3161\n",
      "Epoch 1325/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6742 - mae: 16.6742 - val_loss: 16.3194 - val_mae: 16.3194\n",
      "Epoch 1326/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6762 - mae: 16.6762 - val_loss: 16.3093 - val_mae: 16.3093\n",
      "Epoch 1327/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6738 - mae: 16.6738 - val_loss: 16.3130 - val_mae: 16.3130\n",
      "Epoch 1328/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6770 - mae: 16.6770 - val_loss: 16.2900 - val_mae: 16.2900\n",
      "Epoch 1329/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6645 - mae: 16.6645 - val_loss: 16.3171 - val_mae: 16.3171\n",
      "Epoch 1330/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6785 - mae: 16.6785 - val_loss: 16.2975 - val_mae: 16.2975\n",
      "Epoch 1331/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6709 - mae: 16.6709 - val_loss: 16.3436 - val_mae: 16.3436\n",
      "Epoch 1332/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6679 - mae: 16.6679 - val_loss: 16.2950 - val_mae: 16.2950\n",
      "Epoch 1333/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6707 - mae: 16.6707 - val_loss: 16.2872 - val_mae: 16.2872\n",
      "Epoch 1334/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6735 - mae: 16.6735 - val_loss: 16.3283 - val_mae: 16.3283\n",
      "Epoch 1335/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6700 - mae: 16.6700 - val_loss: 16.3223 - val_mae: 16.3223\n",
      "Epoch 1336/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6796 - mae: 16.6796 - val_loss: 16.3169 - val_mae: 16.3169\n",
      "Epoch 1337/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6726 - mae: 16.6726 - val_loss: 16.3812 - val_mae: 16.3812\n",
      "Epoch 1338/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6619 - mae: 16.6619 - val_loss: 16.2930 - val_mae: 16.2930\n",
      "Epoch 1339/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6673 - mae: 16.6673 - val_loss: 16.3043 - val_mae: 16.3043\n",
      "Epoch 1340/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6663 - mae: 16.6663 - val_loss: 16.3278 - val_mae: 16.3278\n",
      "Epoch 1341/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6663 - mae: 16.6663 - val_loss: 16.3447 - val_mae: 16.3447\n",
      "Epoch 1342/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6646 - mae: 16.6646 - val_loss: 16.3105 - val_mae: 16.3104\n",
      "Epoch 1343/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6718 - mae: 16.6718 - val_loss: 16.3072 - val_mae: 16.3072\n",
      "Epoch 1344/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6663 - mae: 16.6663 - val_loss: 16.2875 - val_mae: 16.2875\n",
      "Epoch 1345/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.6608 - mae: 16.6609 - val_loss: 16.2838 - val_mae: 16.2838\n",
      "Epoch 1346/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6625 - mae: 16.6625 - val_loss: 16.4275 - val_mae: 16.4275\n",
      "Epoch 1347/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6765 - mae: 16.6765 - val_loss: 16.2872 - val_mae: 16.2872\n",
      "Epoch 1348/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6685 - mae: 16.6685 - val_loss: 16.2855 - val_mae: 16.2855\n",
      "Epoch 1349/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6672 - mae: 16.6672 - val_loss: 16.3156 - val_mae: 16.3156\n",
      "Epoch 1350/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6632 - mae: 16.6632 - val_loss: 16.2969 - val_mae: 16.2969\n",
      "Epoch 1351/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6671 - mae: 16.6671 - val_loss: 16.2877 - val_mae: 16.2877\n",
      "Epoch 1352/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6640 - mae: 16.6640 - val_loss: 16.3228 - val_mae: 16.3228\n",
      "Epoch 1353/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6660 - mae: 16.6660 - val_loss: 16.3169 - val_mae: 16.3169\n",
      "Epoch 1354/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6689 - mae: 16.6689 - val_loss: 16.3181 - val_mae: 16.3181\n",
      "Epoch 1355/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6689 - mae: 16.6689 - val_loss: 16.3064 - val_mae: 16.3064\n",
      "Epoch 1356/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6669 - mae: 16.6668 - val_loss: 16.2843 - val_mae: 16.2843\n",
      "Epoch 1357/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6665 - mae: 16.6665 - val_loss: 16.2940 - val_mae: 16.2940\n",
      "Epoch 1358/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6671 - mae: 16.6671 - val_loss: 16.2815 - val_mae: 16.2815\n",
      "Epoch 1359/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6669 - mae: 16.6669 - val_loss: 16.2923 - val_mae: 16.2923\n",
      "Epoch 1360/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6715 - mae: 16.6715 - val_loss: 16.3356 - val_mae: 16.3356\n",
      "Epoch 1361/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6682 - mae: 16.6682 - val_loss: 16.2836 - val_mae: 16.2836\n",
      "Epoch 1362/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6632 - mae: 16.6632 - val_loss: 16.3112 - val_mae: 16.3112\n",
      "Epoch 1363/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6597 - mae: 16.6597 - val_loss: 16.2820 - val_mae: 16.2820\n",
      "Epoch 1364/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6643 - mae: 16.6643 - val_loss: 16.3025 - val_mae: 16.3025\n",
      "Epoch 1365/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6650 - mae: 16.6650 - val_loss: 16.3199 - val_mae: 16.3199\n",
      "Epoch 1366/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6589 - mae: 16.6589 - val_loss: 16.2993 - val_mae: 16.2993\n",
      "Epoch 1367/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6614 - mae: 16.6614 - val_loss: 16.2837 - val_mae: 16.2837\n",
      "Epoch 1368/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6631 - mae: 16.6632 - val_loss: 16.3092 - val_mae: 16.3092\n",
      "Epoch 1369/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6728 - mae: 16.6728 - val_loss: 16.3152 - val_mae: 16.3152\n",
      "Epoch 1370/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6597 - mae: 16.6597 - val_loss: 16.2907 - val_mae: 16.2907\n",
      "Epoch 1371/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6591 - mae: 16.6591 - val_loss: 16.2879 - val_mae: 16.2879\n",
      "Epoch 1372/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6666 - mae: 16.6666 - val_loss: 16.2869 - val_mae: 16.2869\n",
      "Epoch 1373/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6616 - mae: 16.6616 - val_loss: 16.3138 - val_mae: 16.3137\n",
      "Epoch 1374/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6527 - mae: 16.6527 - val_loss: 16.2810 - val_mae: 16.2810\n",
      "Epoch 1375/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6573 - mae: 16.6573 - val_loss: 16.3254 - val_mae: 16.3254\n",
      "Epoch 1376/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6366 - mae: 16.6366 - val_loss: 16.2773 - val_mae: 16.2773\n",
      "Epoch 1377/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6580 - mae: 16.6580 - val_loss: 16.2817 - val_mae: 16.2817\n",
      "Epoch 1378/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6599 - mae: 16.6599 - val_loss: 16.2808 - val_mae: 16.2808\n",
      "Epoch 1379/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6582 - mae: 16.6582 - val_loss: 16.3016 - val_mae: 16.3016\n",
      "Epoch 1380/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6614 - mae: 16.6614 - val_loss: 16.3263 - val_mae: 16.3263\n",
      "Epoch 1381/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.6595 - mae: 16.6595 - val_loss: 16.2805 - val_mae: 16.2804\n",
      "Epoch 1382/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6644 - mae: 16.6644 - val_loss: 16.3171 - val_mae: 16.3171\n",
      "Epoch 1383/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6599 - mae: 16.6599 - val_loss: 16.3295 - val_mae: 16.3295\n",
      "Epoch 1384/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6628 - mae: 16.6628 - val_loss: 16.2791 - val_mae: 16.2791\n",
      "Epoch 1385/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6568 - mae: 16.6568 - val_loss: 16.2837 - val_mae: 16.2837\n",
      "Epoch 1386/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6591 - mae: 16.6591 - val_loss: 16.2770 - val_mae: 16.2770\n",
      "Epoch 1387/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6625 - mae: 16.6625 - val_loss: 16.2781 - val_mae: 16.2781\n",
      "Epoch 1388/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6470 - mae: 16.6471 - val_loss: 16.2889 - val_mae: 16.2889\n",
      "Epoch 1389/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6604 - mae: 16.6604 - val_loss: 16.2845 - val_mae: 16.2845\n",
      "Epoch 1390/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6518 - mae: 16.6518 - val_loss: 16.3377 - val_mae: 16.3377\n",
      "Epoch 1391/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6599 - mae: 16.6599 - val_loss: 16.2889 - val_mae: 16.2889\n",
      "Epoch 1392/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6522 - mae: 16.6522 - val_loss: 16.3156 - val_mae: 16.3156\n",
      "Epoch 1393/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6550 - mae: 16.6550 - val_loss: 16.3341 - val_mae: 16.3341\n",
      "Epoch 1394/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.6584 - mae: 16.6584 - val_loss: 16.2772 - val_mae: 16.2772\n",
      "Epoch 1395/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6563 - mae: 16.6563 - val_loss: 16.2851 - val_mae: 16.2851\n",
      "Epoch 1396/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.6537 - mae: 16.6537 - val_loss: 16.2973 - val_mae: 16.2973\n",
      "Epoch 1397/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6576 - mae: 16.6576 - val_loss: 16.2830 - val_mae: 16.2830\n",
      "Epoch 1398/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6513 - mae: 16.6513 - val_loss: 16.2756 - val_mae: 16.2756\n",
      "Epoch 1399/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6570 - mae: 16.6570 - val_loss: 16.3158 - val_mae: 16.3158\n",
      "Epoch 1400/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6541 - mae: 16.6541 - val_loss: 16.3389 - val_mae: 16.3389\n",
      "Epoch 1401/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6511 - mae: 16.6511 - val_loss: 16.2795 - val_mae: 16.2795\n",
      "Epoch 1402/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6555 - mae: 16.6555 - val_loss: 16.2751 - val_mae: 16.2751\n",
      "Epoch 1403/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6566 - mae: 16.6566 - val_loss: 16.2918 - val_mae: 16.2918\n",
      "Epoch 1404/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6537 - mae: 16.6537 - val_loss: 16.2909 - val_mae: 16.2909\n",
      "Epoch 1405/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6515 - mae: 16.6515 - val_loss: 16.2968 - val_mae: 16.2968\n",
      "Epoch 1406/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6466 - mae: 16.6466 - val_loss: 16.2828 - val_mae: 16.2828\n",
      "Epoch 1407/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6507 - mae: 16.6507 - val_loss: 16.2887 - val_mae: 16.2887\n",
      "Epoch 1408/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6533 - mae: 16.6532 - val_loss: 16.3021 - val_mae: 16.3021\n",
      "Epoch 1409/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6576 - mae: 16.6576 - val_loss: 16.2978 - val_mae: 16.2978\n",
      "Epoch 1410/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6505 - mae: 16.6505 - val_loss: 16.3660 - val_mae: 16.3660\n",
      "Epoch 1411/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6564 - mae: 16.6564 - val_loss: 16.2882 - val_mae: 16.2882\n",
      "Epoch 1412/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6577 - mae: 16.6577 - val_loss: 16.2759 - val_mae: 16.2759\n",
      "Epoch 1413/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6521 - mae: 16.6521 - val_loss: 16.2757 - val_mae: 16.2757\n",
      "Epoch 1414/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6561 - mae: 16.6561 - val_loss: 16.3419 - val_mae: 16.3419\n",
      "Epoch 1415/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6518 - mae: 16.6518 - val_loss: 16.3073 - val_mae: 16.3073\n",
      "Epoch 1416/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6490 - mae: 16.6490 - val_loss: 16.2778 - val_mae: 16.2778\n",
      "Epoch 1417/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6531 - mae: 16.6531 - val_loss: 16.2798 - val_mae: 16.2798\n",
      "Epoch 1418/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6505 - mae: 16.6505 - val_loss: 16.2799 - val_mae: 16.2800\n",
      "Epoch 1419/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6499 - mae: 16.6499 - val_loss: 16.3575 - val_mae: 16.3575\n",
      "Epoch 1420/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6523 - mae: 16.6523 - val_loss: 16.2911 - val_mae: 16.2911\n",
      "Epoch 1421/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6490 - mae: 16.6490 - val_loss: 16.2809 - val_mae: 16.2809\n",
      "Epoch 1422/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6516 - mae: 16.6516 - val_loss: 16.3215 - val_mae: 16.3215\n",
      "Epoch 1423/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6478 - mae: 16.6478 - val_loss: 16.3308 - val_mae: 16.3308\n",
      "Epoch 1424/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6503 - mae: 16.6503 - val_loss: 16.2974 - val_mae: 16.2974\n",
      "Epoch 1425/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6457 - mae: 16.6457 - val_loss: 16.2881 - val_mae: 16.2881\n",
      "Epoch 1426/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6524 - mae: 16.6524 - val_loss: 16.3007 - val_mae: 16.3007\n",
      "Epoch 1427/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6534 - mae: 16.6534 - val_loss: 16.2855 - val_mae: 16.2855\n",
      "Epoch 1428/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6521 - mae: 16.6521 - val_loss: 16.2828 - val_mae: 16.2828\n",
      "Epoch 1429/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6428 - mae: 16.6428 - val_loss: 16.2936 - val_mae: 16.2935\n",
      "Epoch 1430/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.6505 - mae: 16.6505 - val_loss: 16.3038 - val_mae: 16.3038\n",
      "Epoch 1431/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6504 - mae: 16.6504 - val_loss: 16.3020 - val_mae: 16.3020\n",
      "Epoch 1432/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6525 - mae: 16.6526 - val_loss: 16.2736 - val_mae: 16.2736\n",
      "Epoch 1433/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6483 - mae: 16.6483 - val_loss: 16.2866 - val_mae: 16.2866\n",
      "Epoch 1434/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6465 - mae: 16.6465 - val_loss: 16.2977 - val_mae: 16.2977\n",
      "Epoch 1435/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6448 - mae: 16.6448 - val_loss: 16.2766 - val_mae: 16.2766\n",
      "Epoch 1436/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6446 - mae: 16.6446 - val_loss: 16.2786 - val_mae: 16.2785\n",
      "Epoch 1437/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6510 - mae: 16.6510 - val_loss: 16.2942 - val_mae: 16.2942\n",
      "Epoch 1438/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6412 - mae: 16.6413 - val_loss: 16.2749 - val_mae: 16.2749\n",
      "Epoch 1439/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6484 - mae: 16.6484 - val_loss: 16.2778 - val_mae: 16.2778\n",
      "Epoch 1440/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6491 - mae: 16.6491 - val_loss: 16.3418 - val_mae: 16.3418\n",
      "Epoch 1441/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.6539 - mae: 16.6538 - val_loss: 16.2945 - val_mae: 16.2945\n",
      "Epoch 1442/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.6485 - mae: 16.6486 - val_loss: 16.2761 - val_mae: 16.2761\n",
      "Epoch 1443/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6477 - mae: 16.6477 - val_loss: 16.3203 - val_mae: 16.3203\n",
      "Epoch 1444/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.6424 - mae: 16.6424 - val_loss: 16.2968 - val_mae: 16.2969\n",
      "Epoch 1445/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6431 - mae: 16.6431 - val_loss: 16.2784 - val_mae: 16.2784\n",
      "Epoch 1446/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6440 - mae: 16.6440 - val_loss: 16.3046 - val_mae: 16.3046\n",
      "Epoch 1447/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.6471 - mae: 16.6471 - val_loss: 16.2975 - val_mae: 16.2975\n",
      "Epoch 1448/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6540 - mae: 16.6540 - val_loss: 16.2857 - val_mae: 16.2857\n",
      "Epoch 1449/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.6439 - mae: 16.6439 - val_loss: 16.2736 - val_mae: 16.2736\n",
      "Epoch 1450/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6406 - mae: 16.6406 - val_loss: 16.2924 - val_mae: 16.2924\n",
      "Epoch 1451/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6481 - mae: 16.6481 - val_loss: 16.2880 - val_mae: 16.2880\n",
      "Epoch 1452/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.6439 - mae: 16.6439 - val_loss: 16.3193 - val_mae: 16.3193\n",
      "Epoch 1453/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.6424 - mae: 16.6424 - val_loss: 16.2805 - val_mae: 16.2805\n",
      "Epoch 1454/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6434 - mae: 16.6434 - val_loss: 16.3205 - val_mae: 16.3205\n",
      "Epoch 1455/3000\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 16.6410 - mae: 16.6409 - val_loss: 16.2835 - val_mae: 16.2835\n",
      "Epoch 1456/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6446 - mae: 16.6446 - val_loss: 16.2926 - val_mae: 16.2926\n",
      "Epoch 1457/3000\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 16.6494 - mae: 16.6494 - val_loss: 16.3331 - val_mae: 16.3331\n",
      "Epoch 1458/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6442 - mae: 16.6442 - val_loss: 16.2742 - val_mae: 16.2742\n",
      "Epoch 1459/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6414 - mae: 16.6414 - val_loss: 16.2668 - val_mae: 16.2668\n",
      "Epoch 1460/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6427 - mae: 16.6427 - val_loss: 16.3029 - val_mae: 16.3029\n",
      "Epoch 1461/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.6388 - mae: 16.6387 - val_loss: 16.2743 - val_mae: 16.2743\n",
      "Epoch 1462/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6455 - mae: 16.6456 - val_loss: 16.3257 - val_mae: 16.3257\n",
      "Epoch 1463/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.6441 - mae: 16.6441 - val_loss: 16.3344 - val_mae: 16.3344\n",
      "Epoch 1464/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6497 - mae: 16.6497 - val_loss: 16.2845 - val_mae: 16.2845\n",
      "Epoch 1465/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6411 - mae: 16.6411 - val_loss: 16.2727 - val_mae: 16.2727\n",
      "Epoch 1466/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.6427 - mae: 16.6427 - val_loss: 16.2947 - val_mae: 16.2947\n",
      "Epoch 1467/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.6392 - mae: 16.6392 - val_loss: 16.3144 - val_mae: 16.3144\n",
      "Epoch 1468/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6473 - mae: 16.6473 - val_loss: 16.2829 - val_mae: 16.2829\n",
      "Epoch 1469/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.6405 - mae: 16.6405 - val_loss: 16.3213 - val_mae: 16.3213\n",
      "Epoch 1470/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6366 - mae: 16.6366 - val_loss: 16.2871 - val_mae: 16.2871\n",
      "Epoch 1471/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6398 - mae: 16.6398 - val_loss: 16.2766 - val_mae: 16.2766\n",
      "Epoch 1472/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6416 - mae: 16.6417 - val_loss: 16.3055 - val_mae: 16.3055\n",
      "Epoch 1473/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6372 - mae: 16.6372 - val_loss: 16.2910 - val_mae: 16.2910\n",
      "Epoch 1474/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6446 - mae: 16.6446 - val_loss: 16.2702 - val_mae: 16.2702\n",
      "Epoch 1475/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6344 - mae: 16.6344 - val_loss: 16.2746 - val_mae: 16.2746\n",
      "Epoch 1476/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6382 - mae: 16.6382 - val_loss: 16.3138 - val_mae: 16.3138\n",
      "Epoch 1477/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6404 - mae: 16.6404 - val_loss: 16.2708 - val_mae: 16.2708\n",
      "Epoch 1478/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6367 - mae: 16.6367 - val_loss: 16.2766 - val_mae: 16.2766\n",
      "Epoch 1479/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6381 - mae: 16.6381 - val_loss: 16.2792 - val_mae: 16.2792\n",
      "Epoch 1480/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6290 - mae: 16.6290 - val_loss: 16.2812 - val_mae: 16.2812\n",
      "Epoch 1481/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6361 - mae: 16.6361 - val_loss: 16.2930 - val_mae: 16.2930\n",
      "Epoch 1482/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6400 - mae: 16.6400 - val_loss: 16.2731 - val_mae: 16.2731\n",
      "Epoch 1483/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6404 - mae: 16.6404 - val_loss: 16.2719 - val_mae: 16.2719\n",
      "Epoch 1484/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6363 - mae: 16.6363 - val_loss: 16.2707 - val_mae: 16.2707\n",
      "Epoch 1485/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6397 - mae: 16.6397 - val_loss: 16.2696 - val_mae: 16.2696\n",
      "Epoch 1486/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6381 - mae: 16.6381 - val_loss: 16.2809 - val_mae: 16.2809\n",
      "Epoch 1487/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6369 - mae: 16.6369 - val_loss: 16.3061 - val_mae: 16.3061\n",
      "Epoch 1488/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6386 - mae: 16.6386 - val_loss: 16.2712 - val_mae: 16.2712\n",
      "Epoch 1489/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6385 - mae: 16.6385 - val_loss: 16.2681 - val_mae: 16.2681\n",
      "Epoch 1490/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6362 - mae: 16.6362 - val_loss: 16.2678 - val_mae: 16.2678\n",
      "Epoch 1491/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6341 - mae: 16.6341 - val_loss: 16.3324 - val_mae: 16.3324\n",
      "Epoch 1492/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6337 - mae: 16.6337 - val_loss: 16.2832 - val_mae: 16.2832\n",
      "Epoch 1493/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6383 - mae: 16.6382 - val_loss: 16.2677 - val_mae: 16.2677\n",
      "Epoch 1494/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6389 - mae: 16.6389 - val_loss: 16.2828 - val_mae: 16.2828\n",
      "Epoch 1495/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6409 - mae: 16.6409 - val_loss: 16.2696 - val_mae: 16.2696\n",
      "Epoch 1496/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6381 - mae: 16.6381 - val_loss: 16.3219 - val_mae: 16.3220\n",
      "Epoch 1497/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6370 - mae: 16.6370 - val_loss: 16.2674 - val_mae: 16.2674\n",
      "Epoch 1498/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6357 - mae: 16.6357 - val_loss: 16.2703 - val_mae: 16.2703\n",
      "Epoch 1499/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6328 - mae: 16.6328 - val_loss: 16.2864 - val_mae: 16.2864\n",
      "Epoch 1500/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6179 - mae: 16.6179 - val_loss: 16.2789 - val_mae: 16.2789\n",
      "Epoch 1501/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6349 - mae: 16.6349 - val_loss: 16.3019 - val_mae: 16.3019\n",
      "Epoch 1502/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6422 - mae: 16.6422 - val_loss: 16.3159 - val_mae: 16.3159\n",
      "Epoch 1503/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6358 - mae: 16.6358 - val_loss: 16.3001 - val_mae: 16.3001\n",
      "Epoch 1504/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6336 - mae: 16.6336 - val_loss: 16.3392 - val_mae: 16.3392\n",
      "Epoch 1505/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6372 - mae: 16.6371 - val_loss: 16.2884 - val_mae: 16.2884\n",
      "Epoch 1506/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6317 - mae: 16.6317 - val_loss: 16.3608 - val_mae: 16.3608\n",
      "Epoch 1507/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6216 - mae: 16.6216 - val_loss: 16.3338 - val_mae: 16.3338\n",
      "Epoch 1508/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6364 - mae: 16.6364 - val_loss: 16.2661 - val_mae: 16.2661\n",
      "Epoch 1509/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6295 - mae: 16.6295 - val_loss: 16.2820 - val_mae: 16.2820\n",
      "Epoch 1510/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6294 - mae: 16.6294 - val_loss: 16.2836 - val_mae: 16.2836\n",
      "Epoch 1511/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6328 - mae: 16.6328 - val_loss: 16.2710 - val_mae: 16.2710\n",
      "Epoch 1512/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6116 - mae: 16.6116 - val_loss: 16.4246 - val_mae: 16.4246\n",
      "Epoch 1513/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6362 - mae: 16.6362 - val_loss: 16.3408 - val_mae: 16.3408\n",
      "Epoch 1514/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6302 - mae: 16.6302 - val_loss: 16.3159 - val_mae: 16.3159\n",
      "Epoch 1515/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6276 - mae: 16.6276 - val_loss: 16.2892 - val_mae: 16.2892\n",
      "Epoch 1516/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6305 - mae: 16.6305 - val_loss: 16.2744 - val_mae: 16.2744\n",
      "Epoch 1517/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6241 - mae: 16.6241 - val_loss: 16.2837 - val_mae: 16.2837\n",
      "Epoch 1518/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6321 - mae: 16.6321 - val_loss: 16.2739 - val_mae: 16.2739\n",
      "Epoch 1519/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6317 - mae: 16.6317 - val_loss: 16.2867 - val_mae: 16.2867\n",
      "Epoch 1520/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6334 - mae: 16.6334 - val_loss: 16.2816 - val_mae: 16.2816\n",
      "Epoch 1521/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6331 - mae: 16.6331 - val_loss: 16.2726 - val_mae: 16.2726\n",
      "Epoch 1522/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6327 - mae: 16.6327 - val_loss: 16.2645 - val_mae: 16.2645\n",
      "Epoch 1523/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6243 - mae: 16.6243 - val_loss: 16.2883 - val_mae: 16.2883\n",
      "Epoch 1524/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6243 - mae: 16.6243 - val_loss: 16.2708 - val_mae: 16.2708\n",
      "Epoch 1525/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6319 - mae: 16.6319 - val_loss: 16.3198 - val_mae: 16.3198\n",
      "Epoch 1526/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6246 - mae: 16.6246 - val_loss: 16.2726 - val_mae: 16.2726\n",
      "Epoch 1527/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6267 - mae: 16.6267 - val_loss: 16.2616 - val_mae: 16.2616\n",
      "Epoch 1528/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6340 - mae: 16.6340 - val_loss: 16.2741 - val_mae: 16.2741\n",
      "Epoch 1529/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6321 - mae: 16.6321 - val_loss: 16.2649 - val_mae: 16.2649\n",
      "Epoch 1530/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6242 - mae: 16.6242 - val_loss: 16.3028 - val_mae: 16.3028\n",
      "Epoch 1531/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6212 - mae: 16.6212 - val_loss: 16.2833 - val_mae: 16.2833\n",
      "Epoch 1532/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6309 - mae: 16.6309 - val_loss: 16.3314 - val_mae: 16.3314\n",
      "Epoch 1533/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6308 - mae: 16.6308 - val_loss: 16.2788 - val_mae: 16.2788\n",
      "Epoch 1534/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6318 - mae: 16.6318 - val_loss: 16.2613 - val_mae: 16.2613\n",
      "Epoch 1535/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6244 - mae: 16.6244 - val_loss: 16.3192 - val_mae: 16.3192\n",
      "Epoch 1536/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6310 - mae: 16.6310 - val_loss: 16.2989 - val_mae: 16.2989\n",
      "Epoch 1537/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6180 - mae: 16.6180 - val_loss: 16.2910 - val_mae: 16.2910\n",
      "Epoch 1538/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6292 - mae: 16.6292 - val_loss: 16.2959 - val_mae: 16.2959\n",
      "Epoch 1539/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6317 - mae: 16.6317 - val_loss: 16.2650 - val_mae: 16.2650\n",
      "Epoch 1540/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6234 - mae: 16.6234 - val_loss: 16.2833 - val_mae: 16.2833\n",
      "Epoch 1541/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6280 - mae: 16.6280 - val_loss: 16.2670 - val_mae: 16.2670\n",
      "Epoch 1542/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6298 - mae: 16.6298 - val_loss: 16.2635 - val_mae: 16.2635\n",
      "Epoch 1543/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6230 - mae: 16.6231 - val_loss: 16.3052 - val_mae: 16.3052\n",
      "Epoch 1544/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6262 - mae: 16.6262 - val_loss: 16.2628 - val_mae: 16.2627\n",
      "Epoch 1545/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6278 - mae: 16.6278 - val_loss: 16.2843 - val_mae: 16.2843\n",
      "Epoch 1546/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6284 - mae: 16.6284 - val_loss: 16.2725 - val_mae: 16.2725\n",
      "Epoch 1547/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6270 - mae: 16.6270 - val_loss: 16.2664 - val_mae: 16.2664\n",
      "Epoch 1548/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6262 - mae: 16.6262 - val_loss: 16.2688 - val_mae: 16.2688\n",
      "Epoch 1549/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6278 - mae: 16.6278 - val_loss: 16.2980 - val_mae: 16.2980\n",
      "Epoch 1550/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6177 - mae: 16.6177 - val_loss: 16.2774 - val_mae: 16.2774\n",
      "Epoch 1551/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6211 - mae: 16.6211 - val_loss: 16.2954 - val_mae: 16.2954\n",
      "Epoch 1552/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6202 - mae: 16.6202 - val_loss: 16.2880 - val_mae: 16.2880\n",
      "Epoch 1553/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6227 - mae: 16.6227 - val_loss: 16.2632 - val_mae: 16.2632\n",
      "Epoch 1554/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6269 - mae: 16.6269 - val_loss: 16.2671 - val_mae: 16.2671\n",
      "Epoch 1555/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6236 - mae: 16.6236 - val_loss: 16.2775 - val_mae: 16.2776\n",
      "Epoch 1556/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6270 - mae: 16.6270 - val_loss: 16.2817 - val_mae: 16.2817\n",
      "Epoch 1557/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6156 - mae: 16.6156 - val_loss: 16.3583 - val_mae: 16.3583\n",
      "Epoch 1558/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6238 - mae: 16.6238 - val_loss: 16.2617 - val_mae: 16.2617\n",
      "Epoch 1559/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6204 - mae: 16.6204 - val_loss: 16.2669 - val_mae: 16.2669\n",
      "Epoch 1560/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.6232 - mae: 16.6232 - val_loss: 16.2683 - val_mae: 16.2683\n",
      "Epoch 1561/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6269 - mae: 16.6269 - val_loss: 16.2691 - val_mae: 16.2691\n",
      "Epoch 1562/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6244 - mae: 16.6244 - val_loss: 16.2908 - val_mae: 16.2908\n",
      "Epoch 1563/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6203 - mae: 16.6203 - val_loss: 16.2656 - val_mae: 16.2656\n",
      "Epoch 1564/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6233 - mae: 16.6233 - val_loss: 16.2869 - val_mae: 16.2869\n",
      "Epoch 1565/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6231 - mae: 16.6231 - val_loss: 16.2603 - val_mae: 16.2603\n",
      "Epoch 1566/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6149 - mae: 16.6149 - val_loss: 16.4021 - val_mae: 16.4021\n",
      "Epoch 1567/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6288 - mae: 16.6288 - val_loss: 16.2607 - val_mae: 16.2607\n",
      "Epoch 1568/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6245 - mae: 16.6245 - val_loss: 16.2603 - val_mae: 16.2603\n",
      "Epoch 1569/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6202 - mae: 16.6201 - val_loss: 16.2935 - val_mae: 16.2934\n",
      "Epoch 1570/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6217 - mae: 16.6217 - val_loss: 16.2620 - val_mae: 16.2619\n",
      "Epoch 1571/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6257 - mae: 16.6257 - val_loss: 16.2617 - val_mae: 16.2617\n",
      "Epoch 1572/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6191 - mae: 16.6191 - val_loss: 16.2812 - val_mae: 16.2812\n",
      "Epoch 1573/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6278 - mae: 16.6278 - val_loss: 16.2676 - val_mae: 16.2676\n",
      "Epoch 1574/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6254 - mae: 16.6254 - val_loss: 16.2724 - val_mae: 16.2724\n",
      "Epoch 1575/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6223 - mae: 16.6223 - val_loss: 16.2811 - val_mae: 16.2811\n",
      "Epoch 1576/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6174 - mae: 16.6174 - val_loss: 16.2645 - val_mae: 16.2645\n",
      "Epoch 1577/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6242 - mae: 16.6242 - val_loss: 16.2697 - val_mae: 16.2697\n",
      "Epoch 1578/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6232 - mae: 16.6232 - val_loss: 16.2602 - val_mae: 16.2602\n",
      "Epoch 1579/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6191 - mae: 16.6191 - val_loss: 16.2625 - val_mae: 16.2625\n",
      "Epoch 1580/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6173 - mae: 16.6173 - val_loss: 16.2604 - val_mae: 16.2604\n",
      "Epoch 1581/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6212 - mae: 16.6212 - val_loss: 16.2607 - val_mae: 16.2607\n",
      "Epoch 1582/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6164 - mae: 16.6164 - val_loss: 16.2636 - val_mae: 16.2636\n",
      "Epoch 1583/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6245 - mae: 16.6245 - val_loss: 16.2651 - val_mae: 16.2651\n",
      "Epoch 1584/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6220 - mae: 16.6220 - val_loss: 16.2606 - val_mae: 16.2606\n",
      "Epoch 1585/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6166 - mae: 16.6166 - val_loss: 16.2595 - val_mae: 16.2595\n",
      "Epoch 1586/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6110 - mae: 16.6110 - val_loss: 16.3147 - val_mae: 16.3147\n",
      "Epoch 1587/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6233 - mae: 16.6233 - val_loss: 16.3026 - val_mae: 16.3026\n",
      "Epoch 1588/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6181 - mae: 16.6181 - val_loss: 16.2821 - val_mae: 16.2821\n",
      "Epoch 1589/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6164 - mae: 16.6164 - val_loss: 16.2602 - val_mae: 16.2602\n",
      "Epoch 1590/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6148 - mae: 16.6148 - val_loss: 16.2884 - val_mae: 16.2884\n",
      "Epoch 1591/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6179 - mae: 16.6179 - val_loss: 16.2765 - val_mae: 16.2765\n",
      "Epoch 1592/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6140 - mae: 16.6140 - val_loss: 16.3480 - val_mae: 16.3480\n",
      "Epoch 1593/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6191 - mae: 16.6191 - val_loss: 16.2588 - val_mae: 16.2588\n",
      "Epoch 1594/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6190 - mae: 16.6190 - val_loss: 16.2781 - val_mae: 16.2780\n",
      "Epoch 1595/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6152 - mae: 16.6152 - val_loss: 16.3223 - val_mae: 16.3223\n",
      "Epoch 1596/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6186 - mae: 16.6186 - val_loss: 16.3013 - val_mae: 16.3013\n",
      "Epoch 1597/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6171 - mae: 16.6171 - val_loss: 16.3277 - val_mae: 16.3277\n",
      "Epoch 1598/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6164 - mae: 16.6164 - val_loss: 16.2957 - val_mae: 16.2957\n",
      "Epoch 1599/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6069 - mae: 16.6069 - val_loss: 16.2662 - val_mae: 16.2662\n",
      "Epoch 1600/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6155 - mae: 16.6155 - val_loss: 16.2596 - val_mae: 16.2596\n",
      "Epoch 1601/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6134 - mae: 16.6134 - val_loss: 16.2977 - val_mae: 16.2977\n",
      "Epoch 1602/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6120 - mae: 16.6120 - val_loss: 16.2851 - val_mae: 16.2851\n",
      "Epoch 1603/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6140 - mae: 16.6140 - val_loss: 16.2637 - val_mae: 16.2637\n",
      "Epoch 1604/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6124 - mae: 16.6124 - val_loss: 16.3162 - val_mae: 16.3162\n",
      "Epoch 1605/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6168 - mae: 16.6168 - val_loss: 16.2771 - val_mae: 16.2771\n",
      "Epoch 1606/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6108 - mae: 16.6108 - val_loss: 16.2597 - val_mae: 16.2597\n",
      "Epoch 1607/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6235 - mae: 16.6235 - val_loss: 16.2962 - val_mae: 16.2962\n",
      "Epoch 1608/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6076 - mae: 16.6076 - val_loss: 16.2590 - val_mae: 16.2590\n",
      "Epoch 1609/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6125 - mae: 16.6125 - val_loss: 16.2619 - val_mae: 16.2619\n",
      "Epoch 1610/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6053 - mae: 16.6053 - val_loss: 16.2806 - val_mae: 16.2806\n",
      "Epoch 1611/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6134 - mae: 16.6134 - val_loss: 16.2827 - val_mae: 16.2827\n",
      "Epoch 1612/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6071 - mae: 16.6071 - val_loss: 16.2560 - val_mae: 16.2560\n",
      "Epoch 1613/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6185 - mae: 16.6184 - val_loss: 16.2938 - val_mae: 16.2938\n",
      "Epoch 1614/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6105 - mae: 16.6105 - val_loss: 16.3451 - val_mae: 16.3451\n",
      "Epoch 1615/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6149 - mae: 16.6149 - val_loss: 16.3219 - val_mae: 16.3219\n",
      "Epoch 1616/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6107 - mae: 16.6107 - val_loss: 16.2736 - val_mae: 16.2736\n",
      "Epoch 1617/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6140 - mae: 16.6140 - val_loss: 16.2681 - val_mae: 16.2681\n",
      "Epoch 1618/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6103 - mae: 16.6103 - val_loss: 16.2824 - val_mae: 16.2824\n",
      "Epoch 1619/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6088 - mae: 16.6087 - val_loss: 16.2689 - val_mae: 16.2689\n",
      "Epoch 1620/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6135 - mae: 16.6135 - val_loss: 16.2665 - val_mae: 16.2665\n",
      "Epoch 1621/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6141 - mae: 16.6141 - val_loss: 16.3037 - val_mae: 16.3037\n",
      "Epoch 1622/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6098 - mae: 16.6098 - val_loss: 16.2636 - val_mae: 16.2636\n",
      "Epoch 1623/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6051 - mae: 16.6051 - val_loss: 16.2730 - val_mae: 16.2730\n",
      "Epoch 1624/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6081 - mae: 16.6081 - val_loss: 16.3461 - val_mae: 16.3461\n",
      "Epoch 1625/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6163 - mae: 16.6163 - val_loss: 16.2780 - val_mae: 16.2780\n",
      "Epoch 1626/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.6103 - mae: 16.6103 - val_loss: 16.2953 - val_mae: 16.2953\n",
      "Epoch 1627/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6151 - mae: 16.6151 - val_loss: 16.2603 - val_mae: 16.2603\n",
      "Epoch 1628/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.6122 - mae: 16.6122 - val_loss: 16.2648 - val_mae: 16.2648\n",
      "Epoch 1629/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6130 - mae: 16.6130 - val_loss: 16.3155 - val_mae: 16.3155\n",
      "Epoch 1630/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6121 - mae: 16.6121 - val_loss: 16.2572 - val_mae: 16.2572\n",
      "Epoch 1631/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6119 - mae: 16.6119 - val_loss: 16.2671 - val_mae: 16.2671\n",
      "Epoch 1632/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6034 - mae: 16.6034 - val_loss: 16.2626 - val_mae: 16.2626\n",
      "Epoch 1633/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6120 - mae: 16.6120 - val_loss: 16.2777 - val_mae: 16.2777\n",
      "Epoch 1634/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6016 - mae: 16.6016 - val_loss: 16.4290 - val_mae: 16.4290\n",
      "Epoch 1635/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6098 - mae: 16.6098 - val_loss: 16.2590 - val_mae: 16.2590\n",
      "Epoch 1636/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6078 - mae: 16.6077 - val_loss: 16.2814 - val_mae: 16.2814\n",
      "Epoch 1637/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6096 - mae: 16.6096 - val_loss: 16.2551 - val_mae: 16.2551\n",
      "Epoch 1638/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6042 - mae: 16.6042 - val_loss: 16.2710 - val_mae: 16.2710\n",
      "Epoch 1639/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6112 - mae: 16.6112 - val_loss: 16.2707 - val_mae: 16.2707\n",
      "Epoch 1640/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.6085 - mae: 16.6085 - val_loss: 16.2824 - val_mae: 16.2824\n",
      "Epoch 1641/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6053 - mae: 16.6053 - val_loss: 16.3119 - val_mae: 16.3119\n",
      "Epoch 1642/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6055 - mae: 16.6055 - val_loss: 16.2543 - val_mae: 16.2543\n",
      "Epoch 1643/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6139 - mae: 16.6139 - val_loss: 16.2600 - val_mae: 16.2600\n",
      "Epoch 1644/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6026 - mae: 16.6025 - val_loss: 16.3171 - val_mae: 16.3171\n",
      "Epoch 1645/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6014 - mae: 16.6014 - val_loss: 16.2607 - val_mae: 16.2606\n",
      "Epoch 1646/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6053 - mae: 16.6053 - val_loss: 16.3230 - val_mae: 16.3230\n",
      "Epoch 1647/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6070 - mae: 16.6070 - val_loss: 16.2551 - val_mae: 16.2551\n",
      "Epoch 1648/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6054 - mae: 16.6054 - val_loss: 16.2552 - val_mae: 16.2552\n",
      "Epoch 1649/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6063 - mae: 16.6063 - val_loss: 16.2532 - val_mae: 16.2532\n",
      "Epoch 1650/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6074 - mae: 16.6074 - val_loss: 16.3127 - val_mae: 16.3127\n",
      "Epoch 1651/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6084 - mae: 16.6084 - val_loss: 16.2606 - val_mae: 16.2606\n",
      "Epoch 1652/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.6039 - mae: 16.6039 - val_loss: 16.2711 - val_mae: 16.2712\n",
      "Epoch 1653/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6064 - mae: 16.6064 - val_loss: 16.2668 - val_mae: 16.2668\n",
      "Epoch 1654/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6085 - mae: 16.6085 - val_loss: 16.3332 - val_mae: 16.3332\n",
      "Epoch 1655/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.6082 - mae: 16.6082 - val_loss: 16.3171 - val_mae: 16.3171\n",
      "Epoch 1656/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6043 - mae: 16.6043 - val_loss: 16.3017 - val_mae: 16.3017\n",
      "Epoch 1657/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5981 - mae: 16.5981 - val_loss: 16.3807 - val_mae: 16.3807\n",
      "Epoch 1658/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6132 - mae: 16.6132 - val_loss: 16.2627 - val_mae: 16.2627\n",
      "Epoch 1659/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.6055 - mae: 16.6055 - val_loss: 16.2824 - val_mae: 16.2824\n",
      "Epoch 1660/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5993 - mae: 16.5993 - val_loss: 16.3213 - val_mae: 16.3213\n",
      "Epoch 1661/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6037 - mae: 16.6037 - val_loss: 16.2579 - val_mae: 16.2579\n",
      "Epoch 1662/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6065 - mae: 16.6065 - val_loss: 16.2737 - val_mae: 16.2737\n",
      "Epoch 1663/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5982 - mae: 16.5982 - val_loss: 16.3962 - val_mae: 16.3962\n",
      "Epoch 1664/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6075 - mae: 16.6075 - val_loss: 16.2842 - val_mae: 16.2841\n",
      "Epoch 1665/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6095 - mae: 16.6095 - val_loss: 16.2509 - val_mae: 16.2509\n",
      "Epoch 1666/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.5977 - mae: 16.5977 - val_loss: 16.2924 - val_mae: 16.2924\n",
      "Epoch 1667/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6054 - mae: 16.6054 - val_loss: 16.2701 - val_mae: 16.2701\n",
      "Epoch 1668/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6012 - mae: 16.6012 - val_loss: 16.2664 - val_mae: 16.2664\n",
      "Epoch 1669/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6036 - mae: 16.6036 - val_loss: 16.2605 - val_mae: 16.2605\n",
      "Epoch 1670/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6019 - mae: 16.6018 - val_loss: 16.2859 - val_mae: 16.2859\n",
      "Epoch 1671/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6061 - mae: 16.6061 - val_loss: 16.2530 - val_mae: 16.2530\n",
      "Epoch 1672/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6048 - mae: 16.6048 - val_loss: 16.2869 - val_mae: 16.2869\n",
      "Epoch 1673/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6078 - mae: 16.6078 - val_loss: 16.2598 - val_mae: 16.2598\n",
      "Epoch 1674/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6080 - mae: 16.6080 - val_loss: 16.2522 - val_mae: 16.2522\n",
      "Epoch 1675/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6048 - mae: 16.6048 - val_loss: 16.2597 - val_mae: 16.2597\n",
      "Epoch 1676/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6056 - mae: 16.6056 - val_loss: 16.2538 - val_mae: 16.2538\n",
      "Epoch 1677/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6006 - mae: 16.6006 - val_loss: 16.2524 - val_mae: 16.2525\n",
      "Epoch 1678/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6012 - mae: 16.6012 - val_loss: 16.3179 - val_mae: 16.3179\n",
      "Epoch 1679/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5988 - mae: 16.5988 - val_loss: 16.2555 - val_mae: 16.2555\n",
      "Epoch 1680/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5966 - mae: 16.5966 - val_loss: 16.2577 - val_mae: 16.2577\n",
      "Epoch 1681/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6020 - mae: 16.6020 - val_loss: 16.2680 - val_mae: 16.2680\n",
      "Epoch 1682/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5939 - mae: 16.5939 - val_loss: 16.2522 - val_mae: 16.2522\n",
      "Epoch 1683/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.6020 - mae: 16.6020 - val_loss: 16.2496 - val_mae: 16.2496\n",
      "Epoch 1684/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6031 - mae: 16.6031 - val_loss: 16.3163 - val_mae: 16.3163\n",
      "Epoch 1685/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6031 - mae: 16.6031 - val_loss: 16.2824 - val_mae: 16.2824\n",
      "Epoch 1686/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6029 - mae: 16.6029 - val_loss: 16.3266 - val_mae: 16.3266\n",
      "Epoch 1687/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6026 - mae: 16.6026 - val_loss: 16.2507 - val_mae: 16.2507\n",
      "Epoch 1688/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5993 - mae: 16.5993 - val_loss: 16.2658 - val_mae: 16.2658\n",
      "Epoch 1689/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5980 - mae: 16.5980 - val_loss: 16.2524 - val_mae: 16.2524\n",
      "Epoch 1690/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6035 - mae: 16.6035 - val_loss: 16.2547 - val_mae: 16.2547\n",
      "Epoch 1691/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6059 - mae: 16.6059 - val_loss: 16.2686 - val_mae: 16.2686\n",
      "Epoch 1692/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6047 - mae: 16.6047 - val_loss: 16.2549 - val_mae: 16.2549\n",
      "Epoch 1693/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6042 - mae: 16.6042 - val_loss: 16.2512 - val_mae: 16.2512\n",
      "Epoch 1694/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6001 - mae: 16.6001 - val_loss: 16.2630 - val_mae: 16.2630\n",
      "Epoch 1695/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5992 - mae: 16.5992 - val_loss: 16.3042 - val_mae: 16.3042\n",
      "Epoch 1696/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5951 - mae: 16.5951 - val_loss: 16.2512 - val_mae: 16.2512\n",
      "Epoch 1697/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5928 - mae: 16.5928 - val_loss: 16.2552 - val_mae: 16.2552\n",
      "Epoch 1698/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.6001 - mae: 16.6001 - val_loss: 16.2524 - val_mae: 16.2524\n",
      "Epoch 1699/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6012 - mae: 16.6012 - val_loss: 16.2589 - val_mae: 16.2589\n",
      "Epoch 1700/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5963 - mae: 16.5962 - val_loss: 16.2787 - val_mae: 16.2788\n",
      "Epoch 1701/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5960 - mae: 16.5960 - val_loss: 16.2582 - val_mae: 16.2582\n",
      "Epoch 1702/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.6023 - mae: 16.6023 - val_loss: 16.2630 - val_mae: 16.2630\n",
      "Epoch 1703/3000\n",
      "10496/10496 [==============================] - 1s 58us/sample - loss: 16.5990 - mae: 16.5990 - val_loss: 16.2520 - val_mae: 16.2520\n",
      "Epoch 1704/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5958 - mae: 16.5958 - val_loss: 16.2520 - val_mae: 16.2520\n",
      "Epoch 1705/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5965 - mae: 16.5965 - val_loss: 16.2483 - val_mae: 16.2483\n",
      "Epoch 1706/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5990 - mae: 16.5990 - val_loss: 16.2507 - val_mae: 16.2507\n",
      "Epoch 1707/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5943 - mae: 16.5943 - val_loss: 16.2510 - val_mae: 16.2510\n",
      "Epoch 1708/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.6012 - mae: 16.6012 - val_loss: 16.2531 - val_mae: 16.2531\n",
      "Epoch 1709/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6004 - mae: 16.6004 - val_loss: 16.2826 - val_mae: 16.2826\n",
      "Epoch 1710/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5983 - mae: 16.5983 - val_loss: 16.2469 - val_mae: 16.2469\n",
      "Epoch 1711/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5956 - mae: 16.5956 - val_loss: 16.2509 - val_mae: 16.2509\n",
      "Epoch 1712/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.5978 - mae: 16.5979 - val_loss: 16.2461 - val_mae: 16.2461\n",
      "Epoch 1713/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5993 - mae: 16.5993 - val_loss: 16.2554 - val_mae: 16.2554\n",
      "Epoch 1714/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5922 - mae: 16.5922 - val_loss: 16.2891 - val_mae: 16.2891\n",
      "Epoch 1715/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5967 - mae: 16.5967 - val_loss: 16.2857 - val_mae: 16.2857\n",
      "Epoch 1716/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5986 - mae: 16.5986 - val_loss: 16.2577 - val_mae: 16.2577\n",
      "Epoch 1717/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5963 - mae: 16.5963 - val_loss: 16.2496 - val_mae: 16.2496\n",
      "Epoch 1718/3000\n",
      "10496/10496 [==============================] - 1s 58us/sample - loss: 16.5937 - mae: 16.5937 - val_loss: 16.2581 - val_mae: 16.2581\n",
      "Epoch 1719/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.5916 - mae: 16.5916 - val_loss: 16.2557 - val_mae: 16.2557\n",
      "Epoch 1720/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5883 - mae: 16.5883 - val_loss: 16.2845 - val_mae: 16.2845\n",
      "Epoch 1721/3000\n",
      "10496/10496 [==============================] - 1s 67us/sample - loss: 16.5884 - mae: 16.5884 - val_loss: 16.3539 - val_mae: 16.3539\n",
      "Epoch 1722/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.6026 - mae: 16.6026 - val_loss: 16.2523 - val_mae: 16.2523\n",
      "Epoch 1723/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5939 - mae: 16.5939 - val_loss: 16.2486 - val_mae: 16.2486\n",
      "Epoch 1724/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5891 - mae: 16.5891 - val_loss: 16.2451 - val_mae: 16.2451\n",
      "Epoch 1725/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5873 - mae: 16.5873 - val_loss: 16.2677 - val_mae: 16.2677\n",
      "Epoch 1726/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5886 - mae: 16.5886 - val_loss: 16.3078 - val_mae: 16.3078\n",
      "Epoch 1727/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.6018 - mae: 16.6018 - val_loss: 16.2666 - val_mae: 16.2666\n",
      "Epoch 1728/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5854 - mae: 16.5854 - val_loss: 16.2736 - val_mae: 16.2736\n",
      "Epoch 1729/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5957 - mae: 16.5957 - val_loss: 16.2691 - val_mae: 16.2691\n",
      "Epoch 1730/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5953 - mae: 16.5953 - val_loss: 16.2626 - val_mae: 16.2626\n",
      "Epoch 1731/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5967 - mae: 16.5967 - val_loss: 16.2951 - val_mae: 16.2951\n",
      "Epoch 1732/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5884 - mae: 16.5884 - val_loss: 16.2642 - val_mae: 16.2642\n",
      "Epoch 1733/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5917 - mae: 16.5917 - val_loss: 16.2691 - val_mae: 16.2691\n",
      "Epoch 1734/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5953 - mae: 16.5953 - val_loss: 16.2529 - val_mae: 16.2529\n",
      "Epoch 1735/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5938 - mae: 16.5938 - val_loss: 16.3018 - val_mae: 16.3018\n",
      "Epoch 1736/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.5919 - mae: 16.5919 - val_loss: 16.2751 - val_mae: 16.2751\n",
      "Epoch 1737/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5860 - mae: 16.5860 - val_loss: 16.2435 - val_mae: 16.2435\n",
      "Epoch 1738/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5934 - mae: 16.5934 - val_loss: 16.2679 - val_mae: 16.2679\n",
      "Epoch 1739/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5942 - mae: 16.5942 - val_loss: 16.2493 - val_mae: 16.2493\n",
      "Epoch 1740/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5904 - mae: 16.5904 - val_loss: 16.2466 - val_mae: 16.2466\n",
      "Epoch 1741/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.6002 - mae: 16.6002 - val_loss: 16.2797 - val_mae: 16.2797\n",
      "Epoch 1742/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5831 - mae: 16.5831 - val_loss: 16.2551 - val_mae: 16.2551\n",
      "Epoch 1743/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5986 - mae: 16.5986 - val_loss: 16.2461 - val_mae: 16.2461\n",
      "Epoch 1744/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5899 - mae: 16.5899 - val_loss: 16.2971 - val_mae: 16.2971\n",
      "Epoch 1745/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5871 - mae: 16.5871 - val_loss: 16.2626 - val_mae: 16.2626\n",
      "Epoch 1746/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5956 - mae: 16.5956 - val_loss: 16.2478 - val_mae: 16.2478\n",
      "Epoch 1747/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5862 - mae: 16.5862 - val_loss: 16.2518 - val_mae: 16.2518\n",
      "Epoch 1748/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5829 - mae: 16.5829 - val_loss: 16.2602 - val_mae: 16.2602\n",
      "Epoch 1749/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5942 - mae: 16.5942 - val_loss: 16.2470 - val_mae: 16.2470\n",
      "Epoch 1750/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5963 - mae: 16.5963 - val_loss: 16.2476 - val_mae: 16.2476\n",
      "Epoch 1751/3000\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 16.5866 - mae: 16.5866 - val_loss: 16.2629 - val_mae: 16.2629\n",
      "Epoch 1752/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5945 - mae: 16.5945 - val_loss: 16.2496 - val_mae: 16.2496\n",
      "Epoch 1753/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5858 - mae: 16.5858 - val_loss: 16.2495 - val_mae: 16.2495\n",
      "Epoch 1754/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5930 - mae: 16.5930 - val_loss: 16.2907 - val_mae: 16.2907\n",
      "Epoch 1755/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5960 - mae: 16.5960 - val_loss: 16.2657 - val_mae: 16.2656\n",
      "Epoch 1756/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5936 - mae: 16.5936 - val_loss: 16.2633 - val_mae: 16.2633\n",
      "Epoch 1757/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5947 - mae: 16.5947 - val_loss: 16.2447 - val_mae: 16.2447\n",
      "Epoch 1758/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5953 - mae: 16.5953 - val_loss: 16.2621 - val_mae: 16.2621\n",
      "Epoch 1759/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5910 - mae: 16.5910 - val_loss: 16.2456 - val_mae: 16.2456\n",
      "Epoch 1760/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5949 - mae: 16.5949 - val_loss: 16.2499 - val_mae: 16.2499\n",
      "Epoch 1761/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5908 - mae: 16.5908 - val_loss: 16.2700 - val_mae: 16.2700\n",
      "Epoch 1762/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5858 - mae: 16.5858 - val_loss: 16.2776 - val_mae: 16.2776\n",
      "Epoch 1763/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5911 - mae: 16.5911 - val_loss: 16.2735 - val_mae: 16.2735\n",
      "Epoch 1764/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5908 - mae: 16.5908 - val_loss: 16.2429 - val_mae: 16.2429\n",
      "Epoch 1765/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5858 - mae: 16.5858 - val_loss: 16.3016 - val_mae: 16.3016\n",
      "Epoch 1766/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5862 - mae: 16.5862 - val_loss: 16.2475 - val_mae: 16.2475\n",
      "Epoch 1767/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5866 - mae: 16.5866 - val_loss: 16.2742 - val_mae: 16.2742\n",
      "Epoch 1768/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5905 - mae: 16.5905 - val_loss: 16.2444 - val_mae: 16.2444\n",
      "Epoch 1769/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5859 - mae: 16.5859 - val_loss: 16.2435 - val_mae: 16.2435\n",
      "Epoch 1770/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5873 - mae: 16.5873 - val_loss: 16.2776 - val_mae: 16.2776\n",
      "Epoch 1771/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5868 - mae: 16.5868 - val_loss: 16.2757 - val_mae: 16.2757\n",
      "Epoch 1772/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5772 - mae: 16.5772 - val_loss: 16.2430 - val_mae: 16.2430\n",
      "Epoch 1773/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5940 - mae: 16.5940 - val_loss: 16.2561 - val_mae: 16.2561\n",
      "Epoch 1774/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5852 - mae: 16.5852 - val_loss: 16.2433 - val_mae: 16.2433\n",
      "Epoch 1775/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5859 - mae: 16.5859 - val_loss: 16.2781 - val_mae: 16.2780\n",
      "Epoch 1776/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5860 - mae: 16.5860 - val_loss: 16.2427 - val_mae: 16.2427\n",
      "Epoch 1777/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5830 - mae: 16.5830 - val_loss: 16.2678 - val_mae: 16.2678\n",
      "Epoch 1778/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.5904 - mae: 16.5904 - val_loss: 16.2681 - val_mae: 16.2681\n",
      "Epoch 1779/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5862 - mae: 16.5862 - val_loss: 16.2467 - val_mae: 16.2467\n",
      "Epoch 1780/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5921 - mae: 16.5921 - val_loss: 16.2495 - val_mae: 16.2494\n",
      "Epoch 1781/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5860 - mae: 16.5860 - val_loss: 16.2706 - val_mae: 16.2706\n",
      "Epoch 1782/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5865 - mae: 16.5865 - val_loss: 16.2407 - val_mae: 16.2407\n",
      "Epoch 1783/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5885 - mae: 16.5885 - val_loss: 16.2536 - val_mae: 16.2535\n",
      "Epoch 1784/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5783 - mae: 16.5783 - val_loss: 16.2451 - val_mae: 16.2451\n",
      "Epoch 1785/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5830 - mae: 16.5830 - val_loss: 16.2446 - val_mae: 16.2446\n",
      "Epoch 1786/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5852 - mae: 16.5852 - val_loss: 16.2398 - val_mae: 16.2398\n",
      "Epoch 1787/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5828 - mae: 16.5828 - val_loss: 16.2538 - val_mae: 16.2538\n",
      "Epoch 1788/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5712 - mae: 16.5712 - val_loss: 16.2426 - val_mae: 16.2426\n",
      "Epoch 1789/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5867 - mae: 16.5867 - val_loss: 16.2402 - val_mae: 16.2402\n",
      "Epoch 1790/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5897 - mae: 16.5896 - val_loss: 16.2571 - val_mae: 16.2571\n",
      "Epoch 1791/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5886 - mae: 16.5886 - val_loss: 16.2424 - val_mae: 16.2424\n",
      "Epoch 1792/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5864 - mae: 16.5864 - val_loss: 16.2530 - val_mae: 16.2530\n",
      "Epoch 1793/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5817 - mae: 16.5817 - val_loss: 16.2827 - val_mae: 16.2827\n",
      "Epoch 1794/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5858 - mae: 16.5857 - val_loss: 16.2859 - val_mae: 16.2859\n",
      "Epoch 1795/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5883 - mae: 16.5883 - val_loss: 16.2466 - val_mae: 16.2466\n",
      "Epoch 1796/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5822 - mae: 16.5822 - val_loss: 16.2422 - val_mae: 16.2422\n",
      "Epoch 1797/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5891 - mae: 16.5891 - val_loss: 16.2394 - val_mae: 16.2393\n",
      "Epoch 1798/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5855 - mae: 16.5855 - val_loss: 16.2736 - val_mae: 16.2736\n",
      "Epoch 1799/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5865 - mae: 16.5865 - val_loss: 16.2400 - val_mae: 16.2400\n",
      "Epoch 1800/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5880 - mae: 16.5880 - val_loss: 16.2434 - val_mae: 16.2434\n",
      "Epoch 1801/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5840 - mae: 16.5840 - val_loss: 16.2391 - val_mae: 16.2391\n",
      "Epoch 1802/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5869 - mae: 16.5869 - val_loss: 16.2396 - val_mae: 16.2396\n",
      "Epoch 1803/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5838 - mae: 16.5838 - val_loss: 16.2413 - val_mae: 16.2413\n",
      "Epoch 1804/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5835 - mae: 16.5835 - val_loss: 16.2654 - val_mae: 16.2654\n",
      "Epoch 1805/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5859 - mae: 16.5859 - val_loss: 16.2394 - val_mae: 16.2394\n",
      "Epoch 1806/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5848 - mae: 16.5848 - val_loss: 16.2423 - val_mae: 16.2423\n",
      "Epoch 1807/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5833 - mae: 16.5833 - val_loss: 16.2575 - val_mae: 16.2575\n",
      "Epoch 1808/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5855 - mae: 16.5855 - val_loss: 16.2548 - val_mae: 16.2548\n",
      "Epoch 1809/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5872 - mae: 16.5872 - val_loss: 16.2383 - val_mae: 16.2383\n",
      "Epoch 1810/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5861 - mae: 16.5861 - val_loss: 16.2486 - val_mae: 16.2486\n",
      "Epoch 1811/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5881 - mae: 16.5881 - val_loss: 16.2581 - val_mae: 16.2581\n",
      "Epoch 1812/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5765 - mae: 16.5765 - val_loss: 16.2385 - val_mae: 16.2385\n",
      "Epoch 1813/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5817 - mae: 16.5817 - val_loss: 16.2440 - val_mae: 16.2440\n",
      "Epoch 1814/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5844 - mae: 16.5844 - val_loss: 16.2368 - val_mae: 16.2368\n",
      "Epoch 1815/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5855 - mae: 16.5855 - val_loss: 16.2421 - val_mae: 16.2421\n",
      "Epoch 1816/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5807 - mae: 16.5807 - val_loss: 16.2575 - val_mae: 16.2575\n",
      "Epoch 1817/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5846 - mae: 16.5846 - val_loss: 16.2410 - val_mae: 16.2410\n",
      "Epoch 1818/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5814 - mae: 16.5814 - val_loss: 16.2474 - val_mae: 16.2474\n",
      "Epoch 1819/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5810 - mae: 16.5810 - val_loss: 16.2510 - val_mae: 16.2510\n",
      "Epoch 1820/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5842 - mae: 16.5842 - val_loss: 16.2696 - val_mae: 16.2696\n",
      "Epoch 1821/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5798 - mae: 16.5798 - val_loss: 16.2609 - val_mae: 16.2609\n",
      "Epoch 1822/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5858 - mae: 16.5857 - val_loss: 16.2896 - val_mae: 16.2896\n",
      "Epoch 1823/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5791 - mae: 16.5791 - val_loss: 16.2753 - val_mae: 16.2753\n",
      "Epoch 1824/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5802 - mae: 16.5802 - val_loss: 16.2420 - val_mae: 16.2420\n",
      "Epoch 1825/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5821 - mae: 16.5821 - val_loss: 16.2702 - val_mae: 16.2702\n",
      "Epoch 1826/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5839 - mae: 16.5840 - val_loss: 16.2418 - val_mae: 16.2418\n",
      "Epoch 1827/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5820 - mae: 16.5820 - val_loss: 16.2405 - val_mae: 16.2405\n",
      "Epoch 1828/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5861 - mae: 16.5861 - val_loss: 16.2535 - val_mae: 16.2535\n",
      "Epoch 1829/3000\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 16.5794 - mae: 16.5794 - val_loss: 16.2378 - val_mae: 16.2378\n",
      "Epoch 1830/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5833 - mae: 16.5833 - val_loss: 16.2551 - val_mae: 16.2551\n",
      "Epoch 1831/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5772 - mae: 16.5772 - val_loss: 16.2518 - val_mae: 16.2518\n",
      "Epoch 1832/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5880 - mae: 16.5880 - val_loss: 16.2517 - val_mae: 16.2517\n",
      "Epoch 1833/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5798 - mae: 16.5798 - val_loss: 16.3207 - val_mae: 16.3207\n",
      "Epoch 1834/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5829 - mae: 16.5829 - val_loss: 16.2662 - val_mae: 16.2662\n",
      "Epoch 1835/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5808 - mae: 16.5808 - val_loss: 16.2516 - val_mae: 16.2516\n",
      "Epoch 1836/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5805 - mae: 16.5805 - val_loss: 16.2449 - val_mae: 16.2449\n",
      "Epoch 1837/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5818 - mae: 16.5818 - val_loss: 16.2399 - val_mae: 16.2399\n",
      "Epoch 1838/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5778 - mae: 16.5778 - val_loss: 16.2481 - val_mae: 16.2481\n",
      "Epoch 1839/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5838 - mae: 16.5838 - val_loss: 16.2538 - val_mae: 16.2538\n",
      "Epoch 1840/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5718 - mae: 16.5718 - val_loss: 16.3413 - val_mae: 16.3413\n",
      "Epoch 1841/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5797 - mae: 16.5797 - val_loss: 16.2386 - val_mae: 16.2386\n",
      "Epoch 1842/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5820 - mae: 16.5820 - val_loss: 16.2809 - val_mae: 16.2809\n",
      "Epoch 1843/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5729 - mae: 16.5729 - val_loss: 16.3113 - val_mae: 16.3113\n",
      "Epoch 1844/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5752 - mae: 16.5752 - val_loss: 16.2501 - val_mae: 16.2501\n",
      "Epoch 1845/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5785 - mae: 16.5785 - val_loss: 16.2346 - val_mae: 16.2346\n",
      "Epoch 1846/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5811 - mae: 16.5811 - val_loss: 16.3028 - val_mae: 16.3028\n",
      "Epoch 1847/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5802 - mae: 16.5802 - val_loss: 16.2544 - val_mae: 16.2544\n",
      "Epoch 1848/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5785 - mae: 16.5785 - val_loss: 16.2354 - val_mae: 16.2354\n",
      "Epoch 1849/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5761 - mae: 16.5761 - val_loss: 16.2347 - val_mae: 16.2347\n",
      "Epoch 1850/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5763 - mae: 16.5763 - val_loss: 16.2556 - val_mae: 16.2556\n",
      "Epoch 1851/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5817 - mae: 16.5817 - val_loss: 16.2509 - val_mae: 16.2509\n",
      "Epoch 1852/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5756 - mae: 16.5756 - val_loss: 16.2537 - val_mae: 16.2537\n",
      "Epoch 1853/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5784 - mae: 16.5784 - val_loss: 16.3022 - val_mae: 16.3022\n",
      "Epoch 1854/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5823 - mae: 16.5823 - val_loss: 16.2492 - val_mae: 16.2492\n",
      "Epoch 1855/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5783 - mae: 16.5783 - val_loss: 16.2392 - val_mae: 16.2392\n",
      "Epoch 1856/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5771 - mae: 16.5771 - val_loss: 16.2441 - val_mae: 16.2441\n",
      "Epoch 1857/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5763 - mae: 16.5763 - val_loss: 16.2451 - val_mae: 16.2451\n",
      "Epoch 1858/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5780 - mae: 16.5780 - val_loss: 16.2340 - val_mae: 16.2340\n",
      "Epoch 1859/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5782 - mae: 16.5782 - val_loss: 16.2436 - val_mae: 16.2436\n",
      "Epoch 1860/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5731 - mae: 16.5731 - val_loss: 16.2400 - val_mae: 16.2400\n",
      "Epoch 1861/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5753 - mae: 16.5753 - val_loss: 16.2457 - val_mae: 16.2457\n",
      "Epoch 1862/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5795 - mae: 16.5795 - val_loss: 16.2395 - val_mae: 16.2395\n",
      "Epoch 1863/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5761 - mae: 16.5761 - val_loss: 16.2525 - val_mae: 16.2525\n",
      "Epoch 1864/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5714 - mae: 16.5714 - val_loss: 16.2350 - val_mae: 16.2350\n",
      "Epoch 1865/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5751 - mae: 16.5751 - val_loss: 16.2557 - val_mae: 16.2557\n",
      "Epoch 1866/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5764 - mae: 16.5764 - val_loss: 16.2380 - val_mae: 16.2380\n",
      "Epoch 1867/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5701 - mae: 16.5701 - val_loss: 16.2345 - val_mae: 16.2345\n",
      "Epoch 1868/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5676 - mae: 16.5676 - val_loss: 16.2394 - val_mae: 16.2394\n",
      "Epoch 1869/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.5802 - mae: 16.5802 - val_loss: 16.2373 - val_mae: 16.2373\n",
      "Epoch 1870/3000\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 16.5764 - mae: 16.5764 - val_loss: 16.2381 - val_mae: 16.2381\n",
      "Epoch 1871/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5692 - mae: 16.5692 - val_loss: 16.2719 - val_mae: 16.2719\n",
      "Epoch 1872/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5678 - mae: 16.5678 - val_loss: 16.2436 - val_mae: 16.2436\n",
      "Epoch 1873/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5630 - mae: 16.5630 - val_loss: 16.2569 - val_mae: 16.2569\n",
      "Epoch 1874/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5700 - mae: 16.5700 - val_loss: 16.3030 - val_mae: 16.3030\n",
      "Epoch 1875/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5782 - mae: 16.5782 - val_loss: 16.2886 - val_mae: 16.2886\n",
      "Epoch 1876/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.5652 - mae: 16.5652 - val_loss: 16.2913 - val_mae: 16.2912\n",
      "Epoch 1877/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5801 - mae: 16.5801 - val_loss: 16.2715 - val_mae: 16.2715\n",
      "Epoch 1878/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5700 - mae: 16.5700 - val_loss: 16.2375 - val_mae: 16.2375\n",
      "Epoch 1879/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5684 - mae: 16.5684 - val_loss: 16.2380 - val_mae: 16.2380\n",
      "Epoch 1880/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5705 - mae: 16.5705 - val_loss: 16.2455 - val_mae: 16.2455\n",
      "Epoch 1881/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5722 - mae: 16.5722 - val_loss: 16.2500 - val_mae: 16.2500\n",
      "Epoch 1882/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5757 - mae: 16.5757 - val_loss: 16.2361 - val_mae: 16.2361\n",
      "Epoch 1883/3000\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 16.5747 - mae: 16.5747 - val_loss: 16.2414 - val_mae: 16.2413\n",
      "Epoch 1884/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5704 - mae: 16.5704 - val_loss: 16.2421 - val_mae: 16.2421\n",
      "Epoch 1885/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5693 - mae: 16.5693 - val_loss: 16.2529 - val_mae: 16.2529\n",
      "Epoch 1886/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5781 - mae: 16.5781 - val_loss: 16.2434 - val_mae: 16.2434\n",
      "Epoch 1887/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5592 - mae: 16.5592 - val_loss: 16.2323 - val_mae: 16.2323\n",
      "Epoch 1888/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5696 - mae: 16.5696 - val_loss: 16.2342 - val_mae: 16.2342\n",
      "Epoch 1889/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5695 - mae: 16.5695 - val_loss: 16.2498 - val_mae: 16.2498\n",
      "Epoch 1890/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5703 - mae: 16.5703 - val_loss: 16.2427 - val_mae: 16.2427\n",
      "Epoch 1891/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5650 - mae: 16.5650 - val_loss: 16.2931 - val_mae: 16.2931\n",
      "Epoch 1892/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5682 - mae: 16.5682 - val_loss: 16.2336 - val_mae: 16.2336\n",
      "Epoch 1893/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5638 - mae: 16.5638 - val_loss: 16.2662 - val_mae: 16.2662\n",
      "Epoch 1894/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5682 - mae: 16.5682 - val_loss: 16.2304 - val_mae: 16.2304\n",
      "Epoch 1895/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5688 - mae: 16.5688 - val_loss: 16.2411 - val_mae: 16.2411\n",
      "Epoch 1896/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5678 - mae: 16.5678 - val_loss: 16.3330 - val_mae: 16.3330\n",
      "Epoch 1897/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5687 - mae: 16.5687 - val_loss: 16.2570 - val_mae: 16.2570\n",
      "Epoch 1898/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5736 - mae: 16.5736 - val_loss: 16.2382 - val_mae: 16.2382\n",
      "Epoch 1899/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5712 - mae: 16.5712 - val_loss: 16.2577 - val_mae: 16.2577\n",
      "Epoch 1900/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5704 - mae: 16.5704 - val_loss: 16.2556 - val_mae: 16.2556\n",
      "Epoch 1901/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5736 - mae: 16.5736 - val_loss: 16.2350 - val_mae: 16.2350\n",
      "Epoch 1902/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5676 - mae: 16.5676 - val_loss: 16.2331 - val_mae: 16.2331\n",
      "Epoch 1903/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5638 - mae: 16.5638 - val_loss: 16.2909 - val_mae: 16.2908\n",
      "Epoch 1904/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5654 - mae: 16.5654 - val_loss: 16.2313 - val_mae: 16.2313\n",
      "Epoch 1905/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5729 - mae: 16.5729 - val_loss: 16.2538 - val_mae: 16.2538\n",
      "Epoch 1906/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5680 - mae: 16.5680 - val_loss: 16.2377 - val_mae: 16.2377\n",
      "Epoch 1907/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5693 - mae: 16.5693 - val_loss: 16.2516 - val_mae: 16.2516\n",
      "Epoch 1908/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5745 - mae: 16.5745 - val_loss: 16.2337 - val_mae: 16.2337\n",
      "Epoch 1909/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5679 - mae: 16.5679 - val_loss: 16.2307 - val_mae: 16.2307\n",
      "Epoch 1910/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5688 - mae: 16.5688 - val_loss: 16.2311 - val_mae: 16.2311\n",
      "Epoch 1911/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5613 - mae: 16.5613 - val_loss: 16.2952 - val_mae: 16.2952\n",
      "Epoch 1912/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5732 - mae: 16.5732 - val_loss: 16.2446 - val_mae: 16.2446\n",
      "Epoch 1913/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5684 - mae: 16.5684 - val_loss: 16.2476 - val_mae: 16.2476\n",
      "Epoch 1914/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5696 - mae: 16.5696 - val_loss: 16.2294 - val_mae: 16.2294\n",
      "Epoch 1915/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5653 - mae: 16.5653 - val_loss: 16.2655 - val_mae: 16.2655\n",
      "Epoch 1916/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5701 - mae: 16.5701 - val_loss: 16.2310 - val_mae: 16.2310\n",
      "Epoch 1917/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5698 - mae: 16.5698 - val_loss: 16.2493 - val_mae: 16.2493\n",
      "Epoch 1918/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5630 - mae: 16.5630 - val_loss: 16.2763 - val_mae: 16.2763\n",
      "Epoch 1919/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5704 - mae: 16.5704 - val_loss: 16.2399 - val_mae: 16.2399\n",
      "Epoch 1920/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5704 - mae: 16.5704 - val_loss: 16.2405 - val_mae: 16.2405\n",
      "Epoch 1921/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5677 - mae: 16.5677 - val_loss: 16.2753 - val_mae: 16.2753\n",
      "Epoch 1922/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5717 - mae: 16.5717 - val_loss: 16.2533 - val_mae: 16.2533\n",
      "Epoch 1923/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5726 - mae: 16.5726 - val_loss: 16.2468 - val_mae: 16.2468\n",
      "Epoch 1924/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5735 - mae: 16.5735 - val_loss: 16.2354 - val_mae: 16.2354\n",
      "Epoch 1925/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5651 - mae: 16.5651 - val_loss: 16.2377 - val_mae: 16.2377\n",
      "Epoch 1926/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5630 - mae: 16.5630 - val_loss: 16.2873 - val_mae: 16.2873\n",
      "Epoch 1927/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5675 - mae: 16.5675 - val_loss: 16.2423 - val_mae: 16.2423\n",
      "Epoch 1928/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5651 - mae: 16.5651 - val_loss: 16.2530 - val_mae: 16.2530\n",
      "Epoch 1929/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5622 - mae: 16.5622 - val_loss: 16.2324 - val_mae: 16.2324\n",
      "Epoch 1930/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5685 - mae: 16.5685 - val_loss: 16.2273 - val_mae: 16.2273\n",
      "Epoch 1931/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5632 - mae: 16.5632 - val_loss: 16.2303 - val_mae: 16.2303\n",
      "Epoch 1932/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.5667 - mae: 16.5667 - val_loss: 16.2409 - val_mae: 16.2409\n",
      "Epoch 1933/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5634 - mae: 16.5634 - val_loss: 16.2311 - val_mae: 16.2311\n",
      "Epoch 1934/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5693 - mae: 16.5693 - val_loss: 16.2278 - val_mae: 16.2278\n",
      "Epoch 1935/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5708 - mae: 16.5708 - val_loss: 16.2377 - val_mae: 16.2377\n",
      "Epoch 1936/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5638 - mae: 16.5638 - val_loss: 16.2284 - val_mae: 16.2284\n",
      "Epoch 1937/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5620 - mae: 16.5620 - val_loss: 16.2327 - val_mae: 16.2327\n",
      "Epoch 1938/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5676 - mae: 16.5676 - val_loss: 16.2293 - val_mae: 16.2293\n",
      "Epoch 1939/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5630 - mae: 16.5630 - val_loss: 16.2289 - val_mae: 16.2289\n",
      "Epoch 1940/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5653 - mae: 16.5653 - val_loss: 16.2492 - val_mae: 16.2492\n",
      "Epoch 1941/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5629 - mae: 16.5629 - val_loss: 16.2389 - val_mae: 16.2389\n",
      "Epoch 1942/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5655 - mae: 16.5655 - val_loss: 16.2832 - val_mae: 16.2832\n",
      "Epoch 1943/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5491 - mae: 16.5491 - val_loss: 16.2858 - val_mae: 16.2858\n",
      "Epoch 1944/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5561 - mae: 16.5561 - val_loss: 16.3327 - val_mae: 16.3327\n",
      "Epoch 1945/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5675 - mae: 16.5675 - val_loss: 16.2320 - val_mae: 16.2320\n",
      "Epoch 1946/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5658 - mae: 16.5658 - val_loss: 16.2812 - val_mae: 16.2812\n",
      "Epoch 1947/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5653 - mae: 16.5653 - val_loss: 16.2281 - val_mae: 16.2281\n",
      "Epoch 1948/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5592 - mae: 16.5592 - val_loss: 16.2319 - val_mae: 16.2319\n",
      "Epoch 1949/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5626 - mae: 16.5626 - val_loss: 16.2411 - val_mae: 16.2411\n",
      "Epoch 1950/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5594 - mae: 16.5594 - val_loss: 16.2393 - val_mae: 16.2393\n",
      "Epoch 1951/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5596 - mae: 16.5596 - val_loss: 16.2278 - val_mae: 16.2278\n",
      "Epoch 1952/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5611 - mae: 16.5611 - val_loss: 16.2347 - val_mae: 16.2347\n",
      "Epoch 1953/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5659 - mae: 16.5659 - val_loss: 16.2386 - val_mae: 16.2386\n",
      "Epoch 1954/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5602 - mae: 16.5602 - val_loss: 16.2256 - val_mae: 16.2256\n",
      "Epoch 1955/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5639 - mae: 16.5639 - val_loss: 16.2268 - val_mae: 16.2268\n",
      "Epoch 1956/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5620 - mae: 16.5620 - val_loss: 16.2700 - val_mae: 16.2700\n",
      "Epoch 1957/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5537 - mae: 16.5537 - val_loss: 16.2454 - val_mae: 16.2454\n",
      "Epoch 1958/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5640 - mae: 16.5640 - val_loss: 16.3302 - val_mae: 16.3302\n",
      "Epoch 1959/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5652 - mae: 16.5652 - val_loss: 16.2282 - val_mae: 16.2282\n",
      "Epoch 1960/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5644 - mae: 16.5644 - val_loss: 16.2277 - val_mae: 16.2277\n",
      "Epoch 1961/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5615 - mae: 16.5615 - val_loss: 16.2458 - val_mae: 16.2458\n",
      "Epoch 1962/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5605 - mae: 16.5605 - val_loss: 16.2296 - val_mae: 16.2296\n",
      "Epoch 1963/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5655 - mae: 16.5655 - val_loss: 16.2329 - val_mae: 16.2329\n",
      "Epoch 1964/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5649 - mae: 16.5649 - val_loss: 16.2298 - val_mae: 16.2298\n",
      "Epoch 1965/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5485 - mae: 16.5485 - val_loss: 16.3133 - val_mae: 16.3133\n",
      "Epoch 1966/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5663 - mae: 16.5663 - val_loss: 16.2441 - val_mae: 16.2442\n",
      "Epoch 1967/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5615 - mae: 16.5615 - val_loss: 16.2279 - val_mae: 16.2279\n",
      "Epoch 1968/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5698 - mae: 16.5698 - val_loss: 16.2277 - val_mae: 16.2278\n",
      "Epoch 1969/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5566 - mae: 16.5566 - val_loss: 16.2390 - val_mae: 16.2390\n",
      "Epoch 1970/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5623 - mae: 16.5623 - val_loss: 16.2503 - val_mae: 16.2503\n",
      "Epoch 1971/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5646 - mae: 16.5646 - val_loss: 16.2325 - val_mae: 16.2325\n",
      "Epoch 1972/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5621 - mae: 16.5621 - val_loss: 16.2827 - val_mae: 16.2827\n",
      "Epoch 1973/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5576 - mae: 16.5576 - val_loss: 16.3214 - val_mae: 16.3214\n",
      "Epoch 1974/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5633 - mae: 16.5633 - val_loss: 16.2662 - val_mae: 16.2662\n",
      "Epoch 1975/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5616 - mae: 16.5616 - val_loss: 16.2259 - val_mae: 16.2259\n",
      "Epoch 1976/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5555 - mae: 16.5555 - val_loss: 16.2376 - val_mae: 16.2376\n",
      "Epoch 1977/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5552 - mae: 16.5552 - val_loss: 16.2800 - val_mae: 16.2800\n",
      "Epoch 1978/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5554 - mae: 16.5554 - val_loss: 16.3189 - val_mae: 16.3189\n",
      "Epoch 1979/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5656 - mae: 16.5656 - val_loss: 16.2244 - val_mae: 16.2244\n",
      "Epoch 1980/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5585 - mae: 16.5585 - val_loss: 16.2622 - val_mae: 16.2622\n",
      "Epoch 1981/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5633 - mae: 16.5633 - val_loss: 16.2317 - val_mae: 16.2317\n",
      "Epoch 1982/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5543 - mae: 16.5543 - val_loss: 16.2359 - val_mae: 16.2359\n",
      "Epoch 1983/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5612 - mae: 16.5612 - val_loss: 16.2263 - val_mae: 16.2263\n",
      "Epoch 1984/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5601 - mae: 16.5601 - val_loss: 16.2233 - val_mae: 16.2233\n",
      "Epoch 1985/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5553 - mae: 16.5553 - val_loss: 16.2239 - val_mae: 16.2239\n",
      "Epoch 1986/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5602 - mae: 16.5602 - val_loss: 16.2303 - val_mae: 16.2302\n",
      "Epoch 1987/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5609 - mae: 16.5609 - val_loss: 16.2549 - val_mae: 16.2549\n",
      "Epoch 1988/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5631 - mae: 16.5631 - val_loss: 16.2246 - val_mae: 16.2246\n",
      "Epoch 1989/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5584 - mae: 16.5584 - val_loss: 16.2405 - val_mae: 16.2405\n",
      "Epoch 1990/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5557 - mae: 16.5557 - val_loss: 16.2326 - val_mae: 16.2326\n",
      "Epoch 1991/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5609 - mae: 16.5609 - val_loss: 16.2322 - val_mae: 16.2322\n",
      "Epoch 1992/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5594 - mae: 16.5594 - val_loss: 16.2478 - val_mae: 16.2478\n",
      "Epoch 1993/3000\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 16.5593 - mae: 16.5593 - val_loss: 16.2237 - val_mae: 16.2237\n",
      "Epoch 1994/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5503 - mae: 16.5503 - val_loss: 16.2211 - val_mae: 16.2211\n",
      "Epoch 1995/3000\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 16.5570 - mae: 16.5570 - val_loss: 16.2221 - val_mae: 16.2221\n",
      "Epoch 1996/3000\n",
      "10496/10496 [==============================] - 1s 57us/sample - loss: 16.5571 - mae: 16.5571 - val_loss: 16.2233 - val_mae: 16.2233\n",
      "Epoch 1997/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5528 - mae: 16.5528 - val_loss: 16.2466 - val_mae: 16.2466\n",
      "Epoch 1998/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5602 - mae: 16.5602 - val_loss: 16.2270 - val_mae: 16.2270\n",
      "Epoch 1999/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5570 - mae: 16.5570 - val_loss: 16.2988 - val_mae: 16.2988\n",
      "Epoch 2000/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5565 - mae: 16.5565 - val_loss: 16.2276 - val_mae: 16.2276\n",
      "Epoch 2001/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5506 - mae: 16.5506 - val_loss: 16.2455 - val_mae: 16.2455\n",
      "Epoch 2002/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.5452 - mae: 16.5452 - val_loss: 16.3232 - val_mae: 16.3232\n",
      "Epoch 2003/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5629 - mae: 16.5629 - val_loss: 16.2496 - val_mae: 16.2496\n",
      "Epoch 2004/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5563 - mae: 16.5563 - val_loss: 16.2241 - val_mae: 16.2241\n",
      "Epoch 2005/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5459 - mae: 16.5459 - val_loss: 16.2456 - val_mae: 16.2456\n",
      "Epoch 2006/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5504 - mae: 16.5504 - val_loss: 16.3093 - val_mae: 16.3093\n",
      "Epoch 2007/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5528 - mae: 16.5528 - val_loss: 16.2565 - val_mae: 16.2565\n",
      "Epoch 2008/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5547 - mae: 16.5547 - val_loss: 16.2503 - val_mae: 16.2503\n",
      "Epoch 2009/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5481 - mae: 16.5481 - val_loss: 16.2723 - val_mae: 16.2723\n",
      "Epoch 2010/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5530 - mae: 16.5530 - val_loss: 16.2775 - val_mae: 16.2775\n",
      "Epoch 2011/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5485 - mae: 16.5485 - val_loss: 16.2442 - val_mae: 16.2442\n",
      "Epoch 2012/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5572 - mae: 16.5572 - val_loss: 16.2414 - val_mae: 16.2414\n",
      "Epoch 2013/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5550 - mae: 16.5550 - val_loss: 16.2226 - val_mae: 16.2226\n",
      "Epoch 2014/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5532 - mae: 16.5532 - val_loss: 16.2226 - val_mae: 16.2226\n",
      "Epoch 2015/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5574 - mae: 16.5574 - val_loss: 16.2208 - val_mae: 16.2208\n",
      "Epoch 2016/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5549 - mae: 16.5549 - val_loss: 16.2734 - val_mae: 16.2734\n",
      "Epoch 2017/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5559 - mae: 16.5559 - val_loss: 16.2954 - val_mae: 16.2954\n",
      "Epoch 2018/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5566 - mae: 16.5566 - val_loss: 16.2257 - val_mae: 16.2257\n",
      "Epoch 2019/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5496 - mae: 16.5496 - val_loss: 16.2324 - val_mae: 16.2324\n",
      "Epoch 2020/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5461 - mae: 16.5461 - val_loss: 16.2203 - val_mae: 16.2203\n",
      "Epoch 2021/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5471 - mae: 16.5471 - val_loss: 16.2270 - val_mae: 16.2270\n",
      "Epoch 2022/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.5502 - mae: 16.5502 - val_loss: 16.2602 - val_mae: 16.2602\n",
      "Epoch 2023/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5577 - mae: 16.5577 - val_loss: 16.2320 - val_mae: 16.2320\n",
      "Epoch 2024/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5558 - mae: 16.5558 - val_loss: 16.2329 - val_mae: 16.2329\n",
      "Epoch 2025/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5537 - mae: 16.5537 - val_loss: 16.2333 - val_mae: 16.2333\n",
      "Epoch 2026/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5539 - mae: 16.5539 - val_loss: 16.3014 - val_mae: 16.3013\n",
      "Epoch 2027/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.5551 - mae: 16.5551 - val_loss: 16.2266 - val_mae: 16.2266\n",
      "Epoch 2028/3000\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 16.5515 - mae: 16.5515 - val_loss: 16.2318 - val_mae: 16.2317\n",
      "Epoch 2029/3000\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 16.5512 - mae: 16.5512 - val_loss: 16.2200 - val_mae: 16.2200\n",
      "Epoch 2030/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5449 - mae: 16.5450 - val_loss: 16.2405 - val_mae: 16.2405\n",
      "Epoch 2031/3000\n",
      "10496/10496 [==============================] - 1s 56us/sample - loss: 16.5481 - mae: 16.5481 - val_loss: 16.3274 - val_mae: 16.3274\n",
      "Epoch 2032/3000\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 16.5495 - mae: 16.5495 - val_loss: 16.2340 - val_mae: 16.2340\n",
      "Epoch 2033/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5545 - mae: 16.5545 - val_loss: 16.2748 - val_mae: 16.2748\n",
      "Epoch 2034/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5462 - mae: 16.5462 - val_loss: 16.2235 - val_mae: 16.2235\n",
      "Epoch 2035/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5514 - mae: 16.5513 - val_loss: 16.2527 - val_mae: 16.2527\n",
      "Epoch 2036/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5512 - mae: 16.5512 - val_loss: 16.2240 - val_mae: 16.2240\n",
      "Epoch 2037/3000\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 16.5481 - mae: 16.5481 - val_loss: 16.2313 - val_mae: 16.2313\n",
      "Epoch 2038/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5538 - mae: 16.5538 - val_loss: 16.2275 - val_mae: 16.2274\n",
      "Epoch 2039/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.5458 - mae: 16.5458 - val_loss: 16.2316 - val_mae: 16.2316\n",
      "Epoch 2040/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5491 - mae: 16.5491 - val_loss: 16.2442 - val_mae: 16.2442\n",
      "Epoch 2041/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5527 - mae: 16.5527 - val_loss: 16.2248 - val_mae: 16.2248\n",
      "Epoch 2042/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5534 - mae: 16.5534 - val_loss: 16.2458 - val_mae: 16.2458\n",
      "Epoch 2043/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5539 - mae: 16.5539 - val_loss: 16.2204 - val_mae: 16.2204\n",
      "Epoch 2044/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5438 - mae: 16.5438 - val_loss: 16.2214 - val_mae: 16.2214\n",
      "Epoch 2045/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5511 - mae: 16.5510 - val_loss: 16.2262 - val_mae: 16.2262\n",
      "Epoch 2046/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5381 - mae: 16.5381 - val_loss: 16.2428 - val_mae: 16.2428\n",
      "Epoch 2047/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5460 - mae: 16.5460 - val_loss: 16.2192 - val_mae: 16.2192\n",
      "Epoch 2048/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5440 - mae: 16.5440 - val_loss: 16.2234 - val_mae: 16.2234\n",
      "Epoch 2049/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5403 - mae: 16.5403 - val_loss: 16.2318 - val_mae: 16.2317\n",
      "Epoch 2050/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5398 - mae: 16.5398 - val_loss: 16.2260 - val_mae: 16.2260\n",
      "Epoch 2051/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5474 - mae: 16.5474 - val_loss: 16.2203 - val_mae: 16.2203\n",
      "Epoch 2052/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5494 - mae: 16.5494 - val_loss: 16.2170 - val_mae: 16.2170\n",
      "Epoch 2053/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5406 - mae: 16.5406 - val_loss: 16.2283 - val_mae: 16.2283\n",
      "Epoch 2054/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5487 - mae: 16.5487 - val_loss: 16.2372 - val_mae: 16.2372\n",
      "Epoch 2055/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5482 - mae: 16.5482 - val_loss: 16.2646 - val_mae: 16.2646\n",
      "Epoch 2056/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5445 - mae: 16.5445 - val_loss: 16.2164 - val_mae: 16.2164\n",
      "Epoch 2057/3000\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 16.5486 - mae: 16.5486 - val_loss: 16.2165 - val_mae: 16.2165\n",
      "Epoch 2058/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5497 - mae: 16.5497 - val_loss: 16.2533 - val_mae: 16.2533\n",
      "Epoch 2059/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5390 - mae: 16.5390 - val_loss: 16.2159 - val_mae: 16.2159\n",
      "Epoch 2060/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5451 - mae: 16.5451 - val_loss: 16.2578 - val_mae: 16.2578\n",
      "Epoch 2061/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5433 - mae: 16.5433 - val_loss: 16.2970 - val_mae: 16.2970\n",
      "Epoch 2062/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5483 - mae: 16.5483 - val_loss: 16.2479 - val_mae: 16.2479\n",
      "Epoch 2063/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5509 - mae: 16.5509 - val_loss: 16.2229 - val_mae: 16.2229\n",
      "Epoch 2064/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5427 - mae: 16.5427 - val_loss: 16.2394 - val_mae: 16.2394\n",
      "Epoch 2065/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5524 - mae: 16.5524 - val_loss: 16.2753 - val_mae: 16.2753\n",
      "Epoch 2066/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5479 - mae: 16.5479 - val_loss: 16.2244 - val_mae: 16.2244\n",
      "Epoch 2067/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5486 - mae: 16.5486 - val_loss: 16.2516 - val_mae: 16.2516\n",
      "Epoch 2068/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5485 - mae: 16.5485 - val_loss: 16.2180 - val_mae: 16.2180\n",
      "Epoch 2069/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5430 - mae: 16.5430 - val_loss: 16.2732 - val_mae: 16.2732\n",
      "Epoch 2070/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5511 - mae: 16.5511 - val_loss: 16.2618 - val_mae: 16.2618\n",
      "Epoch 2071/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5526 - mae: 16.5526 - val_loss: 16.2748 - val_mae: 16.2748\n",
      "Epoch 2072/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5465 - mae: 16.5465 - val_loss: 16.2388 - val_mae: 16.2388\n",
      "Epoch 2073/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5495 - mae: 16.5496 - val_loss: 16.2472 - val_mae: 16.2472\n",
      "Epoch 2074/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5428 - mae: 16.5428 - val_loss: 16.2537 - val_mae: 16.2537\n",
      "Epoch 2075/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5454 - mae: 16.5454 - val_loss: 16.2191 - val_mae: 16.2191\n",
      "Epoch 2076/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5483 - mae: 16.5483 - val_loss: 16.2188 - val_mae: 16.2188\n",
      "Epoch 2077/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5432 - mae: 16.5432 - val_loss: 16.2156 - val_mae: 16.2156\n",
      "Epoch 2078/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5458 - mae: 16.5458 - val_loss: 16.2201 - val_mae: 16.2201\n",
      "Epoch 2079/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5480 - mae: 16.5480 - val_loss: 16.2326 - val_mae: 16.2326\n",
      "Epoch 2080/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5388 - mae: 16.5388 - val_loss: 16.2448 - val_mae: 16.2448\n",
      "Epoch 2081/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5526 - mae: 16.5526 - val_loss: 16.2208 - val_mae: 16.2208\n",
      "Epoch 2082/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5473 - mae: 16.5473 - val_loss: 16.2213 - val_mae: 16.2213\n",
      "Epoch 2083/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5500 - mae: 16.5499 - val_loss: 16.2170 - val_mae: 16.2170\n",
      "Epoch 2084/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5460 - mae: 16.5460 - val_loss: 16.2412 - val_mae: 16.2412\n",
      "Epoch 2085/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5386 - mae: 16.5386 - val_loss: 16.2560 - val_mae: 16.2560\n",
      "Epoch 2086/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5464 - mae: 16.5464 - val_loss: 16.2202 - val_mae: 16.2202\n",
      "Epoch 2087/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5461 - mae: 16.5461 - val_loss: 16.2256 - val_mae: 16.2256\n",
      "Epoch 2088/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5380 - mae: 16.5380 - val_loss: 16.2601 - val_mae: 16.2601\n",
      "Epoch 2089/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5395 - mae: 16.5395 - val_loss: 16.2130 - val_mae: 16.2130\n",
      "Epoch 2090/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5393 - mae: 16.5393 - val_loss: 16.2226 - val_mae: 16.2226\n",
      "Epoch 2091/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5426 - mae: 16.5426 - val_loss: 16.2325 - val_mae: 16.2325\n",
      "Epoch 2092/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5462 - mae: 16.5462 - val_loss: 16.2394 - val_mae: 16.2394\n",
      "Epoch 2093/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5422 - mae: 16.5422 - val_loss: 16.2262 - val_mae: 16.2262\n",
      "Epoch 2094/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5456 - mae: 16.5456 - val_loss: 16.2368 - val_mae: 16.2368\n",
      "Epoch 2095/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5417 - mae: 16.5417 - val_loss: 16.2200 - val_mae: 16.2200\n",
      "Epoch 2096/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5383 - mae: 16.5383 - val_loss: 16.2388 - val_mae: 16.2388\n",
      "Epoch 2097/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5491 - mae: 16.5491 - val_loss: 16.2138 - val_mae: 16.2138\n",
      "Epoch 2098/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5422 - mae: 16.5422 - val_loss: 16.2229 - val_mae: 16.2229\n",
      "Epoch 2099/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5364 - mae: 16.5364 - val_loss: 16.2866 - val_mae: 16.2866\n",
      "Epoch 2100/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5406 - mae: 16.5406 - val_loss: 16.2413 - val_mae: 16.2413\n",
      "Epoch 2101/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5456 - mae: 16.5456 - val_loss: 16.2360 - val_mae: 16.2360\n",
      "Epoch 2102/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5454 - mae: 16.5454 - val_loss: 16.2241 - val_mae: 16.2241\n",
      "Epoch 2103/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5406 - mae: 16.5406 - val_loss: 16.2201 - val_mae: 16.2201\n",
      "Epoch 2104/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5427 - mae: 16.5427 - val_loss: 16.2404 - val_mae: 16.2404\n",
      "Epoch 2105/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5377 - mae: 16.5377 - val_loss: 16.2363 - val_mae: 16.2363\n",
      "Epoch 2106/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5486 - mae: 16.5486 - val_loss: 16.2186 - val_mae: 16.2186\n",
      "Epoch 2107/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5415 - mae: 16.5415 - val_loss: 16.2126 - val_mae: 16.2126\n",
      "Epoch 2108/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5251 - mae: 16.5251 - val_loss: 16.2681 - val_mae: 16.2681\n",
      "Epoch 2109/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5415 - mae: 16.5415 - val_loss: 16.2412 - val_mae: 16.2412\n",
      "Epoch 2110/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5459 - mae: 16.5459 - val_loss: 16.2155 - val_mae: 16.2155\n",
      "Epoch 2111/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5434 - mae: 16.5434 - val_loss: 16.2329 - val_mae: 16.2329\n",
      "Epoch 2112/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5365 - mae: 16.5365 - val_loss: 16.2145 - val_mae: 16.2145\n",
      "Epoch 2113/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5454 - mae: 16.5454 - val_loss: 16.2269 - val_mae: 16.2269\n",
      "Epoch 2114/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5372 - mae: 16.5372 - val_loss: 16.2147 - val_mae: 16.2147\n",
      "Epoch 2115/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5442 - mae: 16.5442 - val_loss: 16.2686 - val_mae: 16.2686\n",
      "Epoch 2116/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5440 - mae: 16.5440 - val_loss: 16.2165 - val_mae: 16.2165\n",
      "Epoch 2117/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5372 - mae: 16.5372 - val_loss: 16.2202 - val_mae: 16.2202\n",
      "Epoch 2118/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5360 - mae: 16.5360 - val_loss: 16.2211 - val_mae: 16.2211\n",
      "Epoch 2119/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5426 - mae: 16.5426 - val_loss: 16.2179 - val_mae: 16.2179\n",
      "Epoch 2120/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5350 - mae: 16.5350 - val_loss: 16.2179 - val_mae: 16.2179\n",
      "Epoch 2121/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5481 - mae: 16.5481 - val_loss: 16.2205 - val_mae: 16.2205\n",
      "Epoch 2122/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5427 - mae: 16.5427 - val_loss: 16.2237 - val_mae: 16.2237\n",
      "Epoch 2123/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5425 - mae: 16.5425 - val_loss: 16.2158 - val_mae: 16.2158\n",
      "Epoch 2124/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5411 - mae: 16.5411 - val_loss: 16.2467 - val_mae: 16.2467\n",
      "Epoch 2125/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5338 - mae: 16.5338 - val_loss: 16.2163 - val_mae: 16.2163\n",
      "Epoch 2126/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5426 - mae: 16.5426 - val_loss: 16.2171 - val_mae: 16.2171\n",
      "Epoch 2127/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5368 - mae: 16.5368 - val_loss: 16.2268 - val_mae: 16.2268\n",
      "Epoch 2128/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5403 - mae: 16.5403 - val_loss: 16.2188 - val_mae: 16.2188\n",
      "Epoch 2129/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5363 - mae: 16.5363 - val_loss: 16.2812 - val_mae: 16.2812\n",
      "Epoch 2130/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5378 - mae: 16.5378 - val_loss: 16.2360 - val_mae: 16.2360\n",
      "Epoch 2131/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5438 - mae: 16.5438 - val_loss: 16.2317 - val_mae: 16.2317\n",
      "Epoch 2132/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5359 - mae: 16.5359 - val_loss: 16.2123 - val_mae: 16.2123\n",
      "Epoch 2133/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5364 - mae: 16.5364 - val_loss: 16.2155 - val_mae: 16.2155\n",
      "Epoch 2134/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5363 - mae: 16.5363 - val_loss: 16.2287 - val_mae: 16.2287\n",
      "Epoch 2135/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5400 - mae: 16.5400 - val_loss: 16.2279 - val_mae: 16.2279\n",
      "Epoch 2136/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5401 - mae: 16.5402 - val_loss: 16.2179 - val_mae: 16.2179\n",
      "Epoch 2137/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5346 - mae: 16.5346 - val_loss: 16.2943 - val_mae: 16.2943\n",
      "Epoch 2138/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5352 - mae: 16.5352 - val_loss: 16.2507 - val_mae: 16.2507\n",
      "Epoch 2139/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5347 - mae: 16.5347 - val_loss: 16.2364 - val_mae: 16.2364\n",
      "Epoch 2140/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5398 - mae: 16.5398 - val_loss: 16.2158 - val_mae: 16.2158\n",
      "Epoch 2141/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5399 - mae: 16.5399 - val_loss: 16.2404 - val_mae: 16.2404\n",
      "Epoch 2142/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5318 - mae: 16.5318 - val_loss: 16.2158 - val_mae: 16.2158\n",
      "Epoch 2143/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5415 - mae: 16.5415 - val_loss: 16.2336 - val_mae: 16.2336\n",
      "Epoch 2144/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5403 - mae: 16.5403 - val_loss: 16.2214 - val_mae: 16.2214\n",
      "Epoch 2145/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5285 - mae: 16.5285 - val_loss: 16.3063 - val_mae: 16.3063\n",
      "Epoch 2146/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5446 - mae: 16.5446 - val_loss: 16.2314 - val_mae: 16.2314\n",
      "Epoch 2147/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5306 - mae: 16.5306 - val_loss: 16.2248 - val_mae: 16.2248\n",
      "Epoch 2148/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5409 - mae: 16.5409 - val_loss: 16.2153 - val_mae: 16.2153\n",
      "Epoch 2149/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5392 - mae: 16.5392 - val_loss: 16.2105 - val_mae: 16.2105\n",
      "Epoch 2150/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5379 - mae: 16.5379 - val_loss: 16.2195 - val_mae: 16.2195\n",
      "Epoch 2151/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5399 - mae: 16.5399 - val_loss: 16.2384 - val_mae: 16.2384\n",
      "Epoch 2152/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5414 - mae: 16.5414 - val_loss: 16.2296 - val_mae: 16.2296\n",
      "Epoch 2153/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5341 - mae: 16.5341 - val_loss: 16.2246 - val_mae: 16.2246\n",
      "Epoch 2154/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5358 - mae: 16.5358 - val_loss: 16.2079 - val_mae: 16.2079\n",
      "Epoch 2155/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5336 - mae: 16.5336 - val_loss: 16.2188 - val_mae: 16.2188\n",
      "Epoch 2156/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5271 - mae: 16.5271 - val_loss: 16.2175 - val_mae: 16.2175\n",
      "Epoch 2157/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5320 - mae: 16.5320 - val_loss: 16.2231 - val_mae: 16.2231\n",
      "Epoch 2158/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5290 - mae: 16.5290 - val_loss: 16.2082 - val_mae: 16.2082\n",
      "Epoch 2159/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5291 - mae: 16.5291 - val_loss: 16.2564 - val_mae: 16.2564\n",
      "Epoch 2160/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5394 - mae: 16.5394 - val_loss: 16.2316 - val_mae: 16.2316\n",
      "Epoch 2161/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5361 - mae: 16.5361 - val_loss: 16.2291 - val_mae: 16.2291\n",
      "Epoch 2162/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5395 - mae: 16.5395 - val_loss: 16.2353 - val_mae: 16.2353\n",
      "Epoch 2163/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5302 - mae: 16.5302 - val_loss: 16.2130 - val_mae: 16.2130\n",
      "Epoch 2164/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.5306 - mae: 16.5306 - val_loss: 16.2212 - val_mae: 16.2212\n",
      "Epoch 2165/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5375 - mae: 16.5375 - val_loss: 16.2241 - val_mae: 16.2241\n",
      "Epoch 2166/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5329 - mae: 16.5329 - val_loss: 16.2116 - val_mae: 16.2116\n",
      "Epoch 2167/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5362 - mae: 16.5362 - val_loss: 16.2899 - val_mae: 16.2899\n",
      "Epoch 2168/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5367 - mae: 16.5367 - val_loss: 16.2120 - val_mae: 16.2120\n",
      "Epoch 2169/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5358 - mae: 16.5358 - val_loss: 16.2085 - val_mae: 16.2085\n",
      "Epoch 2170/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5348 - mae: 16.5348 - val_loss: 16.2103 - val_mae: 16.2103\n",
      "Epoch 2171/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5297 - mae: 16.5297 - val_loss: 16.2456 - val_mae: 16.2456\n",
      "Epoch 2172/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5342 - mae: 16.5342 - val_loss: 16.2315 - val_mae: 16.2315\n",
      "Epoch 2173/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5344 - mae: 16.5344 - val_loss: 16.2306 - val_mae: 16.2306\n",
      "Epoch 2174/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5291 - mae: 16.5291 - val_loss: 16.2867 - val_mae: 16.2867\n",
      "Epoch 2175/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5302 - mae: 16.5302 - val_loss: 16.2083 - val_mae: 16.2083\n",
      "Epoch 2176/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5303 - mae: 16.5303 - val_loss: 16.2257 - val_mae: 16.2257\n",
      "Epoch 2177/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5362 - mae: 16.5362 - val_loss: 16.2146 - val_mae: 16.2146\n",
      "Epoch 2178/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5313 - mae: 16.5313 - val_loss: 16.2315 - val_mae: 16.2314\n",
      "Epoch 2179/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5315 - mae: 16.5315 - val_loss: 16.2115 - val_mae: 16.2115\n",
      "Epoch 2180/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5359 - mae: 16.5359 - val_loss: 16.2126 - val_mae: 16.2126\n",
      "Epoch 2181/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5310 - mae: 16.5310 - val_loss: 16.2089 - val_mae: 16.2089\n",
      "Epoch 2182/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5310 - mae: 16.5310 - val_loss: 16.2103 - val_mae: 16.2103\n",
      "Epoch 2183/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5370 - mae: 16.5370 - val_loss: 16.2215 - val_mae: 16.2215\n",
      "Epoch 2184/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5323 - mae: 16.5322 - val_loss: 16.2449 - val_mae: 16.2449\n",
      "Epoch 2185/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5327 - mae: 16.5327 - val_loss: 16.2369 - val_mae: 16.2369\n",
      "Epoch 2186/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5380 - mae: 16.5380 - val_loss: 16.2447 - val_mae: 16.2447\n",
      "Epoch 2187/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5180 - mae: 16.5180 - val_loss: 16.2167 - val_mae: 16.2167\n",
      "Epoch 2188/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5273 - mae: 16.5273 - val_loss: 16.2510 - val_mae: 16.2510\n",
      "Epoch 2189/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5285 - mae: 16.5285 - val_loss: 16.2909 - val_mae: 16.2909\n",
      "Epoch 2190/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5318 - mae: 16.5318 - val_loss: 16.2096 - val_mae: 16.2095\n",
      "Epoch 2191/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5265 - mae: 16.5265 - val_loss: 16.2334 - val_mae: 16.2335\n",
      "Epoch 2192/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5325 - mae: 16.5325 - val_loss: 16.2292 - val_mae: 16.2292\n",
      "Epoch 2193/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5364 - mae: 16.5364 - val_loss: 16.2600 - val_mae: 16.2600\n",
      "Epoch 2194/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5286 - mae: 16.5286 - val_loss: 16.2068 - val_mae: 16.2068\n",
      "Epoch 2195/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5276 - mae: 16.5276 - val_loss: 16.2080 - val_mae: 16.2080\n",
      "Epoch 2196/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5327 - mae: 16.5327 - val_loss: 16.2290 - val_mae: 16.2290\n",
      "Epoch 2197/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5286 - mae: 16.5286 - val_loss: 16.2070 - val_mae: 16.2070\n",
      "Epoch 2198/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5253 - mae: 16.5252 - val_loss: 16.2114 - val_mae: 16.2114\n",
      "Epoch 2199/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5277 - mae: 16.5277 - val_loss: 16.2077 - val_mae: 16.2077\n",
      "Epoch 2200/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5340 - mae: 16.5340 - val_loss: 16.2428 - val_mae: 16.2428\n",
      "Epoch 2201/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5295 - mae: 16.5295 - val_loss: 16.2545 - val_mae: 16.2545\n",
      "Epoch 2202/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5312 - mae: 16.5312 - val_loss: 16.2457 - val_mae: 16.2457\n",
      "Epoch 2203/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5313 - mae: 16.5313 - val_loss: 16.2337 - val_mae: 16.2337\n",
      "Epoch 2204/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5306 - mae: 16.5306 - val_loss: 16.2401 - val_mae: 16.2401\n",
      "Epoch 2205/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5329 - mae: 16.5329 - val_loss: 16.2078 - val_mae: 16.2078\n",
      "Epoch 2206/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5249 - mae: 16.5249 - val_loss: 16.2129 - val_mae: 16.2129\n",
      "Epoch 2207/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5305 - mae: 16.5305 - val_loss: 16.2118 - val_mae: 16.2118\n",
      "Epoch 2208/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5370 - mae: 16.5370 - val_loss: 16.2208 - val_mae: 16.2208\n",
      "Epoch 2209/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5321 - mae: 16.5321 - val_loss: 16.2059 - val_mae: 16.2059\n",
      "Epoch 2210/3000\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 16.5225 - mae: 16.5225 - val_loss: 16.2759 - val_mae: 16.2759\n",
      "Epoch 2211/3000\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 16.5258 - mae: 16.5258 - val_loss: 16.2118 - val_mae: 16.2118\n",
      "Epoch 2212/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5329 - mae: 16.5329 - val_loss: 16.2349 - val_mae: 16.2349\n",
      "Epoch 2213/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5294 - mae: 16.5294 - val_loss: 16.2247 - val_mae: 16.2247\n",
      "Epoch 2214/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5305 - mae: 16.5305 - val_loss: 16.2200 - val_mae: 16.2200\n",
      "Epoch 2215/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.5215 - mae: 16.5215 - val_loss: 16.2174 - val_mae: 16.2174\n",
      "Epoch 2216/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5272 - mae: 16.5272 - val_loss: 16.2100 - val_mae: 16.2101\n",
      "Epoch 2217/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.5267 - mae: 16.5267 - val_loss: 16.2378 - val_mae: 16.2378\n",
      "Epoch 2218/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5297 - mae: 16.5297 - val_loss: 16.2078 - val_mae: 16.2078\n",
      "Epoch 2219/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5283 - mae: 16.5283 - val_loss: 16.2179 - val_mae: 16.2179\n",
      "Epoch 2220/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5278 - mae: 16.5278 - val_loss: 16.2552 - val_mae: 16.2552\n",
      "Epoch 2221/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5281 - mae: 16.5281 - val_loss: 16.2121 - val_mae: 16.2121\n",
      "Epoch 2222/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5313 - mae: 16.5313 - val_loss: 16.2103 - val_mae: 16.2103\n",
      "Epoch 2223/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5309 - mae: 16.5309 - val_loss: 16.2227 - val_mae: 16.2227\n",
      "Epoch 2224/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5242 - mae: 16.5242 - val_loss: 16.2298 - val_mae: 16.2298\n",
      "Epoch 2225/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.5280 - mae: 16.5280 - val_loss: 16.2071 - val_mae: 16.2071\n",
      "Epoch 2226/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5230 - mae: 16.5230 - val_loss: 16.2375 - val_mae: 16.2375\n",
      "Epoch 2227/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5261 - mae: 16.5261 - val_loss: 16.2079 - val_mae: 16.2079\n",
      "Epoch 2228/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5254 - mae: 16.5254 - val_loss: 16.2126 - val_mae: 16.2126\n",
      "Epoch 2229/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5331 - mae: 16.5331 - val_loss: 16.2279 - val_mae: 16.2279\n",
      "Epoch 2230/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5289 - mae: 16.5289 - val_loss: 16.2059 - val_mae: 16.2059\n",
      "Epoch 2231/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5291 - mae: 16.5290 - val_loss: 16.2177 - val_mae: 16.2177\n",
      "Epoch 2232/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5170 - mae: 16.5170 - val_loss: 16.2114 - val_mae: 16.2114\n",
      "Epoch 2233/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5305 - mae: 16.5305 - val_loss: 16.2193 - val_mae: 16.2193\n",
      "Epoch 2234/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5267 - mae: 16.5266 - val_loss: 16.2942 - val_mae: 16.2942\n",
      "Epoch 2235/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5306 - mae: 16.5306 - val_loss: 16.2446 - val_mae: 16.2446\n",
      "Epoch 2236/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5311 - mae: 16.5311 - val_loss: 16.2156 - val_mae: 16.2156\n",
      "Epoch 2237/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5288 - mae: 16.5288 - val_loss: 16.2295 - val_mae: 16.2295\n",
      "Epoch 2238/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5211 - mae: 16.5211 - val_loss: 16.2063 - val_mae: 16.2063\n",
      "Epoch 2239/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5259 - mae: 16.5259 - val_loss: 16.2029 - val_mae: 16.2029\n",
      "Epoch 2240/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5313 - mae: 16.5313 - val_loss: 16.2173 - val_mae: 16.2173\n",
      "Epoch 2241/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5169 - mae: 16.5169 - val_loss: 16.2582 - val_mae: 16.2582\n",
      "Epoch 2242/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5232 - mae: 16.5232 - val_loss: 16.2691 - val_mae: 16.2691\n",
      "Epoch 2243/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5250 - mae: 16.5250 - val_loss: 16.2167 - val_mae: 16.2167\n",
      "Epoch 2244/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5189 - mae: 16.5189 - val_loss: 16.2099 - val_mae: 16.2099\n",
      "Epoch 2245/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5271 - mae: 16.5271 - val_loss: 16.2043 - val_mae: 16.2043\n",
      "Epoch 2246/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5198 - mae: 16.5198 - val_loss: 16.2088 - val_mae: 16.2088\n",
      "Epoch 2247/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5281 - mae: 16.5281 - val_loss: 16.2228 - val_mae: 16.2228\n",
      "Epoch 2248/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5230 - mae: 16.5230 - val_loss: 16.2241 - val_mae: 16.2241\n",
      "Epoch 2249/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5267 - mae: 16.5267 - val_loss: 16.2071 - val_mae: 16.2071\n",
      "Epoch 2250/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5244 - mae: 16.5244 - val_loss: 16.2076 - val_mae: 16.2076\n",
      "Epoch 2251/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5279 - mae: 16.5279 - val_loss: 16.2325 - val_mae: 16.2325\n",
      "Epoch 2252/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5234 - mae: 16.5234 - val_loss: 16.2314 - val_mae: 16.2314\n",
      "Epoch 2253/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5144 - mae: 16.5144 - val_loss: 16.2222 - val_mae: 16.2222\n",
      "Epoch 2254/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5302 - mae: 16.5302 - val_loss: 16.2269 - val_mae: 16.2269\n",
      "Epoch 2255/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5213 - mae: 16.5213 - val_loss: 16.2724 - val_mae: 16.2724\n",
      "Epoch 2256/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5240 - mae: 16.5240 - val_loss: 16.2171 - val_mae: 16.2171\n",
      "Epoch 2257/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5245 - mae: 16.5245 - val_loss: 16.2068 - val_mae: 16.2068\n",
      "Epoch 2258/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5181 - mae: 16.5181 - val_loss: 16.2568 - val_mae: 16.2568\n",
      "Epoch 2259/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5288 - mae: 16.5288 - val_loss: 16.2155 - val_mae: 16.2155\n",
      "Epoch 2260/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5248 - mae: 16.5248 - val_loss: 16.2057 - val_mae: 16.2057\n",
      "Epoch 2261/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5224 - mae: 16.5223 - val_loss: 16.2301 - val_mae: 16.2301\n",
      "Epoch 2262/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5268 - mae: 16.5268 - val_loss: 16.2089 - val_mae: 16.2089\n",
      "Epoch 2263/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5233 - mae: 16.5233 - val_loss: 16.2046 - val_mae: 16.2046\n",
      "Epoch 2264/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5231 - mae: 16.5231 - val_loss: 16.2227 - val_mae: 16.2227\n",
      "Epoch 2265/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5205 - mae: 16.5204 - val_loss: 16.2567 - val_mae: 16.2567\n",
      "Epoch 2266/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5132 - mae: 16.5132 - val_loss: 16.2173 - val_mae: 16.2173\n",
      "Epoch 2267/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5182 - mae: 16.5182 - val_loss: 16.2108 - val_mae: 16.2108\n",
      "Epoch 2268/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5268 - mae: 16.5268 - val_loss: 16.2070 - val_mae: 16.2070\n",
      "Epoch 2269/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5203 - mae: 16.5203 - val_loss: 16.2188 - val_mae: 16.2188\n",
      "Epoch 2270/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5282 - mae: 16.5282 - val_loss: 16.2190 - val_mae: 16.2190\n",
      "Epoch 2271/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5239 - mae: 16.5239 - val_loss: 16.2112 - val_mae: 16.2112\n",
      "Epoch 2272/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5238 - mae: 16.5238 - val_loss: 16.2504 - val_mae: 16.2504\n",
      "Epoch 2273/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5220 - mae: 16.5220 - val_loss: 16.2254 - val_mae: 16.2254\n",
      "Epoch 2274/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5208 - mae: 16.5208 - val_loss: 16.2019 - val_mae: 16.2019\n",
      "Epoch 2275/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5221 - mae: 16.5221 - val_loss: 16.2017 - val_mae: 16.2017\n",
      "Epoch 2276/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5211 - mae: 16.5211 - val_loss: 16.2468 - val_mae: 16.2468\n",
      "Epoch 2277/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5250 - mae: 16.5250 - val_loss: 16.2222 - val_mae: 16.2222\n",
      "Epoch 2278/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5104 - mae: 16.5104 - val_loss: 16.2723 - val_mae: 16.2723\n",
      "Epoch 2279/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5246 - mae: 16.5246 - val_loss: 16.2015 - val_mae: 16.2015\n",
      "Epoch 2280/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5137 - mae: 16.5137 - val_loss: 16.2391 - val_mae: 16.2391\n",
      "Epoch 2281/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5255 - mae: 16.5255 - val_loss: 16.2019 - val_mae: 16.2019\n",
      "Epoch 2282/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5184 - mae: 16.5184 - val_loss: 16.2043 - val_mae: 16.2043\n",
      "Epoch 2283/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.5251 - mae: 16.5251 - val_loss: 16.2202 - val_mae: 16.2202\n",
      "Epoch 2284/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5101 - mae: 16.5101 - val_loss: 16.2046 - val_mae: 16.2046\n",
      "Epoch 2285/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5155 - mae: 16.5155 - val_loss: 16.2110 - val_mae: 16.2110\n",
      "Epoch 2286/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5175 - mae: 16.5175 - val_loss: 16.2113 - val_mae: 16.2113\n",
      "Epoch 2287/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5243 - mae: 16.5243 - val_loss: 16.2525 - val_mae: 16.2525\n",
      "Epoch 2288/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5165 - mae: 16.5165 - val_loss: 16.2201 - val_mae: 16.2201\n",
      "Epoch 2289/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5206 - mae: 16.5206 - val_loss: 16.2156 - val_mae: 16.2156\n",
      "Epoch 2290/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5197 - mae: 16.5197 - val_loss: 16.2857 - val_mae: 16.2857\n",
      "Epoch 2291/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5175 - mae: 16.5175 - val_loss: 16.2225 - val_mae: 16.2225\n",
      "Epoch 2292/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5208 - mae: 16.5208 - val_loss: 16.2324 - val_mae: 16.2324\n",
      "Epoch 2293/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5201 - mae: 16.5200 - val_loss: 16.2507 - val_mae: 16.2507\n",
      "Epoch 2294/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5195 - mae: 16.5195 - val_loss: 16.2258 - val_mae: 16.2258\n",
      "Epoch 2295/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5176 - mae: 16.5176 - val_loss: 16.2566 - val_mae: 16.2566\n",
      "Epoch 2296/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5131 - mae: 16.5131 - val_loss: 16.2506 - val_mae: 16.2506\n",
      "Epoch 2297/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5258 - mae: 16.5258 - val_loss: 16.2124 - val_mae: 16.2124\n",
      "Epoch 2298/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5189 - mae: 16.5189 - val_loss: 16.2178 - val_mae: 16.2178\n",
      "Epoch 2299/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5190 - mae: 16.5190 - val_loss: 16.2134 - val_mae: 16.2134\n",
      "Epoch 2300/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5178 - mae: 16.5178 - val_loss: 16.2042 - val_mae: 16.2042\n",
      "Epoch 2301/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.5189 - mae: 16.5189 - val_loss: 16.2272 - val_mae: 16.2272\n",
      "Epoch 2302/3000\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 16.5176 - mae: 16.5177 - val_loss: 16.2083 - val_mae: 16.2083\n",
      "Epoch 2303/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5168 - mae: 16.5168 - val_loss: 16.2665 - val_mae: 16.2665\n",
      "Epoch 2304/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5165 - mae: 16.5165 - val_loss: 16.2553 - val_mae: 16.2553\n",
      "Epoch 2305/3000\n",
      "10496/10496 [==============================] - 1s 58us/sample - loss: 16.5171 - mae: 16.5171 - val_loss: 16.2056 - val_mae: 16.2056\n",
      "Epoch 2306/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5099 - mae: 16.5099 - val_loss: 16.2248 - val_mae: 16.2248\n",
      "Epoch 2307/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5141 - mae: 16.5142 - val_loss: 16.2257 - val_mae: 16.2257\n",
      "Epoch 2308/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5210 - mae: 16.5210 - val_loss: 16.2055 - val_mae: 16.2055\n",
      "Epoch 2309/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5133 - mae: 16.5133 - val_loss: 16.2005 - val_mae: 16.2005\n",
      "Epoch 2310/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5175 - mae: 16.5175 - val_loss: 16.2334 - val_mae: 16.2334\n",
      "Epoch 2311/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5129 - mae: 16.5129 - val_loss: 16.2064 - val_mae: 16.2064\n",
      "Epoch 2312/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5194 - mae: 16.5194 - val_loss: 16.2005 - val_mae: 16.2005\n",
      "Epoch 2313/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5191 - mae: 16.5191 - val_loss: 16.2499 - val_mae: 16.2499\n",
      "Epoch 2314/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5188 - mae: 16.5188 - val_loss: 16.2094 - val_mae: 16.2094\n",
      "Epoch 2315/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5157 - mae: 16.5157 - val_loss: 16.2357 - val_mae: 16.2357\n",
      "Epoch 2316/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5157 - mae: 16.5157 - val_loss: 16.2185 - val_mae: 16.2185\n",
      "Epoch 2317/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5211 - mae: 16.5211 - val_loss: 16.2263 - val_mae: 16.2263\n",
      "Epoch 2318/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5118 - mae: 16.5118 - val_loss: 16.2538 - val_mae: 16.2538\n",
      "Epoch 2319/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5139 - mae: 16.5139 - val_loss: 16.2025 - val_mae: 16.2025\n",
      "Epoch 2320/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5147 - mae: 16.5147 - val_loss: 16.2301 - val_mae: 16.2301\n",
      "Epoch 2321/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5198 - mae: 16.5198 - val_loss: 16.2033 - val_mae: 16.2033\n",
      "Epoch 2322/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5212 - mae: 16.5212 - val_loss: 16.2051 - val_mae: 16.2051\n",
      "Epoch 2323/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5177 - mae: 16.5177 - val_loss: 16.2431 - val_mae: 16.2431\n",
      "Epoch 2324/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5160 - mae: 16.5160 - val_loss: 16.2046 - val_mae: 16.2046\n",
      "Epoch 2325/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5134 - mae: 16.5134 - val_loss: 16.2466 - val_mae: 16.2466\n",
      "Epoch 2326/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5114 - mae: 16.5114 - val_loss: 16.2319 - val_mae: 16.2319\n",
      "Epoch 2327/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5166 - mae: 16.5166 - val_loss: 16.2583 - val_mae: 16.2583\n",
      "Epoch 2328/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5129 - mae: 16.5129 - val_loss: 16.2231 - val_mae: 16.2231\n",
      "Epoch 2329/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5112 - mae: 16.5112 - val_loss: 16.2323 - val_mae: 16.2323\n",
      "Epoch 2330/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5170 - mae: 16.5170 - val_loss: 16.2065 - val_mae: 16.2065\n",
      "Epoch 2331/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5152 - mae: 16.5152 - val_loss: 16.2185 - val_mae: 16.2185\n",
      "Epoch 2332/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5162 - mae: 16.5162 - val_loss: 16.2020 - val_mae: 16.2020\n",
      "Epoch 2333/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5123 - mae: 16.5123 - val_loss: 16.2062 - val_mae: 16.2062\n",
      "Epoch 2334/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5115 - mae: 16.5115 - val_loss: 16.2095 - val_mae: 16.2095\n",
      "Epoch 2335/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5151 - mae: 16.5151 - val_loss: 16.2441 - val_mae: 16.2441\n",
      "Epoch 2336/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5136 - mae: 16.5136 - val_loss: 16.2055 - val_mae: 16.2055\n",
      "Epoch 2337/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5137 - mae: 16.5137 - val_loss: 16.2239 - val_mae: 16.2239\n",
      "Epoch 2338/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5160 - mae: 16.5160 - val_loss: 16.2411 - val_mae: 16.2411\n",
      "Epoch 2339/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5080 - mae: 16.5080 - val_loss: 16.2266 - val_mae: 16.2266\n",
      "Epoch 2340/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5123 - mae: 16.5123 - val_loss: 16.2007 - val_mae: 16.2007\n",
      "Epoch 2341/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5194 - mae: 16.5194 - val_loss: 16.1993 - val_mae: 16.1993\n",
      "Epoch 2342/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5092 - mae: 16.5092 - val_loss: 16.2498 - val_mae: 16.2498\n",
      "Epoch 2343/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5180 - mae: 16.5180 - val_loss: 16.2024 - val_mae: 16.2024\n",
      "Epoch 2344/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5107 - mae: 16.5107 - val_loss: 16.2761 - val_mae: 16.2761\n",
      "Epoch 2345/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5143 - mae: 16.5143 - val_loss: 16.2315 - val_mae: 16.2315\n",
      "Epoch 2346/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5151 - mae: 16.5151 - val_loss: 16.2025 - val_mae: 16.2025\n",
      "Epoch 2347/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5123 - mae: 16.5123 - val_loss: 16.2099 - val_mae: 16.2099\n",
      "Epoch 2348/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5133 - mae: 16.5133 - val_loss: 16.2244 - val_mae: 16.2244\n",
      "Epoch 2349/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5166 - mae: 16.5166 - val_loss: 16.2012 - val_mae: 16.2012\n",
      "Epoch 2350/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5055 - mae: 16.5055 - val_loss: 16.1992 - val_mae: 16.1992\n",
      "Epoch 2351/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5143 - mae: 16.5143 - val_loss: 16.1995 - val_mae: 16.1995\n",
      "Epoch 2352/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5131 - mae: 16.5131 - val_loss: 16.2013 - val_mae: 16.2013\n",
      "Epoch 2353/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5154 - mae: 16.5154 - val_loss: 16.2580 - val_mae: 16.2580\n",
      "Epoch 2354/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5187 - mae: 16.5187 - val_loss: 16.2031 - val_mae: 16.2031\n",
      "Epoch 2355/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5113 - mae: 16.5113 - val_loss: 16.2514 - val_mae: 16.2514\n",
      "Epoch 2356/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5151 - mae: 16.5151 - val_loss: 16.2082 - val_mae: 16.2082\n",
      "Epoch 2357/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5127 - mae: 16.5127 - val_loss: 16.2150 - val_mae: 16.2150\n",
      "Epoch 2358/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5187 - mae: 16.5187 - val_loss: 16.2100 - val_mae: 16.2100\n",
      "Epoch 2359/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5086 - mae: 16.5086 - val_loss: 16.2383 - val_mae: 16.2383\n",
      "Epoch 2360/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5169 - mae: 16.5169 - val_loss: 16.2098 - val_mae: 16.2098\n",
      "Epoch 2361/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5131 - mae: 16.5131 - val_loss: 16.1997 - val_mae: 16.1996\n",
      "Epoch 2362/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5175 - mae: 16.5175 - val_loss: 16.2190 - val_mae: 16.2190\n",
      "Epoch 2363/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5197 - mae: 16.5197 - val_loss: 16.2175 - val_mae: 16.2175\n",
      "Epoch 2364/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5087 - mae: 16.5087 - val_loss: 16.1990 - val_mae: 16.1990\n",
      "Epoch 2365/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5108 - mae: 16.5108 - val_loss: 16.1963 - val_mae: 16.1963\n",
      "Epoch 2366/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5119 - mae: 16.5119 - val_loss: 16.2050 - val_mae: 16.2050\n",
      "Epoch 2367/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5118 - mae: 16.5118 - val_loss: 16.2406 - val_mae: 16.2406\n",
      "Epoch 2368/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5133 - mae: 16.5133 - val_loss: 16.2023 - val_mae: 16.2023\n",
      "Epoch 2369/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5166 - mae: 16.5166 - val_loss: 16.1976 - val_mae: 16.1976\n",
      "Epoch 2370/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5105 - mae: 16.5105 - val_loss: 16.2082 - val_mae: 16.2082\n",
      "Epoch 2371/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5077 - mae: 16.5077 - val_loss: 16.2358 - val_mae: 16.2358\n",
      "Epoch 2372/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5119 - mae: 16.5119 - val_loss: 16.1997 - val_mae: 16.1997\n",
      "Epoch 2373/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5121 - mae: 16.5121 - val_loss: 16.2258 - val_mae: 16.2258\n",
      "Epoch 2374/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5124 - mae: 16.5124 - val_loss: 16.2008 - val_mae: 16.2008\n",
      "Epoch 2375/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5077 - mae: 16.5077 - val_loss: 16.2207 - val_mae: 16.2207\n",
      "Epoch 2376/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5116 - mae: 16.5116 - val_loss: 16.2303 - val_mae: 16.2303\n",
      "Epoch 2377/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5088 - mae: 16.5088 - val_loss: 16.2079 - val_mae: 16.2079\n",
      "Epoch 2378/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5143 - mae: 16.5143 - val_loss: 16.2173 - val_mae: 16.2173\n",
      "Epoch 2379/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5112 - mae: 16.5112 - val_loss: 16.2606 - val_mae: 16.2606\n",
      "Epoch 2380/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5143 - mae: 16.5143 - val_loss: 16.2050 - val_mae: 16.2050\n",
      "Epoch 2381/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4996 - mae: 16.4996 - val_loss: 16.2010 - val_mae: 16.2010\n",
      "Epoch 2382/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5155 - mae: 16.5155 - val_loss: 16.2004 - val_mae: 16.2004\n",
      "Epoch 2383/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5087 - mae: 16.5087 - val_loss: 16.2805 - val_mae: 16.2805\n",
      "Epoch 2384/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5101 - mae: 16.5101 - val_loss: 16.2060 - val_mae: 16.2060\n",
      "Epoch 2385/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5044 - mae: 16.5044 - val_loss: 16.1973 - val_mae: 16.1973\n",
      "Epoch 2386/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5115 - mae: 16.5115 - val_loss: 16.2002 - val_mae: 16.2002\n",
      "Epoch 2387/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5089 - mae: 16.5089 - val_loss: 16.2163 - val_mae: 16.2162\n",
      "Epoch 2388/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5120 - mae: 16.5120 - val_loss: 16.2379 - val_mae: 16.2379\n",
      "Epoch 2389/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5132 - mae: 16.5132 - val_loss: 16.2036 - val_mae: 16.2036\n",
      "Epoch 2390/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5080 - mae: 16.5080 - val_loss: 16.1970 - val_mae: 16.1970\n",
      "Epoch 2391/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5059 - mae: 16.5059 - val_loss: 16.2022 - val_mae: 16.2022\n",
      "Epoch 2392/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5034 - mae: 16.5033 - val_loss: 16.1996 - val_mae: 16.1996\n",
      "Epoch 2393/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5099 - mae: 16.5099 - val_loss: 16.2058 - val_mae: 16.2058\n",
      "Epoch 2394/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5115 - mae: 16.5115 - val_loss: 16.1960 - val_mae: 16.1960\n",
      "Epoch 2395/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5056 - mae: 16.5056 - val_loss: 16.1957 - val_mae: 16.1957\n",
      "Epoch 2396/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5098 - mae: 16.5098 - val_loss: 16.2050 - val_mae: 16.2050\n",
      "Epoch 2397/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5124 - mae: 16.5124 - val_loss: 16.1970 - val_mae: 16.1970\n",
      "Epoch 2398/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5069 - mae: 16.5069 - val_loss: 16.1972 - val_mae: 16.1972\n",
      "Epoch 2399/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5118 - mae: 16.5118 - val_loss: 16.2039 - val_mae: 16.2039\n",
      "Epoch 2400/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4937 - mae: 16.4937 - val_loss: 16.3217 - val_mae: 16.3217\n",
      "Epoch 2401/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5104 - mae: 16.5104 - val_loss: 16.2119 - val_mae: 16.2119\n",
      "Epoch 2402/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5108 - mae: 16.5108 - val_loss: 16.2305 - val_mae: 16.2305\n",
      "Epoch 2403/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5079 - mae: 16.5079 - val_loss: 16.2001 - val_mae: 16.2001\n",
      "Epoch 2404/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5126 - mae: 16.5125 - val_loss: 16.1987 - val_mae: 16.1987\n",
      "Epoch 2405/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5099 - mae: 16.5099 - val_loss: 16.2389 - val_mae: 16.2389\n",
      "Epoch 2406/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5092 - mae: 16.5092 - val_loss: 16.2171 - val_mae: 16.2171\n",
      "Epoch 2407/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5116 - mae: 16.5116 - val_loss: 16.2326 - val_mae: 16.2326\n",
      "Epoch 2408/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5047 - mae: 16.5047 - val_loss: 16.2702 - val_mae: 16.2702\n",
      "Epoch 2409/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5064 - mae: 16.5064 - val_loss: 16.2004 - val_mae: 16.2004\n",
      "Epoch 2410/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5119 - mae: 16.5119 - val_loss: 16.1975 - val_mae: 16.1975\n",
      "Epoch 2411/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5087 - mae: 16.5087 - val_loss: 16.2503 - val_mae: 16.2503\n",
      "Epoch 2412/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5089 - mae: 16.5089 - val_loss: 16.2013 - val_mae: 16.2013\n",
      "Epoch 2413/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5023 - mae: 16.5023 - val_loss: 16.2764 - val_mae: 16.2764\n",
      "Epoch 2414/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5053 - mae: 16.5053 - val_loss: 16.1981 - val_mae: 16.1980\n",
      "Epoch 2415/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5040 - mae: 16.5040 - val_loss: 16.2921 - val_mae: 16.2921\n",
      "Epoch 2416/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5044 - mae: 16.5044 - val_loss: 16.2030 - val_mae: 16.2030\n",
      "Epoch 2417/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5046 - mae: 16.5046 - val_loss: 16.1949 - val_mae: 16.1949\n",
      "Epoch 2418/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5099 - mae: 16.5099 - val_loss: 16.2074 - val_mae: 16.2074\n",
      "Epoch 2419/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5056 - mae: 16.5056 - val_loss: 16.2537 - val_mae: 16.2537\n",
      "Epoch 2420/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5084 - mae: 16.5084 - val_loss: 16.2546 - val_mae: 16.2546\n",
      "Epoch 2421/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5083 - mae: 16.5083 - val_loss: 16.1956 - val_mae: 16.1956\n",
      "Epoch 2422/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5117 - mae: 16.5117 - val_loss: 16.2443 - val_mae: 16.2443\n",
      "Epoch 2423/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5096 - mae: 16.5096 - val_loss: 16.2045 - val_mae: 16.2045\n",
      "Epoch 2424/3000\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 16.5088 - mae: 16.5088 - val_loss: 16.1992 - val_mae: 16.1992\n",
      "Epoch 2425/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5005 - mae: 16.5005 - val_loss: 16.2008 - val_mae: 16.2008\n",
      "Epoch 2426/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5040 - mae: 16.5040 - val_loss: 16.2049 - val_mae: 16.2049\n",
      "Epoch 2427/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5119 - mae: 16.5119 - val_loss: 16.2028 - val_mae: 16.2028\n",
      "Epoch 2428/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.5096 - mae: 16.5096 - val_loss: 16.2088 - val_mae: 16.2088\n",
      "Epoch 2429/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5047 - mae: 16.5047 - val_loss: 16.2122 - val_mae: 16.2122\n",
      "Epoch 2430/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5021 - mae: 16.5021 - val_loss: 16.2721 - val_mae: 16.2721\n",
      "Epoch 2431/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4987 - mae: 16.4987 - val_loss: 16.1987 - val_mae: 16.1987\n",
      "Epoch 2432/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5040 - mae: 16.5040 - val_loss: 16.2000 - val_mae: 16.2000\n",
      "Epoch 2433/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5080 - mae: 16.5080 - val_loss: 16.1987 - val_mae: 16.1987\n",
      "Epoch 2434/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5034 - mae: 16.5034 - val_loss: 16.2034 - val_mae: 16.2034\n",
      "Epoch 2435/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5093 - mae: 16.5093 - val_loss: 16.2022 - val_mae: 16.2022\n",
      "Epoch 2436/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5036 - mae: 16.5036 - val_loss: 16.2032 - val_mae: 16.2032\n",
      "Epoch 2437/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5081 - mae: 16.5081 - val_loss: 16.2121 - val_mae: 16.2121\n",
      "Epoch 2438/3000\n",
      "10496/10496 [==============================] - 1s 55us/sample - loss: 16.5015 - mae: 16.5015 - val_loss: 16.2399 - val_mae: 16.2399\n",
      "Epoch 2439/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5064 - mae: 16.5064 - val_loss: 16.2084 - val_mae: 16.2084\n",
      "Epoch 2440/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5062 - mae: 16.5062 - val_loss: 16.2067 - val_mae: 16.2067\n",
      "Epoch 2441/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5013 - mae: 16.5013 - val_loss: 16.1975 - val_mae: 16.1975\n",
      "Epoch 2442/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5068 - mae: 16.5068 - val_loss: 16.2150 - val_mae: 16.2150\n",
      "Epoch 2443/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5063 - mae: 16.5063 - val_loss: 16.2044 - val_mae: 16.2044\n",
      "Epoch 2444/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4980 - mae: 16.4980 - val_loss: 16.2244 - val_mae: 16.2244\n",
      "Epoch 2445/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5064 - mae: 16.5064 - val_loss: 16.2231 - val_mae: 16.2231\n",
      "Epoch 2446/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5019 - mae: 16.5019 - val_loss: 16.2663 - val_mae: 16.2663\n",
      "Epoch 2447/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5055 - mae: 16.5055 - val_loss: 16.2272 - val_mae: 16.2272\n",
      "Epoch 2448/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4999 - mae: 16.4999 - val_loss: 16.2021 - val_mae: 16.2021\n",
      "Epoch 2449/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5085 - mae: 16.5085 - val_loss: 16.1982 - val_mae: 16.1982\n",
      "Epoch 2450/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5084 - mae: 16.5084 - val_loss: 16.2342 - val_mae: 16.2342\n",
      "Epoch 2451/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5065 - mae: 16.5065 - val_loss: 16.1996 - val_mae: 16.1996\n",
      "Epoch 2452/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5009 - mae: 16.5009 - val_loss: 16.2464 - val_mae: 16.2464\n",
      "Epoch 2453/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4991 - mae: 16.4991 - val_loss: 16.1979 - val_mae: 16.1979\n",
      "Epoch 2454/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5012 - mae: 16.5012 - val_loss: 16.1940 - val_mae: 16.1940\n",
      "Epoch 2455/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5035 - mae: 16.5035 - val_loss: 16.2044 - val_mae: 16.2044\n",
      "Epoch 2456/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4989 - mae: 16.4989 - val_loss: 16.2326 - val_mae: 16.2326\n",
      "Epoch 2457/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5057 - mae: 16.5057 - val_loss: 16.2154 - val_mae: 16.2154\n",
      "Epoch 2458/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4939 - mae: 16.4939 - val_loss: 16.2241 - val_mae: 16.2240\n",
      "Epoch 2459/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5043 - mae: 16.5043 - val_loss: 16.2196 - val_mae: 16.2196\n",
      "Epoch 2460/3000\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 16.5036 - mae: 16.5036 - val_loss: 16.2180 - val_mae: 16.2180\n",
      "Epoch 2461/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5044 - mae: 16.5044 - val_loss: 16.1919 - val_mae: 16.1919\n",
      "Epoch 2462/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5047 - mae: 16.5047 - val_loss: 16.1947 - val_mae: 16.1947\n",
      "Epoch 2463/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5031 - mae: 16.5031 - val_loss: 16.1997 - val_mae: 16.1997\n",
      "Epoch 2464/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4984 - mae: 16.4984 - val_loss: 16.1982 - val_mae: 16.1982\n",
      "Epoch 2465/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5023 - mae: 16.5023 - val_loss: 16.2027 - val_mae: 16.2027\n",
      "Epoch 2466/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5020 - mae: 16.5020 - val_loss: 16.2004 - val_mae: 16.2004\n",
      "Epoch 2467/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5007 - mae: 16.5007 - val_loss: 16.2204 - val_mae: 16.2204\n",
      "Epoch 2468/3000\n",
      "10496/10496 [==============================] - 1s 54us/sample - loss: 16.5037 - mae: 16.5037 - val_loss: 16.2014 - val_mae: 16.2014\n",
      "Epoch 2469/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.5041 - mae: 16.5041 - val_loss: 16.2200 - val_mae: 16.2200\n",
      "Epoch 2470/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5062 - mae: 16.5062 - val_loss: 16.2006 - val_mae: 16.2006\n",
      "Epoch 2471/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5059 - mae: 16.5059 - val_loss: 16.1954 - val_mae: 16.1954\n",
      "Epoch 2472/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5000 - mae: 16.5000 - val_loss: 16.1924 - val_mae: 16.1924\n",
      "Epoch 2473/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5020 - mae: 16.5020 - val_loss: 16.2124 - val_mae: 16.2124\n",
      "Epoch 2474/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5028 - mae: 16.5028 - val_loss: 16.2017 - val_mae: 16.2017\n",
      "Epoch 2475/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5012 - mae: 16.5012 - val_loss: 16.2215 - val_mae: 16.2215\n",
      "Epoch 2476/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5002 - mae: 16.5002 - val_loss: 16.2923 - val_mae: 16.2923\n",
      "Epoch 2477/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5054 - mae: 16.5054 - val_loss: 16.2027 - val_mae: 16.2027\n",
      "Epoch 2478/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5007 - mae: 16.5007 - val_loss: 16.2166 - val_mae: 16.2166\n",
      "Epoch 2479/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4971 - mae: 16.4972 - val_loss: 16.3086 - val_mae: 16.3086\n",
      "Epoch 2480/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5046 - mae: 16.5046 - val_loss: 16.1979 - val_mae: 16.1979\n",
      "Epoch 2481/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5021 - mae: 16.5021 - val_loss: 16.2040 - val_mae: 16.2040\n",
      "Epoch 2482/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5014 - mae: 16.5014 - val_loss: 16.2011 - val_mae: 16.2011\n",
      "Epoch 2483/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.5039 - mae: 16.5039 - val_loss: 16.2196 - val_mae: 16.2196\n",
      "Epoch 2484/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5028 - mae: 16.5028 - val_loss: 16.2022 - val_mae: 16.2022\n",
      "Epoch 2485/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4999 - mae: 16.4999 - val_loss: 16.2002 - val_mae: 16.2001\n",
      "Epoch 2486/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.4997 - mae: 16.4997 - val_loss: 16.1977 - val_mae: 16.1977\n",
      "Epoch 2487/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4917 - mae: 16.4917 - val_loss: 16.2694 - val_mae: 16.2694\n",
      "Epoch 2488/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.5009 - mae: 16.5009 - val_loss: 16.1936 - val_mae: 16.1936\n",
      "Epoch 2489/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4899 - mae: 16.4899 - val_loss: 16.1964 - val_mae: 16.1964\n",
      "Epoch 2490/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4885 - mae: 16.4885 - val_loss: 16.2380 - val_mae: 16.2380\n",
      "Epoch 2491/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.5071 - mae: 16.5071 - val_loss: 16.2004 - val_mae: 16.2004\n",
      "Epoch 2492/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.5042 - mae: 16.5042 - val_loss: 16.2254 - val_mae: 16.2254\n",
      "Epoch 2493/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.5002 - mae: 16.5002 - val_loss: 16.1969 - val_mae: 16.1969\n",
      "Epoch 2494/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4983 - mae: 16.4983 - val_loss: 16.1995 - val_mae: 16.1995\n",
      "Epoch 2495/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4946 - mae: 16.4946 - val_loss: 16.2012 - val_mae: 16.2013\n",
      "Epoch 2496/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4998 - mae: 16.4998 - val_loss: 16.2261 - val_mae: 16.2261\n",
      "Epoch 2497/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4902 - mae: 16.4902 - val_loss: 16.1946 - val_mae: 16.1946\n",
      "Epoch 2498/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5037 - mae: 16.5037 - val_loss: 16.2011 - val_mae: 16.2011\n",
      "Epoch 2499/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5031 - mae: 16.5031 - val_loss: 16.2031 - val_mae: 16.2031\n",
      "Epoch 2500/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4991 - mae: 16.4991 - val_loss: 16.2115 - val_mae: 16.2115\n",
      "Epoch 2501/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.4994 - mae: 16.4994 - val_loss: 16.1945 - val_mae: 16.1945\n",
      "Epoch 2502/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5016 - mae: 16.5016 - val_loss: 16.1930 - val_mae: 16.1930\n",
      "Epoch 2503/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4945 - mae: 16.4945 - val_loss: 16.2257 - val_mae: 16.2257\n",
      "Epoch 2504/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4971 - mae: 16.4971 - val_loss: 16.1968 - val_mae: 16.1968\n",
      "Epoch 2505/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4960 - mae: 16.4960 - val_loss: 16.1974 - val_mae: 16.1974\n",
      "Epoch 2506/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5010 - mae: 16.5010 - val_loss: 16.2673 - val_mae: 16.2673\n",
      "Epoch 2507/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5019 - mae: 16.5019 - val_loss: 16.2275 - val_mae: 16.2275\n",
      "Epoch 2508/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.4945 - mae: 16.4945 - val_loss: 16.2788 - val_mae: 16.2788\n",
      "Epoch 2509/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5013 - mae: 16.5013 - val_loss: 16.2279 - val_mae: 16.2279\n",
      "Epoch 2510/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4956 - mae: 16.4956 - val_loss: 16.2359 - val_mae: 16.2359\n",
      "Epoch 2511/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.5007 - mae: 16.5007 - val_loss: 16.2217 - val_mae: 16.2217\n",
      "Epoch 2512/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.4990 - mae: 16.4990 - val_loss: 16.1967 - val_mae: 16.1967\n",
      "Epoch 2513/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.4999 - mae: 16.4999 - val_loss: 16.2524 - val_mae: 16.2524\n",
      "Epoch 2514/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4976 - mae: 16.4976 - val_loss: 16.2365 - val_mae: 16.2365\n",
      "Epoch 2515/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4950 - mae: 16.4950 - val_loss: 16.2272 - val_mae: 16.2272\n",
      "Epoch 2516/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5016 - mae: 16.5016 - val_loss: 16.1968 - val_mae: 16.1968\n",
      "Epoch 2517/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4983 - mae: 16.4983 - val_loss: 16.2274 - val_mae: 16.2274\n",
      "Epoch 2518/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4961 - mae: 16.4961 - val_loss: 16.2467 - val_mae: 16.2467\n",
      "Epoch 2519/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4966 - mae: 16.4966 - val_loss: 16.2276 - val_mae: 16.2276\n",
      "Epoch 2520/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4961 - mae: 16.4961 - val_loss: 16.1991 - val_mae: 16.1991\n",
      "Epoch 2521/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4976 - mae: 16.4976 - val_loss: 16.2547 - val_mae: 16.2547\n",
      "Epoch 2522/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.5006 - mae: 16.5006 - val_loss: 16.2191 - val_mae: 16.2191\n",
      "Epoch 2523/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4965 - mae: 16.4965 - val_loss: 16.1918 - val_mae: 16.1918\n",
      "Epoch 2524/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4943 - mae: 16.4944 - val_loss: 16.2307 - val_mae: 16.2307\n",
      "Epoch 2525/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4969 - mae: 16.4969 - val_loss: 16.2063 - val_mae: 16.2063\n",
      "Epoch 2526/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4936 - mae: 16.4936 - val_loss: 16.1973 - val_mae: 16.1972\n",
      "Epoch 2527/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4974 - mae: 16.4974 - val_loss: 16.2404 - val_mae: 16.2404\n",
      "Epoch 2528/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4913 - mae: 16.4913 - val_loss: 16.1963 - val_mae: 16.1963\n",
      "Epoch 2529/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4965 - mae: 16.4965 - val_loss: 16.1951 - val_mae: 16.1951\n",
      "Epoch 2530/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4954 - mae: 16.4954 - val_loss: 16.2744 - val_mae: 16.2744\n",
      "Epoch 2531/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4953 - mae: 16.4953 - val_loss: 16.2149 - val_mae: 16.2149\n",
      "Epoch 2532/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4923 - mae: 16.4923 - val_loss: 16.2049 - val_mae: 16.2049\n",
      "Epoch 2533/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4872 - mae: 16.4872 - val_loss: 16.2679 - val_mae: 16.2679\n",
      "Epoch 2534/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4968 - mae: 16.4968 - val_loss: 16.2322 - val_mae: 16.2322\n",
      "Epoch 2535/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4948 - mae: 16.4948 - val_loss: 16.1956 - val_mae: 16.1956\n",
      "Epoch 2536/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4935 - mae: 16.4935 - val_loss: 16.2048 - val_mae: 16.2048\n",
      "Epoch 2537/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4961 - mae: 16.4961 - val_loss: 16.2077 - val_mae: 16.2077\n",
      "Epoch 2538/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4958 - mae: 16.4958 - val_loss: 16.2611 - val_mae: 16.2611\n",
      "Epoch 2539/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.5005 - mae: 16.5005 - val_loss: 16.2051 - val_mae: 16.2051\n",
      "Epoch 2540/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4963 - mae: 16.4963 - val_loss: 16.2037 - val_mae: 16.2037\n",
      "Epoch 2541/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.4986 - mae: 16.4986 - val_loss: 16.1905 - val_mae: 16.1905\n",
      "Epoch 2542/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4969 - mae: 16.4969 - val_loss: 16.2056 - val_mae: 16.2056\n",
      "Epoch 2543/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4941 - mae: 16.4941 - val_loss: 16.2101 - val_mae: 16.2101\n",
      "Epoch 2544/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4930 - mae: 16.4930 - val_loss: 16.2015 - val_mae: 16.2015\n",
      "Epoch 2545/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.5003 - mae: 16.5003 - val_loss: 16.1928 - val_mae: 16.1928\n",
      "Epoch 2546/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4999 - mae: 16.4999 - val_loss: 16.2027 - val_mae: 16.2027\n",
      "Epoch 2547/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4937 - mae: 16.4937 - val_loss: 16.2348 - val_mae: 16.2348\n",
      "Epoch 2548/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4991 - mae: 16.4991 - val_loss: 16.2133 - val_mae: 16.2133\n",
      "Epoch 2549/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4907 - mae: 16.4907 - val_loss: 16.2241 - val_mae: 16.2241\n",
      "Epoch 2550/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4932 - mae: 16.4932 - val_loss: 16.2123 - val_mae: 16.2123\n",
      "Epoch 2551/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4986 - mae: 16.4986 - val_loss: 16.2051 - val_mae: 16.2051\n",
      "Epoch 2552/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4988 - mae: 16.4988 - val_loss: 16.1997 - val_mae: 16.1997\n",
      "Epoch 2553/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4992 - mae: 16.4992 - val_loss: 16.2248 - val_mae: 16.2248\n",
      "Epoch 2554/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4924 - mae: 16.4924 - val_loss: 16.2135 - val_mae: 16.2136\n",
      "Epoch 2555/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4939 - mae: 16.4939 - val_loss: 16.2208 - val_mae: 16.2208\n",
      "Epoch 2556/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4881 - mae: 16.4881 - val_loss: 16.1909 - val_mae: 16.1909\n",
      "Epoch 2557/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4940 - mae: 16.4940 - val_loss: 16.1922 - val_mae: 16.1922\n",
      "Epoch 2558/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4928 - mae: 16.4928 - val_loss: 16.1907 - val_mae: 16.1906\n",
      "Epoch 2559/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4964 - mae: 16.4965 - val_loss: 16.1925 - val_mae: 16.1925\n",
      "Epoch 2560/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4951 - mae: 16.4951 - val_loss: 16.1953 - val_mae: 16.1953\n",
      "Epoch 2561/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4886 - mae: 16.4886 - val_loss: 16.2196 - val_mae: 16.2196\n",
      "Epoch 2562/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.4981 - mae: 16.4981 - val_loss: 16.1995 - val_mae: 16.1995\n",
      "Epoch 2563/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.4937 - mae: 16.4937 - val_loss: 16.2065 - val_mae: 16.2065\n",
      "Epoch 2564/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4913 - mae: 16.4913 - val_loss: 16.1990 - val_mae: 16.1990\n",
      "Epoch 2565/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.4958 - mae: 16.4958 - val_loss: 16.1909 - val_mae: 16.1909\n",
      "Epoch 2566/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4947 - mae: 16.4947 - val_loss: 16.1876 - val_mae: 16.1876\n",
      "Epoch 2567/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4939 - mae: 16.4939 - val_loss: 16.2037 - val_mae: 16.2037\n",
      "Epoch 2568/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4874 - mae: 16.4874 - val_loss: 16.1992 - val_mae: 16.1992\n",
      "Epoch 2569/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4960 - mae: 16.4960 - val_loss: 16.1991 - val_mae: 16.1992\n",
      "Epoch 2570/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4942 - mae: 16.4942 - val_loss: 16.2026 - val_mae: 16.2026\n",
      "Epoch 2571/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4895 - mae: 16.4895 - val_loss: 16.1884 - val_mae: 16.1884\n",
      "Epoch 2572/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4930 - mae: 16.4930 - val_loss: 16.1947 - val_mae: 16.1947\n",
      "Epoch 2573/3000\n",
      "10496/10496 [==============================] - 1s 52us/sample - loss: 16.4950 - mae: 16.4950 - val_loss: 16.2043 - val_mae: 16.2043\n",
      "Epoch 2574/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.4925 - mae: 16.4925 - val_loss: 16.2265 - val_mae: 16.2265\n",
      "Epoch 2575/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4904 - mae: 16.4904 - val_loss: 16.1919 - val_mae: 16.1919\n",
      "Epoch 2576/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4961 - mae: 16.4961 - val_loss: 16.1984 - val_mae: 16.1984\n",
      "Epoch 2577/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4926 - mae: 16.4926 - val_loss: 16.2186 - val_mae: 16.2186\n",
      "Epoch 2578/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.4864 - mae: 16.4864 - val_loss: 16.2360 - val_mae: 16.2360\n",
      "Epoch 2579/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4949 - mae: 16.4949 - val_loss: 16.2108 - val_mae: 16.2108\n",
      "Epoch 2580/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4938 - mae: 16.4938 - val_loss: 16.2065 - val_mae: 16.2065\n",
      "Epoch 2581/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4915 - mae: 16.4915 - val_loss: 16.1939 - val_mae: 16.1939\n",
      "Epoch 2582/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4901 - mae: 16.4901 - val_loss: 16.2274 - val_mae: 16.2274\n",
      "Epoch 2583/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4956 - mae: 16.4956 - val_loss: 16.2034 - val_mae: 16.2034\n",
      "Epoch 2584/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4914 - mae: 16.4914 - val_loss: 16.2004 - val_mae: 16.2004\n",
      "Epoch 2585/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4910 - mae: 16.4910 - val_loss: 16.1891 - val_mae: 16.1891\n",
      "Epoch 2586/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4885 - mae: 16.4885 - val_loss: 16.2004 - val_mae: 16.2004\n",
      "Epoch 2587/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4966 - mae: 16.4966 - val_loss: 16.1924 - val_mae: 16.1924\n",
      "Epoch 2588/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4865 - mae: 16.4865 - val_loss: 16.1944 - val_mae: 16.1944\n",
      "Epoch 2589/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4884 - mae: 16.4884 - val_loss: 16.1990 - val_mae: 16.1990\n",
      "Epoch 2590/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4936 - mae: 16.4936 - val_loss: 16.2121 - val_mae: 16.2121\n",
      "Epoch 2591/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4923 - mae: 16.4923 - val_loss: 16.1930 - val_mae: 16.1930\n",
      "Epoch 2592/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4904 - mae: 16.4904 - val_loss: 16.1910 - val_mae: 16.1910\n",
      "Epoch 2593/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4900 - mae: 16.4900 - val_loss: 16.2366 - val_mae: 16.2366\n",
      "Epoch 2594/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4905 - mae: 16.4905 - val_loss: 16.2747 - val_mae: 16.2747\n",
      "Epoch 2595/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4894 - mae: 16.4894 - val_loss: 16.2459 - val_mae: 16.2459\n",
      "Epoch 2596/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4857 - mae: 16.4857 - val_loss: 16.2093 - val_mae: 16.2093\n",
      "Epoch 2597/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4899 - mae: 16.4899 - val_loss: 16.2180 - val_mae: 16.2180\n",
      "Epoch 2598/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4894 - mae: 16.4894 - val_loss: 16.1994 - val_mae: 16.1994\n",
      "Epoch 2599/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4902 - mae: 16.4902 - val_loss: 16.2093 - val_mae: 16.2093\n",
      "Epoch 2600/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4868 - mae: 16.4868 - val_loss: 16.1921 - val_mae: 16.1921\n",
      "Epoch 2601/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4887 - mae: 16.4887 - val_loss: 16.1871 - val_mae: 16.1871\n",
      "Epoch 2602/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4917 - mae: 16.4917 - val_loss: 16.2161 - val_mae: 16.2161\n",
      "Epoch 2603/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4933 - mae: 16.4933 - val_loss: 16.1967 - val_mae: 16.1967\n",
      "Epoch 2604/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4897 - mae: 16.4897 - val_loss: 16.1903 - val_mae: 16.1903\n",
      "Epoch 2605/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4979 - mae: 16.4979 - val_loss: 16.2183 - val_mae: 16.2183\n",
      "Epoch 2606/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4933 - mae: 16.4933 - val_loss: 16.2189 - val_mae: 16.2189\n",
      "Epoch 2607/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4923 - mae: 16.4923 - val_loss: 16.1877 - val_mae: 16.1877\n",
      "Epoch 2608/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4895 - mae: 16.4895 - val_loss: 16.2456 - val_mae: 16.2456\n",
      "Epoch 2609/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4887 - mae: 16.4887 - val_loss: 16.2203 - val_mae: 16.2203\n",
      "Epoch 2610/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4820 - mae: 16.4820 - val_loss: 16.2993 - val_mae: 16.2993\n",
      "Epoch 2611/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4893 - mae: 16.4893 - val_loss: 16.2000 - val_mae: 16.2000\n",
      "Epoch 2612/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4849 - mae: 16.4849 - val_loss: 16.2140 - val_mae: 16.2140\n",
      "Epoch 2613/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4846 - mae: 16.4846 - val_loss: 16.2547 - val_mae: 16.2547\n",
      "Epoch 2614/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4908 - mae: 16.4908 - val_loss: 16.2043 - val_mae: 16.2043\n",
      "Epoch 2615/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4897 - mae: 16.4897 - val_loss: 16.2504 - val_mae: 16.2504\n",
      "Epoch 2616/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4909 - mae: 16.4910 - val_loss: 16.2101 - val_mae: 16.2101\n",
      "Epoch 2617/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4904 - mae: 16.4904 - val_loss: 16.2008 - val_mae: 16.2008\n",
      "Epoch 2618/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4832 - mae: 16.4832 - val_loss: 16.2191 - val_mae: 16.2191\n",
      "Epoch 2619/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4927 - mae: 16.4927 - val_loss: 16.2112 - val_mae: 16.2112\n",
      "Epoch 2620/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.4881 - mae: 16.4881 - val_loss: 16.2130 - val_mae: 16.2130\n",
      "Epoch 2621/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4825 - mae: 16.4825 - val_loss: 16.1890 - val_mae: 16.1890\n",
      "Epoch 2622/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4916 - mae: 16.4916 - val_loss: 16.1973 - val_mae: 16.1973\n",
      "Epoch 2623/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4899 - mae: 16.4899 - val_loss: 16.1925 - val_mae: 16.1925\n",
      "Epoch 2624/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4849 - mae: 16.4849 - val_loss: 16.2179 - val_mae: 16.2179\n",
      "Epoch 2625/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4859 - mae: 16.4859 - val_loss: 16.1871 - val_mae: 16.1871\n",
      "Epoch 2626/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4884 - mae: 16.4884 - val_loss: 16.1865 - val_mae: 16.1865\n",
      "Epoch 2627/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4760 - mae: 16.4760 - val_loss: 16.2011 - val_mae: 16.2011\n",
      "Epoch 2628/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4739 - mae: 16.4739 - val_loss: 16.2045 - val_mae: 16.2045\n",
      "Epoch 2629/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4822 - mae: 16.4822 - val_loss: 16.1848 - val_mae: 16.1848\n",
      "Epoch 2630/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4886 - mae: 16.4886 - val_loss: 16.1900 - val_mae: 16.1900\n",
      "Epoch 2631/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4892 - mae: 16.4892 - val_loss: 16.1868 - val_mae: 16.1868\n",
      "Epoch 2632/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4894 - mae: 16.4894 - val_loss: 16.1903 - val_mae: 16.1903\n",
      "Epoch 2633/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4907 - mae: 16.4907 - val_loss: 16.2135 - val_mae: 16.2135\n",
      "Epoch 2634/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4879 - mae: 16.4879 - val_loss: 16.2344 - val_mae: 16.2344\n",
      "Epoch 2635/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4854 - mae: 16.4854 - val_loss: 16.1935 - val_mae: 16.1935\n",
      "Epoch 2636/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4797 - mae: 16.4797 - val_loss: 16.2791 - val_mae: 16.2791\n",
      "Epoch 2637/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4870 - mae: 16.4870 - val_loss: 16.2055 - val_mae: 16.2055\n",
      "Epoch 2638/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4844 - mae: 16.4844 - val_loss: 16.2075 - val_mae: 16.2075\n",
      "Epoch 2639/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4835 - mae: 16.4835 - val_loss: 16.2329 - val_mae: 16.2329\n",
      "Epoch 2640/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4880 - mae: 16.4880 - val_loss: 16.1907 - val_mae: 16.1907\n",
      "Epoch 2641/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4920 - mae: 16.4920 - val_loss: 16.1883 - val_mae: 16.1883\n",
      "Epoch 2642/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4854 - mae: 16.4854 - val_loss: 16.2144 - val_mae: 16.2143\n",
      "Epoch 2643/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4881 - mae: 16.4881 - val_loss: 16.2215 - val_mae: 16.2215\n",
      "Epoch 2644/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4885 - mae: 16.4885 - val_loss: 16.1878 - val_mae: 16.1878\n",
      "Epoch 2645/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4838 - mae: 16.4838 - val_loss: 16.1934 - val_mae: 16.1934\n",
      "Epoch 2646/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4856 - mae: 16.4856 - val_loss: 16.2124 - val_mae: 16.2124\n",
      "Epoch 2647/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4834 - mae: 16.4834 - val_loss: 16.2122 - val_mae: 16.2122\n",
      "Epoch 2648/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4867 - mae: 16.4867 - val_loss: 16.2196 - val_mae: 16.2196\n",
      "Epoch 2649/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4856 - mae: 16.4856 - val_loss: 16.2271 - val_mae: 16.2271\n",
      "Epoch 2650/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4861 - mae: 16.4861 - val_loss: 16.1970 - val_mae: 16.1970\n",
      "Epoch 2651/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4842 - mae: 16.4842 - val_loss: 16.1909 - val_mae: 16.1909\n",
      "Epoch 2652/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4873 - mae: 16.4872 - val_loss: 16.1886 - val_mae: 16.1886\n",
      "Epoch 2653/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4859 - mae: 16.4859 - val_loss: 16.1909 - val_mae: 16.1909\n",
      "Epoch 2654/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4838 - mae: 16.4838 - val_loss: 16.1897 - val_mae: 16.1897\n",
      "Epoch 2655/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4910 - mae: 16.4910 - val_loss: 16.2051 - val_mae: 16.2051\n",
      "Epoch 2656/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4865 - mae: 16.4865 - val_loss: 16.2105 - val_mae: 16.2105\n",
      "Epoch 2657/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4859 - mae: 16.4859 - val_loss: 16.1953 - val_mae: 16.1953\n",
      "Epoch 2658/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4827 - mae: 16.4827 - val_loss: 16.2091 - val_mae: 16.2091\n",
      "Epoch 2659/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4766 - mae: 16.4766 - val_loss: 16.2982 - val_mae: 16.2982\n",
      "Epoch 2660/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4817 - mae: 16.4817 - val_loss: 16.2219 - val_mae: 16.2219\n",
      "Epoch 2661/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4864 - mae: 16.4864 - val_loss: 16.2042 - val_mae: 16.2042\n",
      "Epoch 2662/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4766 - mae: 16.4766 - val_loss: 16.1909 - val_mae: 16.1909\n",
      "Epoch 2663/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4848 - mae: 16.4848 - val_loss: 16.1865 - val_mae: 16.1865\n",
      "Epoch 2664/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4779 - mae: 16.4779 - val_loss: 16.2276 - val_mae: 16.2276\n",
      "Epoch 2665/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4833 - mae: 16.4833 - val_loss: 16.1890 - val_mae: 16.1890\n",
      "Epoch 2666/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4825 - mae: 16.4825 - val_loss: 16.1924 - val_mae: 16.1924\n",
      "Epoch 2667/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4847 - mae: 16.4847 - val_loss: 16.2088 - val_mae: 16.2088\n",
      "Epoch 2668/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4866 - mae: 16.4866 - val_loss: 16.1932 - val_mae: 16.1932\n",
      "Epoch 2669/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4831 - mae: 16.4831 - val_loss: 16.1899 - val_mae: 16.1899\n",
      "Epoch 2670/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4789 - mae: 16.4789 - val_loss: 16.2471 - val_mae: 16.2471\n",
      "Epoch 2671/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4871 - mae: 16.4871 - val_loss: 16.1908 - val_mae: 16.1908\n",
      "Epoch 2672/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4847 - mae: 16.4847 - val_loss: 16.1860 - val_mae: 16.1859\n",
      "Epoch 2673/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4862 - mae: 16.4862 - val_loss: 16.1952 - val_mae: 16.1952\n",
      "Epoch 2674/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4878 - mae: 16.4878 - val_loss: 16.1851 - val_mae: 16.1851\n",
      "Epoch 2675/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4845 - mae: 16.4845 - val_loss: 16.2149 - val_mae: 16.2149\n",
      "Epoch 2676/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4828 - mae: 16.4828 - val_loss: 16.1951 - val_mae: 16.1951\n",
      "Epoch 2677/3000\n",
      "10496/10496 [==============================] - 1s 69us/sample - loss: 16.4816 - mae: 16.4816 - val_loss: 16.1862 - val_mae: 16.1862\n",
      "Epoch 2678/3000\n",
      "10496/10496 [==============================] - 1s 78us/sample - loss: 16.4819 - mae: 16.4820 - val_loss: 16.1966 - val_mae: 16.1966\n",
      "Epoch 2679/3000\n",
      "10496/10496 [==============================] - 1s 69us/sample - loss: 16.4838 - mae: 16.4838 - val_loss: 16.1860 - val_mae: 16.1860\n",
      "Epoch 2680/3000\n",
      "10496/10496 [==============================] - 1s 58us/sample - loss: 16.4854 - mae: 16.4854 - val_loss: 16.2256 - val_mae: 16.2256\n",
      "Epoch 2681/3000\n",
      "10496/10496 [==============================] - 1s 57us/sample - loss: 16.4795 - mae: 16.4795 - val_loss: 16.2013 - val_mae: 16.2013\n",
      "Epoch 2682/3000\n",
      "10496/10496 [==============================] - 1s 53us/sample - loss: 16.4857 - mae: 16.4858 - val_loss: 16.1940 - val_mae: 16.1940\n",
      "Epoch 2683/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4802 - mae: 16.4802 - val_loss: 16.1951 - val_mae: 16.1951\n",
      "Epoch 2684/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4804 - mae: 16.4804 - val_loss: 16.1841 - val_mae: 16.1841\n",
      "Epoch 2685/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4820 - mae: 16.4820 - val_loss: 16.1863 - val_mae: 16.1863\n",
      "Epoch 2686/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4796 - mae: 16.4796 - val_loss: 16.2079 - val_mae: 16.2079\n",
      "Epoch 2687/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4811 - mae: 16.4811 - val_loss: 16.1882 - val_mae: 16.1882\n",
      "Epoch 2688/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4867 - mae: 16.4867 - val_loss: 16.2077 - val_mae: 16.2077\n",
      "Epoch 2689/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4817 - mae: 16.4817 - val_loss: 16.1930 - val_mae: 16.1930\n",
      "Epoch 2690/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4804 - mae: 16.4804 - val_loss: 16.2061 - val_mae: 16.2061\n",
      "Epoch 2691/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4825 - mae: 16.4825 - val_loss: 16.1921 - val_mae: 16.1921\n",
      "Epoch 2692/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4853 - mae: 16.4853 - val_loss: 16.1870 - val_mae: 16.1869\n",
      "Epoch 2693/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4805 - mae: 16.4805 - val_loss: 16.1892 - val_mae: 16.1892\n",
      "Epoch 2694/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.4831 - mae: 16.4831 - val_loss: 16.1858 - val_mae: 16.1858\n",
      "Epoch 2695/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4813 - mae: 16.4813 - val_loss: 16.1863 - val_mae: 16.1863\n",
      "Epoch 2696/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4834 - mae: 16.4834 - val_loss: 16.1921 - val_mae: 16.1921\n",
      "Epoch 2697/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.4794 - mae: 16.4794 - val_loss: 16.1870 - val_mae: 16.1870\n",
      "Epoch 2698/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4823 - mae: 16.4823 - val_loss: 16.1987 - val_mae: 16.1987\n",
      "Epoch 2699/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4846 - mae: 16.4846 - val_loss: 16.1925 - val_mae: 16.1925\n",
      "Epoch 2700/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.4854 - mae: 16.4854 - val_loss: 16.2165 - val_mae: 16.2165\n",
      "Epoch 2701/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4759 - mae: 16.4759 - val_loss: 16.2251 - val_mae: 16.2251\n",
      "Epoch 2702/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4819 - mae: 16.4819 - val_loss: 16.1917 - val_mae: 16.1917\n",
      "Epoch 2703/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4833 - mae: 16.4833 - val_loss: 16.1960 - val_mae: 16.1960\n",
      "Epoch 2704/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4800 - mae: 16.4800 - val_loss: 16.1942 - val_mae: 16.1942\n",
      "Epoch 2705/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4812 - mae: 16.4812 - val_loss: 16.1874 - val_mae: 16.1874\n",
      "Epoch 2706/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4842 - mae: 16.4842 - val_loss: 16.2121 - val_mae: 16.2121\n",
      "Epoch 2707/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4829 - mae: 16.4829 - val_loss: 16.1859 - val_mae: 16.1859\n",
      "Epoch 2708/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4801 - mae: 16.4801 - val_loss: 16.1842 - val_mae: 16.1842\n",
      "Epoch 2709/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4820 - mae: 16.4820 - val_loss: 16.2059 - val_mae: 16.2059\n",
      "Epoch 2710/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4812 - mae: 16.4812 - val_loss: 16.1913 - val_mae: 16.1913\n",
      "Epoch 2711/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4802 - mae: 16.4802 - val_loss: 16.2158 - val_mae: 16.2158\n",
      "Epoch 2712/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4841 - mae: 16.4841 - val_loss: 16.2118 - val_mae: 16.2118\n",
      "Epoch 2713/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4804 - mae: 16.4804 - val_loss: 16.2160 - val_mae: 16.2160\n",
      "Epoch 2714/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4760 - mae: 16.4760 - val_loss: 16.1867 - val_mae: 16.1867\n",
      "Epoch 2715/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4736 - mae: 16.4736 - val_loss: 16.2015 - val_mae: 16.2015\n",
      "Epoch 2716/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.4865 - mae: 16.4865 - val_loss: 16.1900 - val_mae: 16.1900\n",
      "Epoch 2717/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4811 - mae: 16.4811 - val_loss: 16.2029 - val_mae: 16.2029\n",
      "Epoch 2718/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.4800 - mae: 16.4800 - val_loss: 16.2086 - val_mae: 16.2086\n",
      "Epoch 2719/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4769 - mae: 16.4769 - val_loss: 16.1959 - val_mae: 16.1959\n",
      "Epoch 2720/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4803 - mae: 16.4803 - val_loss: 16.2019 - val_mae: 16.2019\n",
      "Epoch 2721/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4726 - mae: 16.4726 - val_loss: 16.1953 - val_mae: 16.1953\n",
      "Epoch 2722/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4801 - mae: 16.4801 - val_loss: 16.2011 - val_mae: 16.2011\n",
      "Epoch 2723/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4849 - mae: 16.4850 - val_loss: 16.2038 - val_mae: 16.2038\n",
      "Epoch 2724/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4665 - mae: 16.4665 - val_loss: 16.2432 - val_mae: 16.2432\n",
      "Epoch 2725/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4811 - mae: 16.4811 - val_loss: 16.2173 - val_mae: 16.2173\n",
      "Epoch 2726/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4814 - mae: 16.4814 - val_loss: 16.1817 - val_mae: 16.1817\n",
      "Epoch 2727/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4804 - mae: 16.4804 - val_loss: 16.1888 - val_mae: 16.1888\n",
      "Epoch 2728/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4758 - mae: 16.4758 - val_loss: 16.1840 - val_mae: 16.1839\n",
      "Epoch 2729/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4794 - mae: 16.4794 - val_loss: 16.2220 - val_mae: 16.2220\n",
      "Epoch 2730/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4821 - mae: 16.4821 - val_loss: 16.1851 - val_mae: 16.1851\n",
      "Epoch 2731/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4783 - mae: 16.4783 - val_loss: 16.2014 - val_mae: 16.2014\n",
      "Epoch 2732/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4828 - mae: 16.4828 - val_loss: 16.1938 - val_mae: 16.1938\n",
      "Epoch 2733/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4784 - mae: 16.4784 - val_loss: 16.1883 - val_mae: 16.1883\n",
      "Epoch 2734/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4729 - mae: 16.4729 - val_loss: 16.2077 - val_mae: 16.2077\n",
      "Epoch 2735/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4812 - mae: 16.4812 - val_loss: 16.2098 - val_mae: 16.2098\n",
      "Epoch 2736/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4788 - mae: 16.4788 - val_loss: 16.2071 - val_mae: 16.2071\n",
      "Epoch 2737/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.4730 - mae: 16.4730 - val_loss: 16.2029 - val_mae: 16.2029\n",
      "Epoch 2738/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4783 - mae: 16.4783 - val_loss: 16.1864 - val_mae: 16.1864\n",
      "Epoch 2739/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4819 - mae: 16.4819 - val_loss: 16.1897 - val_mae: 16.1897\n",
      "Epoch 2740/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4763 - mae: 16.4763 - val_loss: 16.1928 - val_mae: 16.1928\n",
      "Epoch 2741/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4840 - mae: 16.4840 - val_loss: 16.1863 - val_mae: 16.1863\n",
      "Epoch 2742/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4727 - mae: 16.4727 - val_loss: 16.2036 - val_mae: 16.2036\n",
      "Epoch 2743/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4796 - mae: 16.4796 - val_loss: 16.2127 - val_mae: 16.2127\n",
      "Epoch 2744/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4812 - mae: 16.4812 - val_loss: 16.1871 - val_mae: 16.1871\n",
      "Epoch 2745/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4765 - mae: 16.4766 - val_loss: 16.1900 - val_mae: 16.1900\n",
      "Epoch 2746/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.4741 - mae: 16.4741 - val_loss: 16.1970 - val_mae: 16.1970\n",
      "Epoch 2747/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4725 - mae: 16.4725 - val_loss: 16.2645 - val_mae: 16.2645\n",
      "Epoch 2748/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4816 - mae: 16.4816 - val_loss: 16.1933 - val_mae: 16.1933\n",
      "Epoch 2749/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4756 - mae: 16.4756 - val_loss: 16.1852 - val_mae: 16.1852\n",
      "Epoch 2750/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4660 - mae: 16.4660 - val_loss: 16.2339 - val_mae: 16.2339\n",
      "Epoch 2751/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4775 - mae: 16.4775 - val_loss: 16.1884 - val_mae: 16.1884\n",
      "Epoch 2752/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4812 - mae: 16.4812 - val_loss: 16.1864 - val_mae: 16.1864\n",
      "Epoch 2753/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4816 - mae: 16.4816 - val_loss: 16.1850 - val_mae: 16.1850\n",
      "Epoch 2754/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4792 - mae: 16.4792 - val_loss: 16.1957 - val_mae: 16.1957\n",
      "Epoch 2755/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4744 - mae: 16.4744 - val_loss: 16.2125 - val_mae: 16.2125\n",
      "Epoch 2756/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4769 - mae: 16.4769 - val_loss: 16.2003 - val_mae: 16.2003\n",
      "Epoch 2757/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4751 - mae: 16.4751 - val_loss: 16.1912 - val_mae: 16.1912\n",
      "Epoch 2758/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4743 - mae: 16.4743 - val_loss: 16.1996 - val_mae: 16.1996\n",
      "Epoch 2759/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4767 - mae: 16.4767 - val_loss: 16.1870 - val_mae: 16.1870\n",
      "Epoch 2760/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4787 - mae: 16.4787 - val_loss: 16.2751 - val_mae: 16.2751\n",
      "Epoch 2761/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4725 - mae: 16.4725 - val_loss: 16.1982 - val_mae: 16.1982\n",
      "Epoch 2762/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4768 - mae: 16.4768 - val_loss: 16.1980 - val_mae: 16.1980\n",
      "Epoch 2763/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4781 - mae: 16.4781 - val_loss: 16.1852 - val_mae: 16.1852\n",
      "Epoch 2764/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4757 - mae: 16.4757 - val_loss: 16.2024 - val_mae: 16.2024\n",
      "Epoch 2765/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4774 - mae: 16.4774 - val_loss: 16.1956 - val_mae: 16.1956\n",
      "Epoch 2766/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4790 - mae: 16.4790 - val_loss: 16.1909 - val_mae: 16.1909\n",
      "Epoch 2767/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4802 - mae: 16.4802 - val_loss: 16.2126 - val_mae: 16.2125\n",
      "Epoch 2768/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4771 - mae: 16.4771 - val_loss: 16.1826 - val_mae: 16.1826\n",
      "Epoch 2769/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4772 - mae: 16.4772 - val_loss: 16.1959 - val_mae: 16.1959\n",
      "Epoch 2770/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4704 - mae: 16.4704 - val_loss: 16.2084 - val_mae: 16.2084\n",
      "Epoch 2771/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4790 - mae: 16.4790 - val_loss: 16.1946 - val_mae: 16.1946\n",
      "Epoch 2772/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4738 - mae: 16.4738 - val_loss: 16.1912 - val_mae: 16.1912\n",
      "Epoch 2773/3000\n",
      "10496/10496 [==============================] - 1s 56us/sample - loss: 16.4685 - mae: 16.4685 - val_loss: 16.1896 - val_mae: 16.1896\n",
      "Epoch 2774/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4754 - mae: 16.4754 - val_loss: 16.1852 - val_mae: 16.1852\n",
      "Epoch 2775/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4816 - mae: 16.4816 - val_loss: 16.2028 - val_mae: 16.2028\n",
      "Epoch 2776/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4744 - mae: 16.4744 - val_loss: 16.1902 - val_mae: 16.1902\n",
      "Epoch 2777/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4759 - mae: 16.4759 - val_loss: 16.1932 - val_mae: 16.1932\n",
      "Epoch 2778/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4690 - mae: 16.4690 - val_loss: 16.1869 - val_mae: 16.1870\n",
      "Epoch 2779/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4776 - mae: 16.4776 - val_loss: 16.1936 - val_mae: 16.1936\n",
      "Epoch 2780/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4778 - mae: 16.4778 - val_loss: 16.1877 - val_mae: 16.1877\n",
      "Epoch 2781/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4773 - mae: 16.4773 - val_loss: 16.1821 - val_mae: 16.1821\n",
      "Epoch 2782/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4743 - mae: 16.4743 - val_loss: 16.1971 - val_mae: 16.1971\n",
      "Epoch 2783/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4799 - mae: 16.4799 - val_loss: 16.1854 - val_mae: 16.1854\n",
      "Epoch 2784/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4745 - mae: 16.4745 - val_loss: 16.1947 - val_mae: 16.1947\n",
      "Epoch 2785/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4760 - mae: 16.4760 - val_loss: 16.2264 - val_mae: 16.2264\n",
      "Epoch 2786/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4737 - mae: 16.4737 - val_loss: 16.1879 - val_mae: 16.1879\n",
      "Epoch 2787/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4760 - mae: 16.4760 - val_loss: 16.1829 - val_mae: 16.1829\n",
      "Epoch 2788/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4747 - mae: 16.4747 - val_loss: 16.1898 - val_mae: 16.1898\n",
      "Epoch 2789/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4675 - mae: 16.4675 - val_loss: 16.1891 - val_mae: 16.1891\n",
      "Epoch 2790/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4743 - mae: 16.4743 - val_loss: 16.1879 - val_mae: 16.1879\n",
      "Epoch 2791/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4694 - mae: 16.4694 - val_loss: 16.3007 - val_mae: 16.3007\n",
      "Epoch 2792/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4766 - mae: 16.4766 - val_loss: 16.1903 - val_mae: 16.1903\n",
      "Epoch 2793/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4719 - mae: 16.4719 - val_loss: 16.1816 - val_mae: 16.1816\n",
      "Epoch 2794/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4753 - mae: 16.4753 - val_loss: 16.1867 - val_mae: 16.1867\n",
      "Epoch 2795/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4616 - mae: 16.4616 - val_loss: 16.1814 - val_mae: 16.1814\n",
      "Epoch 2796/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4769 - mae: 16.4769 - val_loss: 16.1819 - val_mae: 16.1819\n",
      "Epoch 2797/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4761 - mae: 16.4761 - val_loss: 16.1928 - val_mae: 16.1928\n",
      "Epoch 2798/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4775 - mae: 16.4775 - val_loss: 16.1944 - val_mae: 16.1944\n",
      "Epoch 2799/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4727 - mae: 16.4727 - val_loss: 16.2831 - val_mae: 16.2831\n",
      "Epoch 2800/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4801 - mae: 16.4801 - val_loss: 16.1850 - val_mae: 16.1850\n",
      "Epoch 2801/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4750 - mae: 16.4750 - val_loss: 16.1882 - val_mae: 16.1882\n",
      "Epoch 2802/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4739 - mae: 16.4739 - val_loss: 16.2112 - val_mae: 16.2112\n",
      "Epoch 2803/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4732 - mae: 16.4732 - val_loss: 16.1855 - val_mae: 16.1855\n",
      "Epoch 2804/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4695 - mae: 16.4695 - val_loss: 16.2049 - val_mae: 16.2049\n",
      "Epoch 2805/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4714 - mae: 16.4714 - val_loss: 16.1920 - val_mae: 16.1920\n",
      "Epoch 2806/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4758 - mae: 16.4758 - val_loss: 16.1957 - val_mae: 16.1957\n",
      "Epoch 2807/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4742 - mae: 16.4742 - val_loss: 16.1907 - val_mae: 16.1907\n",
      "Epoch 2808/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4711 - mae: 16.4711 - val_loss: 16.1901 - val_mae: 16.1901\n",
      "Epoch 2809/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4711 - mae: 16.4711 - val_loss: 16.1956 - val_mae: 16.1956\n",
      "Epoch 2810/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4735 - mae: 16.4735 - val_loss: 16.2460 - val_mae: 16.2460\n",
      "Epoch 2811/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4751 - mae: 16.4751 - val_loss: 16.2079 - val_mae: 16.2079\n",
      "Epoch 2812/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4743 - mae: 16.4743 - val_loss: 16.2051 - val_mae: 16.2051\n",
      "Epoch 2813/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4724 - mae: 16.4724 - val_loss: 16.1966 - val_mae: 16.1966\n",
      "Epoch 2814/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4750 - mae: 16.4751 - val_loss: 16.1914 - val_mae: 16.1914\n",
      "Epoch 2815/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4731 - mae: 16.4731 - val_loss: 16.1822 - val_mae: 16.1822\n",
      "Epoch 2816/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4677 - mae: 16.4677 - val_loss: 16.2111 - val_mae: 16.2111\n",
      "Epoch 2817/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4676 - mae: 16.4676 - val_loss: 16.2385 - val_mae: 16.2385\n",
      "Epoch 2818/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4749 - mae: 16.4749 - val_loss: 16.2351 - val_mae: 16.2351\n",
      "Epoch 2819/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.4716 - mae: 16.4715 - val_loss: 16.1875 - val_mae: 16.1875\n",
      "Epoch 2820/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4660 - mae: 16.4660 - val_loss: 16.1832 - val_mae: 16.1832\n",
      "Epoch 2821/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4738 - mae: 16.4738 - val_loss: 16.1829 - val_mae: 16.1829\n",
      "Epoch 2822/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.4650 - mae: 16.4650 - val_loss: 16.2298 - val_mae: 16.2298\n",
      "Epoch 2823/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4738 - mae: 16.4738 - val_loss: 16.1866 - val_mae: 16.1866\n",
      "Epoch 2824/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4684 - mae: 16.4684 - val_loss: 16.1842 - val_mae: 16.1842\n",
      "Epoch 2825/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4749 - mae: 16.4749 - val_loss: 16.1824 - val_mae: 16.1824\n",
      "Epoch 2826/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4662 - mae: 16.4662 - val_loss: 16.1912 - val_mae: 16.1912\n",
      "Epoch 2827/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4753 - mae: 16.4753 - val_loss: 16.2082 - val_mae: 16.2082\n",
      "Epoch 2828/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4699 - mae: 16.4699 - val_loss: 16.2012 - val_mae: 16.2012\n",
      "Epoch 2829/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4740 - mae: 16.4740 - val_loss: 16.2071 - val_mae: 16.2071\n",
      "Epoch 2830/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4676 - mae: 16.4676 - val_loss: 16.1969 - val_mae: 16.1969\n",
      "Epoch 2831/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4676 - mae: 16.4676 - val_loss: 16.1964 - val_mae: 16.1964\n",
      "Epoch 2832/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4757 - mae: 16.4757 - val_loss: 16.1871 - val_mae: 16.1871\n",
      "Epoch 2833/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4770 - mae: 16.4770 - val_loss: 16.1953 - val_mae: 16.1953\n",
      "Epoch 2834/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4719 - mae: 16.4718 - val_loss: 16.1942 - val_mae: 16.1942\n",
      "Epoch 2835/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4727 - mae: 16.4727 - val_loss: 16.2027 - val_mae: 16.2027\n",
      "Epoch 2836/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4690 - mae: 16.4690 - val_loss: 16.2135 - val_mae: 16.2135\n",
      "Epoch 2837/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4733 - mae: 16.4733 - val_loss: 16.1945 - val_mae: 16.1945\n",
      "Epoch 2838/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4696 - mae: 16.4696 - val_loss: 16.2552 - val_mae: 16.2552\n",
      "Epoch 2839/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4724 - mae: 16.4724 - val_loss: 16.1917 - val_mae: 16.1917\n",
      "Epoch 2840/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4684 - mae: 16.4684 - val_loss: 16.1853 - val_mae: 16.1853\n",
      "Epoch 2841/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4700 - mae: 16.4700 - val_loss: 16.1993 - val_mae: 16.1993\n",
      "Epoch 2842/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4709 - mae: 16.4709 - val_loss: 16.1893 - val_mae: 16.1893\n",
      "Epoch 2843/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4715 - mae: 16.4715 - val_loss: 16.1834 - val_mae: 16.1833\n",
      "Epoch 2844/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4657 - mae: 16.4657 - val_loss: 16.2035 - val_mae: 16.2035\n",
      "Epoch 2845/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4668 - mae: 16.4668 - val_loss: 16.2052 - val_mae: 16.2052\n",
      "Epoch 2846/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4686 - mae: 16.4686 - val_loss: 16.1917 - val_mae: 16.1917\n",
      "Epoch 2847/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4702 - mae: 16.4702 - val_loss: 16.1833 - val_mae: 16.1833\n",
      "Epoch 2848/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4680 - mae: 16.4680 - val_loss: 16.1940 - val_mae: 16.1940\n",
      "Epoch 2849/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4731 - mae: 16.4731 - val_loss: 16.2099 - val_mae: 16.2099\n",
      "Epoch 2850/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4671 - mae: 16.4671 - val_loss: 16.1867 - val_mae: 16.1867\n",
      "Epoch 2851/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4737 - mae: 16.4737 - val_loss: 16.1858 - val_mae: 16.1858\n",
      "Epoch 2852/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4713 - mae: 16.4713 - val_loss: 16.2158 - val_mae: 16.2158\n",
      "Epoch 2853/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4709 - mae: 16.4709 - val_loss: 16.1868 - val_mae: 16.1868\n",
      "Epoch 2854/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4659 - mae: 16.4659 - val_loss: 16.2581 - val_mae: 16.2582\n",
      "Epoch 2855/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4711 - mae: 16.4711 - val_loss: 16.1824 - val_mae: 16.1824\n",
      "Epoch 2856/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4690 - mae: 16.4690 - val_loss: 16.1905 - val_mae: 16.1905\n",
      "Epoch 2857/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4697 - mae: 16.4697 - val_loss: 16.1848 - val_mae: 16.1848\n",
      "Epoch 2858/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4709 - mae: 16.4709 - val_loss: 16.2120 - val_mae: 16.2120\n",
      "Epoch 2859/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4689 - mae: 16.4689 - val_loss: 16.2243 - val_mae: 16.2243\n",
      "Epoch 2860/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4745 - mae: 16.4745 - val_loss: 16.1991 - val_mae: 16.1991\n",
      "Epoch 2861/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4610 - mae: 16.4610 - val_loss: 16.2690 - val_mae: 16.2689\n",
      "Epoch 2862/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4691 - mae: 16.4691 - val_loss: 16.1835 - val_mae: 16.1835\n",
      "Epoch 2863/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.4680 - mae: 16.4680 - val_loss: 16.2234 - val_mae: 16.2234\n",
      "Epoch 2864/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4698 - mae: 16.4698 - val_loss: 16.1871 - val_mae: 16.1871\n",
      "Epoch 2865/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.4742 - mae: 16.4742 - val_loss: 16.1987 - val_mae: 16.1987\n",
      "Epoch 2866/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4715 - mae: 16.4715 - val_loss: 16.1797 - val_mae: 16.1797\n",
      "Epoch 2867/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4677 - mae: 16.4678 - val_loss: 16.2450 - val_mae: 16.2450\n",
      "Epoch 2868/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4684 - mae: 16.4684 - val_loss: 16.2320 - val_mae: 16.2320\n",
      "Epoch 2869/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4683 - mae: 16.4683 - val_loss: 16.1930 - val_mae: 16.1930\n",
      "Epoch 2870/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4634 - mae: 16.4634 - val_loss: 16.1890 - val_mae: 16.1890\n",
      "Epoch 2871/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4665 - mae: 16.4665 - val_loss: 16.2098 - val_mae: 16.2098\n",
      "Epoch 2872/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4729 - mae: 16.4729 - val_loss: 16.1924 - val_mae: 16.1924\n",
      "Epoch 2873/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4737 - mae: 16.4737 - val_loss: 16.2052 - val_mae: 16.2052\n",
      "Epoch 2874/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4657 - mae: 16.4657 - val_loss: 16.1803 - val_mae: 16.1803\n",
      "Epoch 2875/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4734 - mae: 16.4734 - val_loss: 16.1856 - val_mae: 16.1856\n",
      "Epoch 2876/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4708 - mae: 16.4708 - val_loss: 16.1850 - val_mae: 16.1850\n",
      "Epoch 2877/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.4644 - mae: 16.4644 - val_loss: 16.1881 - val_mae: 16.1881\n",
      "Epoch 2878/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4643 - mae: 16.4643 - val_loss: 16.2292 - val_mae: 16.2292\n",
      "Epoch 2879/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4681 - mae: 16.4681 - val_loss: 16.2190 - val_mae: 16.2190\n",
      "Epoch 2880/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4696 - mae: 16.4696 - val_loss: 16.1899 - val_mae: 16.1899\n",
      "Epoch 2881/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4657 - mae: 16.4657 - val_loss: 16.1865 - val_mae: 16.1865\n",
      "Epoch 2882/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4701 - mae: 16.4700 - val_loss: 16.1910 - val_mae: 16.1910\n",
      "Epoch 2883/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4615 - mae: 16.4615 - val_loss: 16.1842 - val_mae: 16.1842\n",
      "Epoch 2884/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4609 - mae: 16.4609 - val_loss: 16.1917 - val_mae: 16.1917\n",
      "Epoch 2885/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4643 - mae: 16.4643 - val_loss: 16.1975 - val_mae: 16.1975\n",
      "Epoch 2886/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.4694 - mae: 16.4694 - val_loss: 16.1884 - val_mae: 16.1884\n",
      "Epoch 2887/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4640 - mae: 16.4640 - val_loss: 16.1909 - val_mae: 16.1909\n",
      "Epoch 2888/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4672 - mae: 16.4672 - val_loss: 16.2151 - val_mae: 16.2151\n",
      "Epoch 2889/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4715 - mae: 16.4715 - val_loss: 16.2269 - val_mae: 16.2269\n",
      "Epoch 2890/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4710 - mae: 16.4710 - val_loss: 16.1809 - val_mae: 16.1809\n",
      "Epoch 2891/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4628 - mae: 16.4628 - val_loss: 16.2195 - val_mae: 16.2195\n",
      "Epoch 2892/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4626 - mae: 16.4626 - val_loss: 16.2163 - val_mae: 16.2163\n",
      "Epoch 2893/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4667 - mae: 16.4667 - val_loss: 16.1849 - val_mae: 16.1849\n",
      "Epoch 2894/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4655 - mae: 16.4656 - val_loss: 16.1813 - val_mae: 16.1813\n",
      "Epoch 2895/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4620 - mae: 16.4620 - val_loss: 16.1848 - val_mae: 16.1848\n",
      "Epoch 2896/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4657 - mae: 16.4657 - val_loss: 16.2181 - val_mae: 16.2181\n",
      "Epoch 2897/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4697 - mae: 16.4697 - val_loss: 16.1809 - val_mae: 16.1809\n",
      "Epoch 2898/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4677 - mae: 16.4677 - val_loss: 16.1947 - val_mae: 16.1947\n",
      "Epoch 2899/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4655 - mae: 16.4655 - val_loss: 16.1828 - val_mae: 16.1828\n",
      "Epoch 2900/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4667 - mae: 16.4667 - val_loss: 16.1794 - val_mae: 16.1794\n",
      "Epoch 2901/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4618 - mae: 16.4618 - val_loss: 16.2113 - val_mae: 16.2113\n",
      "Epoch 2902/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4617 - mae: 16.4617 - val_loss: 16.1871 - val_mae: 16.1871\n",
      "Epoch 2903/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4665 - mae: 16.4665 - val_loss: 16.1805 - val_mae: 16.1805\n",
      "Epoch 2904/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4682 - mae: 16.4682 - val_loss: 16.1951 - val_mae: 16.1951\n",
      "Epoch 2905/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4578 - mae: 16.4578 - val_loss: 16.1835 - val_mae: 16.1835\n",
      "Epoch 2906/3000\n",
      "10496/10496 [==============================] - 0s 45us/sample - loss: 16.4628 - mae: 16.4628 - val_loss: 16.1833 - val_mae: 16.1833\n",
      "Epoch 2907/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4542 - mae: 16.4542 - val_loss: 16.1817 - val_mae: 16.1817\n",
      "Epoch 2908/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4666 - mae: 16.4666 - val_loss: 16.1815 - val_mae: 16.1815\n",
      "Epoch 2909/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4586 - mae: 16.4586 - val_loss: 16.1925 - val_mae: 16.1925\n",
      "Epoch 2910/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4661 - mae: 16.4661 - val_loss: 16.1824 - val_mae: 16.1824\n",
      "Epoch 2911/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4656 - mae: 16.4656 - val_loss: 16.2201 - val_mae: 16.2201\n",
      "Epoch 2912/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4661 - mae: 16.4661 - val_loss: 16.1951 - val_mae: 16.1951\n",
      "Epoch 2913/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4542 - mae: 16.4542 - val_loss: 16.3078 - val_mae: 16.3078\n",
      "Epoch 2914/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4635 - mae: 16.4635 - val_loss: 16.1960 - val_mae: 16.1960\n",
      "Epoch 2915/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4676 - mae: 16.4676 - val_loss: 16.1875 - val_mae: 16.1875\n",
      "Epoch 2916/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4646 - mae: 16.4646 - val_loss: 16.1917 - val_mae: 16.1917\n",
      "Epoch 2917/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4633 - mae: 16.4633 - val_loss: 16.2051 - val_mae: 16.2051\n",
      "Epoch 2918/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4623 - mae: 16.4623 - val_loss: 16.2140 - val_mae: 16.2140\n",
      "Epoch 2919/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4667 - mae: 16.4667 - val_loss: 16.1808 - val_mae: 16.1808\n",
      "Epoch 2920/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4627 - mae: 16.4627 - val_loss: 16.2041 - val_mae: 16.2041\n",
      "Epoch 2921/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4621 - mae: 16.4621 - val_loss: 16.1996 - val_mae: 16.1996\n",
      "Epoch 2922/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4617 - mae: 16.4617 - val_loss: 16.1920 - val_mae: 16.1920\n",
      "Epoch 2923/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4625 - mae: 16.4625 - val_loss: 16.2310 - val_mae: 16.2310\n",
      "Epoch 2924/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4708 - mae: 16.4708 - val_loss: 16.1912 - val_mae: 16.1912\n",
      "Epoch 2925/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.4615 - mae: 16.4615 - val_loss: 16.1812 - val_mae: 16.1812\n",
      "Epoch 2926/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4680 - mae: 16.4680 - val_loss: 16.1930 - val_mae: 16.1930\n",
      "Epoch 2927/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4658 - mae: 16.4658 - val_loss: 16.1874 - val_mae: 16.1874\n",
      "Epoch 2928/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4661 - mae: 16.4661 - val_loss: 16.1903 - val_mae: 16.1903\n",
      "Epoch 2929/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4667 - mae: 16.4667 - val_loss: 16.1834 - val_mae: 16.1834\n",
      "Epoch 2930/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4642 - mae: 16.4642 - val_loss: 16.1784 - val_mae: 16.1784\n",
      "Epoch 2931/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4587 - mae: 16.4587 - val_loss: 16.2242 - val_mae: 16.2242\n",
      "Epoch 2932/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4661 - mae: 16.4661 - val_loss: 16.1956 - val_mae: 16.1956\n",
      "Epoch 2933/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4649 - mae: 16.4649 - val_loss: 16.1816 - val_mae: 16.1816\n",
      "Epoch 2934/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4678 - mae: 16.4678 - val_loss: 16.1819 - val_mae: 16.1819\n",
      "Epoch 2935/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4627 - mae: 16.4627 - val_loss: 16.1876 - val_mae: 16.1876\n",
      "Epoch 2936/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4616 - mae: 16.4616 - val_loss: 16.1799 - val_mae: 16.1799\n",
      "Epoch 2937/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4650 - mae: 16.4651 - val_loss: 16.1854 - val_mae: 16.1854\n",
      "Epoch 2938/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4663 - mae: 16.4663 - val_loss: 16.1853 - val_mae: 16.1853\n",
      "Epoch 2939/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4633 - mae: 16.4633 - val_loss: 16.1986 - val_mae: 16.1986\n",
      "Epoch 2940/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4548 - mae: 16.4548 - val_loss: 16.1889 - val_mae: 16.1889\n",
      "Epoch 2941/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4490 - mae: 16.4490 - val_loss: 16.1774 - val_mae: 16.1774\n",
      "Epoch 2942/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4660 - mae: 16.4660 - val_loss: 16.1787 - val_mae: 16.1787\n",
      "Epoch 2943/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4612 - mae: 16.4612 - val_loss: 16.1951 - val_mae: 16.1951\n",
      "Epoch 2944/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4639 - mae: 16.4639 - val_loss: 16.1871 - val_mae: 16.1871\n",
      "Epoch 2945/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4616 - mae: 16.4616 - val_loss: 16.2129 - val_mae: 16.2129\n",
      "Epoch 2946/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4592 - mae: 16.4592 - val_loss: 16.1838 - val_mae: 16.1838\n",
      "Epoch 2947/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4612 - mae: 16.4612 - val_loss: 16.1834 - val_mae: 16.1834\n",
      "Epoch 2948/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4659 - mae: 16.4659 - val_loss: 16.1948 - val_mae: 16.1948\n",
      "Epoch 2949/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4634 - mae: 16.4634 - val_loss: 16.2071 - val_mae: 16.2071\n",
      "Epoch 2950/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4522 - mae: 16.4522 - val_loss: 16.2109 - val_mae: 16.2109\n",
      "Epoch 2951/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4637 - mae: 16.4637 - val_loss: 16.2108 - val_mae: 16.2108\n",
      "Epoch 2952/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4625 - mae: 16.4625 - val_loss: 16.2391 - val_mae: 16.2391\n",
      "Epoch 2953/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4603 - mae: 16.4603 - val_loss: 16.1841 - val_mae: 16.1841\n",
      "Epoch 2954/3000\n",
      "10496/10496 [==============================] - 1s 50us/sample - loss: 16.4620 - mae: 16.4620 - val_loss: 16.2073 - val_mae: 16.2073\n",
      "Epoch 2955/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4596 - mae: 16.4596 - val_loss: 16.1886 - val_mae: 16.1886\n",
      "Epoch 2956/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4615 - mae: 16.4615 - val_loss: 16.1973 - val_mae: 16.1973\n",
      "Epoch 2957/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.4651 - mae: 16.4651 - val_loss: 16.1945 - val_mae: 16.1945\n",
      "Epoch 2958/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4619 - mae: 16.4619 - val_loss: 16.1928 - val_mae: 16.1928\n",
      "Epoch 2959/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4492 - mae: 16.4492 - val_loss: 16.1958 - val_mae: 16.1958\n",
      "Epoch 2960/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4594 - mae: 16.4594 - val_loss: 16.1993 - val_mae: 16.1993\n",
      "Epoch 2961/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4615 - mae: 16.4615 - val_loss: 16.2518 - val_mae: 16.2518\n",
      "Epoch 2962/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4633 - mae: 16.4633 - val_loss: 16.2117 - val_mae: 16.2117\n",
      "Epoch 2963/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4564 - mae: 16.4564 - val_loss: 16.2020 - val_mae: 16.2020\n",
      "Epoch 2964/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4619 - mae: 16.4619 - val_loss: 16.1823 - val_mae: 16.1823\n",
      "Epoch 2965/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4642 - mae: 16.4642 - val_loss: 16.1851 - val_mae: 16.1851\n",
      "Epoch 2966/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4571 - mae: 16.4571 - val_loss: 16.1773 - val_mae: 16.1773\n",
      "Epoch 2967/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4570 - mae: 16.4570 - val_loss: 16.1779 - val_mae: 16.1779\n",
      "Epoch 2968/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4624 - mae: 16.4624 - val_loss: 16.1925 - val_mae: 16.1925\n",
      "Epoch 2969/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4595 - mae: 16.4595 - val_loss: 16.1902 - val_mae: 16.1902\n",
      "Epoch 2970/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4553 - mae: 16.4553 - val_loss: 16.1848 - val_mae: 16.1848\n",
      "Epoch 2971/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4640 - mae: 16.4640 - val_loss: 16.1771 - val_mae: 16.1771\n",
      "Epoch 2972/3000\n",
      "10496/10496 [==============================] - 0s 46us/sample - loss: 16.4632 - mae: 16.4632 - val_loss: 16.1821 - val_mae: 16.1821\n",
      "Epoch 2973/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4619 - mae: 16.4619 - val_loss: 16.1988 - val_mae: 16.1988\n",
      "Epoch 2974/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4607 - mae: 16.4607 - val_loss: 16.2113 - val_mae: 16.2113\n",
      "Epoch 2975/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4553 - mae: 16.4553 - val_loss: 16.2316 - val_mae: 16.2316\n",
      "Epoch 2976/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.4579 - mae: 16.4579 - val_loss: 16.2364 - val_mae: 16.2364\n",
      "Epoch 2977/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.4622 - mae: 16.4622 - val_loss: 16.1768 - val_mae: 16.1768\n",
      "Epoch 2978/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4648 - mae: 16.4648 - val_loss: 16.1795 - val_mae: 16.1795\n",
      "Epoch 2979/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4582 - mae: 16.4582 - val_loss: 16.2128 - val_mae: 16.2128\n",
      "Epoch 2980/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4627 - mae: 16.4627 - val_loss: 16.1755 - val_mae: 16.1755\n",
      "Epoch 2981/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4626 - mae: 16.4626 - val_loss: 16.1777 - val_mae: 16.1777\n",
      "Epoch 2982/3000\n",
      "10496/10496 [==============================] - 0s 48us/sample - loss: 16.4583 - mae: 16.4583 - val_loss: 16.1756 - val_mae: 16.1756\n",
      "Epoch 2983/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4579 - mae: 16.4579 - val_loss: 16.1772 - val_mae: 16.1772\n",
      "Epoch 2984/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4574 - mae: 16.4574 - val_loss: 16.1911 - val_mae: 16.1911\n",
      "Epoch 2985/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4574 - mae: 16.4574 - val_loss: 16.1816 - val_mae: 16.1816\n",
      "Epoch 2986/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4626 - mae: 16.4626 - val_loss: 16.1787 - val_mae: 16.1787\n",
      "Epoch 2987/3000\n",
      "10496/10496 [==============================] - 1s 51us/sample - loss: 16.4571 - mae: 16.4571 - val_loss: 16.1815 - val_mae: 16.1815\n",
      "Epoch 2988/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4610 - mae: 16.4610 - val_loss: 16.1761 - val_mae: 16.1761\n",
      "Epoch 2989/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4592 - mae: 16.4592 - val_loss: 16.1946 - val_mae: 16.1946\n",
      "Epoch 2990/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4576 - mae: 16.4576 - val_loss: 16.1768 - val_mae: 16.1768\n",
      "Epoch 2991/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4570 - mae: 16.4570 - val_loss: 16.1778 - val_mae: 16.1778\n",
      "Epoch 2992/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4572 - mae: 16.4572 - val_loss: 16.1929 - val_mae: 16.1929\n",
      "Epoch 2993/3000\n",
      "10496/10496 [==============================] - 1s 49us/sample - loss: 16.4509 - mae: 16.4509 - val_loss: 16.2224 - val_mae: 16.2224\n",
      "Epoch 2994/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4605 - mae: 16.4605 - val_loss: 16.1997 - val_mae: 16.1997\n",
      "Epoch 2995/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4565 - mae: 16.4565 - val_loss: 16.1839 - val_mae: 16.1839\n",
      "Epoch 2996/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4533 - mae: 16.4533 - val_loss: 16.1995 - val_mae: 16.1995\n",
      "Epoch 2997/3000\n",
      "10496/10496 [==============================] - 0s 47us/sample - loss: 16.4611 - mae: 16.4611 - val_loss: 16.2049 - val_mae: 16.2049\n",
      "Epoch 2998/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4560 - mae: 16.4560 - val_loss: 16.1735 - val_mae: 16.1735\n",
      "Epoch 2999/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4624 - mae: 16.4624 - val_loss: 16.1797 - val_mae: 16.1797\n",
      "Epoch 3000/3000\n",
      "10496/10496 [==============================] - 1s 48us/sample - loss: 16.4588 - mae: 16.4587 - val_loss: 16.1817 - val_mae: 16.1817\n",
      "Wall time: 25min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x207f08c8e10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y_train, \n",
    "          validation_split=0.33,\n",
    "          epochs=3000\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MhwAHYcS1CIe",
    "outputId": "2ca3e3bb-8d86-4f6b-d166-f1ce6c456ad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.010548923564738"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "mean_absolute_error(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "78-tGeaM1FNa",
    "outputId": "03f3e5d9-f59f-4648-8dd4-435dc145e992"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21.673750717978173"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(random_state=42, depth=)\n",
    "\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "preds = rfc.predict(X_test)\n",
    "mean_absolute_error(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "QOax9zoR1SJA",
    "outputId": "965d353c-bd67-4711-d99d-20f5e7d61d43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0224292 , 0.06236851, 0.04334834, 0.04349176, 0.03488207,\n",
       "       0.02210729, 0.1304766 , 0.17850131, 0.19560488, 0.04941461,\n",
       "       0.1983678 , 0.01900761])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3EGY-WPm1QFp"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'model.pkl'\n",
    "pickle.dump(rfc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    19953\n",
       "0      937\n",
       "Name: wifi, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['wifi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "AirBnB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
